<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://sungyubkim.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://sungyubkim.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-01-29T17:26:51+09:00</updated><id>https://sungyubkim.github.io/feed.xml</id><title type="html">blank</title><subtitle>Research Scientist @ AI Center, Samsung Electronics. Personal academic website with publications, blog posts, and CV. </subtitle><entry><title type="html">Flash Attention 3</title><link href="https://sungyubkim.github.io/blog/fa3/" rel="alternate" type="text/html" title="Flash Attention 3"/><published>2025-06-04T00:00:00+09:00</published><updated>2025-06-04T00:00:00+09:00</updated><id>https://sungyubkim.github.io/blog/fa3</id><content type="html" xml:base="https://sungyubkim.github.io/blog/fa3/"><![CDATA[<h1 id="tldr">TL;DR</h1> <blockquote> <p>FlashAttention-3는 AI 모델(ChatGPT 같은)의 어텐션 메커니즘을 <strong>1.5-2.0배 빠르게</strong> 만들면서 <strong>절반 정밀도(FP8)</strong>를 사용해도 정확도 손실 없이 작동하는 획기적인 최적화 기술입니다.</p> <p><strong>문제점</strong>: FlashAttention-2가 H100 GPU에서 단 35%의 활용률만을 달성 - 즉, 비싼 AI 하드웨어가 가장 중요한 연산 중에 대부분 놀고 있었습니다.</p> <p><strong>해결책</strong>: 함께 작동하는 세 가지 핵심 혁신:</p> <ol> <li><strong>비동기 처리</strong>: 데이터 로딩과 연산을 위한 별도의 작업자를 두는 것과 같음</li> <li><strong>중첩 연산</strong>: 기다리는 대신 여러 작업을 동시에 계산</li> <li><strong>스마트 저정밀도</strong>: 정확도 손실 없이 16비트 대신 8비트 숫자 사용</li> </ol> <p><strong>주요 결과</strong>:</p> <ul> <li><strong>속도</strong>: 최대 740 TFLOPs/s (GPU 활용률 75% vs 이전 35%)</li> <li><strong>효율성</strong>: FP8 정밀도로 거의 1.2 PFLOPs/s</li> <li><strong>정확도</strong>: 표준 FP8 방법보다 2.6배 더 정확</li> <li><strong>임팩트</strong>: 장문맥 AI 애플리케이션(전체 책, 코드베이스 분석)을 실용적으로 만듦</li> </ul> <p>어텐션이 현대 AI의 연산 병목이기 때문에 이를 빠르게 만드는 것은 더 강력한 AI 애플리케이션을 더 낮은 비용으로 가능하게 합니다.</p> </blockquote> <ul> <li>Paper Link: <a href="https://arxiv.org/pdf/2407.08608">https://arxiv.org/pdf/2407.08608</a></li> </ul> <hr/> <h1 id="related-papers">Related Papers</h1> <p><strong>FlashAttention 시리즈 발전:</strong></p> <ul> <li><a href="/blog/fa/">FlashAttention</a> - 원조 IO-aware 어텐션 알고리즘과 타일링 기법의 기초</li> <li><a href="/blog/fa2/">FlashAttention-2</a> - 2배 빠른 속도와 향상된 병렬화로 발전된 버전</li> <li><a href="https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf">From Online Softmax to FlashAttention</a> - 온라인 소프트맥스의 이론적 기초와 수치 안정성</li> </ul> <p><strong>효율적인 어텐션 메커니즘:</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2112.05682">Memory-efficient Attention</a> - 메모리 최적화 어텐션 기법들의 비교 분석</li> <li><a href="https://arxiv.org/pdf/1911.02150">MQA</a> - 멀티쿼리 어텐션으로 키/값 공유를 통한 추론 가속</li> <li><a href="https://arxiv.org/pdf/2305.13245">GQA</a> - 그룹드 쿼리 어텐션의 품질-속도 균형점 탐색</li> </ul> <p><strong>위치 인코딩과 긴 시퀀스 처리:</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2104.09864">RoFormer</a> - 회전 위치 임베딩과의 최적화된 통합</li> <li><a href="https://arxiv.org/pdf/2309.00071">YaRN</a> - RoPE 기반 컨텍스트 길이 확장 기법</li> <li><a href="https://arxiv.org/pdf/2310.05209">Scaling Laws of RoPE-based Extrapolation</a> - 위치 인코딩 확장과 성능 스케일링</li> </ul> <p><strong>하드웨어 최적화 및 시스템 연구:</strong></p> <ul> <li><a href="/blog/tp/">Tensor Parallelism</a> - 텐서 병렬화와 FlashAttention의 결합 전략</li> <li><a href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> - 메모리 효율적인 병렬 훈련 기법</li> <li><a href="/blog/pp/">GPipe</a> - 파이프라인 병렬화와의 통합 방법론</li> </ul> <hr/> <h1 id="takeaways">Takeaways</h1> <h2 id="1-동기-왜-flashattention-3가-필요했는가">1. 동기: 왜 FlashAttention-3가 필요했는가</h2> <h3 id="성능-격차-문제">성능 격차 문제</h3> <p><strong>실험적 동기</strong>: 저자들은 기존 어텐션 구현의 심각한 비효율성을 발견했습니다. FlashAttention-2가 새로운 GPU에서 최적화된 행렬 곱셈(GEMM) 커널 대비 낮은 활용률을 달성하는데, Hopper H100 GPU에서 35% 대 80-90%였습니다.</p> <p>이는 포뮬러 1 자동차를 가지고 있지만 엔진 출력의 35%만 사용하는 것과 같습니다 - 프리미엄 하드웨어 비용을 지불하지만 평범한 성능을 얻고 있었습니다. 저자들은 기존 어텐션 알고리즘이 새로운 GPU 기능을 활용하도록 설계되지 않았음을 깨달았습니다.</p> <p><strong>하드웨어 진화 격차</strong>: 현대 GPU(NVIDIA H100 같은)는 새로운 기능들을 도입했습니다:</p> <ul> <li><strong>비동기 텐서 코어</strong>: 데이터를 로딩하면서 동시에 연산 가능</li> <li><strong>FP8 정밀도</strong>: FP16보다 2배 빠르지만 신중한 처리 필요</li> <li><strong>전용 메모리 유닛</strong>: 특정 연산을 위해 설계된 하드웨어</li> </ul> <p>하지만 FlashAttention-2는 구형 하드웨어를 위해 설계되어 이러한 기능들을 효과적으로 사용할 수 없었습니다.</p> <h3 id="장문맥-도전">장문맥 도전</h3> <p><strong>실제 임팩트</strong>: 어텐션을 더 긴 맥락으로 확장하면 새로운 능력(여러 긴 문서와 대용량 코드베이스의 파일들에 대한 모델링과 추론), 새로운 모달리티(고해상도 이미지, 오디오, 비디오), 새로운 애플리케이션(긴 기록과의 사용자 상호작용, 긴 지평선의 에이전트 워크플로)이 열릴 것입니다.</p> <p>현재 AI 모델들은 다음과 같은 작업에서 어려움을 겪습니다:</p> <ul> <li>전체 책이나 연구 논문 분석</li> <li>완전한 영화 스크립트나 긴 대화 이해</li> <li>고해상도 이미지나 긴 오디오 파일 처리</li> <li>장기간 상호작용에서 맥락 유지</li> </ul> <p>병목은? 어텐션 연산이 시퀀스 길이에 대해 제곱으로 증가하여 긴 맥락을 금지적으로 비싸게 만듭니다.</p> <h2 id="2-핵심-방법-세-가지-상승효과-혁신">2. 핵심 방법: 세 가지 상승효과 혁신</h2> <h3 id="혁신-1-생산자-소비자-비동기성">혁신 1: 생산자-소비자 비동기성</h3> <p><strong>통찰</strong>: 전통적인 어텐션은 데이터가 로드될 때까지 기다린 후 연산합니다. 현대 GPU는 동시에 로드하고 연산할 수 있지만, 작업을 다르게 구성해야 합니다.</p> <p><strong>Python 의사코드</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">asyncio</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="k">class</span> <span class="nc">AsyncAttention</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">data_loader_workers</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 생산자 워프
</span>        <span class="n">self</span><span class="p">.</span><span class="n">compute_workers</span> <span class="o">=</span> <span class="p">[]</span>      <span class="c1"># 소비자 워프
</span>        <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span> <span class="o">=</span> <span class="nc">CircularBuffer</span><span class="p">(</span><span class="n">stages</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    
    <span class="k">async</span> <span class="k">def</span> <span class="nf">producer_worker</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">K_blocks</span><span class="p">,</span> <span class="n">V_blocks</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">공유 버퍼에 데이터를 지속적으로 로드</span><span class="sh">"""</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">K_i</span><span class="p">,</span> <span class="n">V_i</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">K_blocks</span><span class="p">,</span> <span class="n">V_blocks</span><span class="p">)):</span>
            <span class="c1"># 버퍼 슬롯이 비어있을 때까지 대기
</span>            <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">wait_for_slot</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
            
            <span class="c1"># 비동기적으로 데이터 로드 (블록하지 않음)
</span>            <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">gpu_load_async</span><span class="p">(</span><span class="n">K_i</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">slot</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">))</span>
            <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">gpu_load_async</span><span class="p">(</span><span class="n">V_i</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">slot</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">))</span>
            
            <span class="c1"># 데이터 준비 완료 신호
</span>            <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">mark_ready</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
    
    <span class="k">async</span> <span class="k">def</span> <span class="nf">consumer_worker</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Q_block</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">로드된 데이터에 대해 지속적으로 연산</span><span class="sh">"""</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">Q_block</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">K_blocks</span><span class="p">)):</span>
            <span class="c1"># 데이터가 로드될 때까지 대기
</span>            <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">wait_for_data</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
            
            <span class="c1"># 다음 데이터가 백그라운드에서 로드되는 동안 연산
</span>            <span class="n">K_i</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">get_K</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
            <span class="n">V_i</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">get_V</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
            
            <span class="c1"># 어텐션 연산
</span>            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q_block</span><span class="p">,</span> <span class="n">K_i</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">V_i</span><span class="p">)</span>
            
            <span class="c1"># 슬롯이 재사용을 위해 비었다고 신호
</span>            <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">mark_consumed</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p><strong>작동 원리 - 레스토랑 비유</strong>:</p> <ul> <li><strong>전통적 접근</strong>: 셰프가 웨이터가 재료를 가져올 때까지 기다린 후 요리하고, 다시 기다림</li> <li><strong>FlashAttention-3</strong>: 웨이터가 셰프가 사용 가능한 재료로 요리하는 동안 지속적으로 재료를 가져옴</li> <li><strong>결과</strong>: 주방이 놀지 않고 최대 용량으로 운영됨</li> </ul> <p><strong>내 분석</strong>: 이는 현대 CPU의 파이프라이닝 작동 방식과 일치하지만 GPU 어텐션 연산에 적용한 점이 우아합니다. 핵심 통찰은 GPU 메모리 대역폭과 연산 능력을 순차적이 아닌 동시에 활용할 수 있다는 것입니다.</p> <h3 id="혁신-2-gemm-소프트맥스-중첩">혁신 2: GEMM-소프트맥스 중첩</h3> <p><strong>문제</strong>: H100 SXM5 GPU는 989 TFLOPS의 FP16 행렬곱을 가지지만 소프트맥스에 필요한 지수함수 같은 특수 함수는 단 3.9 TFLOPS입니다. 소프트맥스 연산(비싼 지수 함수 포함)이 빠른 행렬 곱셈을 막고 있었습니다.</p> <p><strong>Python 의사코드</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PipelinedAttention</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">stage_current</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">stage_next</span> <span class="o">=</span> <span class="mi">1</span>
    
    <span class="k">async</span> <span class="k">def</span> <span class="nf">two_stage_pipeline</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K_blocks</span><span class="p">,</span> <span class="n">V_blocks</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">QK^T, 소프트맥스, PV 연산을 중첩</span><span class="sh">"""</span>
        <span class="n">O</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
        
        <span class="c1"># 단계 0: 첫 번째 연산 초기화
</span>        <span class="n">S_current</span> <span class="o">=</span> <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">async_matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K_blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">T</span><span class="p">)</span>
        <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">wait_completion</span><span class="p">()</span>
        
        <span class="n">m</span><span class="p">,</span> <span class="n">P_current</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_softmax_stats</span><span class="p">(</span><span class="n">S_current</span><span class="p">)</span>
        
        <span class="c1"># 메인 파이프라인 루프
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">K_blocks</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># 파이프라인 단계 1: 다음 QK^T 시작 (기다리지 않음)
</span>            <span class="n">S_next_future</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">async_matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K_blocks</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">T</span><span class="p">)</span>
            
            <span class="c1"># 파이프라인 단계 2: 현재 PV 시작 (기다리지 않음)  
</span>            <span class="n">O_update_future</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">async_matmul</span><span class="p">(</span><span class="n">P_current</span><span class="p">,</span> <span class="n">V_blocks</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            
            <span class="c1"># 파이프라인 단계 3: QK^T 대기, 소프트맥스 연산
</span>            <span class="n">S_next</span> <span class="o">=</span> <span class="k">await</span> <span class="n">S_next_future</span>
            <span class="n">m</span><span class="p">,</span> <span class="n">P_next</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_softmax_stats</span><span class="p">(</span><span class="n">S_next</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
            
            <span class="c1"># 파이프라인 단계 4: PV 대기, 출력 업데이트
</span>            <span class="n">O_update</span> <span class="o">=</span> <span class="k">await</span> <span class="n">O_update_future</span>
            <span class="n">O</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">rescale_and_add</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">O_update</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
            
            <span class="c1"># 다음 반복을 위해 교체
</span>            <span class="n">P_current</span> <span class="o">=</span> <span class="n">P_next</span>
        
        <span class="k">return</span> <span class="n">O</span>
    
    <span class="k">def</span> <span class="nf">compute_softmax_stats</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">m_old</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">l_old</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">실행 통계를 사용한 수치적으로 안정적인 소프트맥스</span><span class="sh">"""</span>
        <span class="k">if</span> <span class="n">m_old</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">m_old</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="n">S</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>
            <span class="n">l_old</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">S</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        
        <span class="c1"># 행 최댓값 업데이트
</span>        <span class="n">m_new</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">m_old</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
        
        <span class="c1"># 확률 계산 및 행 합계 업데이트
</span>        <span class="n">P</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">S</span> <span class="o">-</span> <span class="n">m_new</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">l_new</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">m_old</span> <span class="o">-</span> <span class="n">m_new</span><span class="p">)</span> <span class="o">*</span> <span class="n">l_old</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">m_new</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">l_new</span>
</code></pre></div></div> <p><strong>조립 라인 비유</strong>:</p> <ul> <li><strong>전통적</strong>: 자동차 조립이 각 스테이션에서 정지 - 엔진 설치, 그 다음 페인팅, 그 다음 시트 설치</li> <li><strong>FlashAttention-3</strong>: 여러 자동차가 동시에 다른 스테이션에서 - A차가 페인팅되는 동안 B차는 엔진, C차는 시트</li> <li><strong>결과</strong>: 같은 자원으로 3배 높은 처리량</li> </ul> <p><strong>숫자를 사용한 실제 예시</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 전통적 순차 타이밍
</span><span class="n">QK_time</span> <span class="o">=</span> <span class="mi">10</span><span class="n">ms</span>    <span class="c1"># 행렬 곱셈
</span><span class="n">Softmax_time</span> <span class="o">=</span> <span class="mi">5</span><span class="n">ms</span> <span class="c1"># 지수 연산  
</span><span class="n">PV_time</span> <span class="o">=</span> <span class="mi">10</span><span class="n">ms</span>    <span class="c1"># 행렬 곱셈
</span><span class="n">Total</span> <span class="o">=</span> <span class="mi">25</span><span class="n">ms</span> <span class="n">per</span> <span class="n">block</span>

<span class="c1"># 파이프라인 타이밍
# 초기 설정 후 세 연산 모두 동시 실행
# 병목은 가장 느린 연산 (10ms)
</span><span class="n">Pipeline_time</span> <span class="o">=</span> <span class="mi">10</span><span class="n">ms</span> <span class="n">per</span> <span class="n">block</span>
<span class="n">Speedup</span> <span class="o">=</span> <span class="mi">25</span><span class="o">/</span><span class="mi">10</span> <span class="o">=</span> <span class="mf">2.5</span><span class="n">x</span> <span class="n">이론적</span>
</code></pre></div></div> <p><strong>내 통찰</strong>: 이는 CPU 명령어 파이프라이닝과 유사하지만 고수준 수학적 연산에 적용된 것입니다. 천재적인 점은 다른 연산들이 다른 하드웨어 유닛을 사용하므로 동시에 실행될 수 있다는 것을 인식한 것입니다.</p> <h3 id="혁신-3-정확도-보존을-가진-fp8">혁신 3: 정확도 보존을 가진 FP8</h3> <p><strong>도전</strong>: FP8은 2배 속도를 제공하지만 일반적으로 수치 정확도를 파괴합니다. 대형 모델들은 일반적으로 대부분의 다른 값들보다 크기가 훨씬 큰 이상치 값들을 가지고 있어 양자화를 어렵게 만듭니다.</p> <p><strong>두 부분 해결책</strong>:</p> <h4 id="파트-a-블록-양자화">파트 A: 블록 양자화</h4> <p><strong>Python 의사코드</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">block_quantization</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">전체 텐서 대신 블록 단위로 양자화</span><span class="sh">"""</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="n">shape</span>
    
    <span class="c1"># 문제: 텐서별 양자화
</span>    <span class="c1"># tensor_max = tensor.abs().max()  # 이상치에 의해 지배됨!
</span>    <span class="c1"># scale = tensor_max / 127
</span>    <span class="c1"># 결과: 대부분의 값이 0 또는 ±1이 됨
</span>    
    <span class="c1"># 해결책: 블록별 양자화
</span>    <span class="n">blocks</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
    
    <span class="c1"># 각 블록이 자체 스케일을 가짐
</span>    <span class="n">block_maxes</span> <span class="o">=</span> <span class="n">blocks</span><span class="p">.</span><span class="nf">abs</span><span class="p">().</span><span class="nf">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">scales</span> <span class="o">=</span> <span class="n">block_maxes</span> <span class="o">/</span> <span class="mf">127.0</span>
    
    <span class="c1"># 각 블록을 별도로 양자화
</span>    <span class="n">quantized_blocks</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">blocks</span> <span class="o">/</span> <span class="n">scales</span><span class="p">).</span><span class="nf">clamp</span><span class="p">(</span><span class="o">-</span><span class="mi">127</span><span class="p">,</span> <span class="mi">127</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">quantized_blocks</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">scales</span>

<span class="c1"># 이상치가 있는 예시
</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">10</span><span class="p">,</span> <span class="p">:</span><span class="mi">10</span><span class="p">]</span> <span class="o">=</span> <span class="mf">50.0</span>  <span class="c1"># 50배 더 큰 1% 이상치
</span>
<span class="c1"># 텐서별 양자화 (나쁨)
</span><span class="n">global_scale</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">abs</span><span class="p">().</span><span class="nf">max</span><span class="p">()</span> <span class="o">/</span> <span class="mi">127</span>  <span class="c1"># = 50/127 = 0.39
</span><span class="n">quantized_global</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">tensor</span> <span class="o">/</span> <span class="n">global_scale</span><span class="p">)</span>
<span class="c1"># 결과: 일반 값들이 0, ±1, ±2가 됨 (끔찍한 정밀도)
</span>
<span class="c1"># 블록 양자화 (좋음) 
</span><span class="n">quantized_blocks</span><span class="p">,</span> <span class="n">scales</span> <span class="o">=</span> <span class="nf">block_quantization</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="c1"># 결과: 일반 블록은 전체 ±127 범위 사용, 이상치 블록은 별도 스케일
</span></code></pre></div></div> <h4 id="파트-b-비일관성-처리">파트 B: 비일관성 처리</h4> <p><strong>Python 의사코드</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">incoherent_processing</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">랜덤 직교 변환을 사용하여 이상치 분산</span><span class="sh">"""</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># 랜덤 직교 행렬 M = D1 * Hadamard * D2 생성
</span>    <span class="c1"># 핵심 통찰: M이 직교이므로 MM^T = I
</span>    <span class="c1"># 따라서 (Q*M) @ (K*M)^T = Q @ M @ M^T @ K^T = Q @ K^T
</span>    <span class="c1"># 어텐션 출력은 변하지 않지만 이상치가 분산됨!
</span>    
    <span class="n">D1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">d</span><span class="p">,))</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># 랜덤 ±1
</span>    <span class="n">D2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">d</span><span class="p">,))</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
    
    <span class="k">def</span> <span class="nf">fast_hadamard_transform</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">O(d^2) 대신 O(d log d) 행렬 곱셈</span><span class="sh">"""</span>
        <span class="k">return</span> <span class="nf">hadamard_recursive</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Walsh-Hadamard 변환
</span>    
    <span class="c1"># Q와 K 모두 변환
</span>    <span class="n">Q_transformed</span> <span class="o">=</span> <span class="nf">fast_hadamard_transform</span><span class="p">(</span><span class="n">Q</span> <span class="o">*</span> <span class="n">D1</span><span class="p">)</span> <span class="o">*</span> <span class="n">D2</span>
    <span class="n">K_transformed</span> <span class="o">=</span> <span class="nf">fast_hadamard_transform</span><span class="p">(</span><span class="n">K</span> <span class="o">*</span> <span class="n">D1</span><span class="p">)</span> <span class="o">*</span> <span class="n">D2</span>
    
    <span class="k">return</span> <span class="n">Q_transformed</span><span class="p">,</span> <span class="n">K_transformed</span>

<span class="c1"># 이상치가 있을 때 작동하는 이유 - 예시
</span><span class="n">original_Q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">]])</span>  <span class="c1"># 하나의 거대한 이상치
# 변환 후: 각 요소가 원래 요소들의 합이 됨
# transformed_Q ≈ [[34, 34, 34]]  # 이상치가 모든 차원에 분산됨
# 양자화하기 훨씬 쉬워짐!
</span></code></pre></div></div> <p><strong>복권 비유</strong>:</p> <ul> <li><strong>문제</strong>: 한 사람이 100만원, 999명이 0원 (공정하게 표현하기 어려움)</li> <li><strong>해결책</strong>: 재분배하여 모든 사람이 1000원씩 (공정하게 표현하기 쉬움)</li> <li><strong>핵심</strong>: 총 가치는 변하지 않지만 분포가 더 균일함</li> </ul> <p><strong>내 평가</strong>: 이는 수학적으로 우아합니다 - 직교 행렬의 성질을 사용하여 최종 결과를 보존하면서 중간 연산을 양자화에 더 친화적으로 만듭니다. 수학적 손재주와 같습니다.</p> <h2 id="3-중요한-성공-조건">3. 중요한 성공 조건</h2> <h3 id="하드웨어-요구사항">하드웨어 요구사항</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">HardwareChecklist</span><span class="p">:</span>
    <span class="n">required_features</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">hopper_gpu</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Hopper 아키텍처를 가진 H100 이상</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">async_tensor_cores</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">WGMMA 명령어 지원</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">tensor_memory_accelerator</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">비동기 메모리 연산을 위한 TMA</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">fp8_support</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">E4M3 형식 텐서 코어</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">shared_memory</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">SM당 최소 228KB</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">register_flexibility</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">동적 레지스터 할당</span><span class="sh">'</span>
    <span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">check_compatibility</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># 실제로는 실제 하드웨어를 쿼리할 것
</span>        <span class="k">for</span> <span class="n">feature</span><span class="p">,</span> <span class="n">description</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">required_features</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">✓ </span><span class="si">{</span><span class="n">feature</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">description</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>내 분석</strong>: 이는 강점이자 제한사항입니다. 최적화들이 특정 하드웨어에 깊이 연결되어 H100에서는 믿을 수 없이 효과적이지만 다른 가속기로의 이식성은 떨어질 수 있습니다.</p> <h3 id="알고리즘-가정">알고리즘 가정</h3> <ol> <li> <p><strong>메모리 대역폭 균형</strong>: 생산자 워프가 메모리 대역폭을 포화시킬 수 있어야 함</p> <ul> <li><strong>함정</strong>: 데이터 로딩이 연산 대비 너무 빠르면 생산자가 유휴 상태가 됨</li> <li><strong>해결책</strong>: 블록 크기와 파이프라인 단계 수의 신중한 조정</li> </ul> </li> <li> <p><strong>파이프라인 단계 타이밍</strong>: GEMM과 소프트맥스 연산이 유사한 타이밍을 가져야 함</p> <ul> <li><strong>함정</strong>: 소프트맥스가 너무 빠르면 중첩 이득 없음; 너무 느리면 병목이 됨</li> <li><strong>성공</strong>: H100의 GEMM과 지수함수 간 256배 속도 차이가 이를 가능하게 함</li> </ul> </li> <li> <p><strong>컴파일러 협력</strong>: 컴파일러가 의도된 명령어 순서를 생성해야 함</p> <ul> <li><strong>함정</strong>: 컴파일러가 명령어를 재배열하여 파이프라인을 깨뜨릴 수 있음</li> <li><strong>해결책</strong>: 메모리 장벽과 인라인 어셈블리 힌트의 신중한 사용</li> </ul> </li> </ol> <h2 id="4-실험-결과-분석">4. 실험 결과 분석</h2> <h3 id="주요-성능-결과">주요 성능 결과</h3> <table> <thead> <tr> <th>구성</th> <th>FlashAttention-2</th> <th>FlashAttention-3</th> <th>가속비</th> <th>내 해석</th> </tr> </thead> <tbody> <tr> <td><strong>헤드차원 64, 16k seq</strong></td> <td>324 TFLOPs/s</td> <td>497 TFLOPs/s</td> <td><strong>1.53×</strong></td> <td>메모리 바운드 영역이 가장 많은 이득</td> </tr> <tr> <td><strong>헤드차원 128, 16k seq</strong></td> <td>370 TFLOPs/s</td> <td>648 TFLOPs/s</td> <td><strong>1.75×</strong></td> <td>최적화의 스위트 스팟</td> </tr> <tr> <td><strong>헤드차원 256, 16k seq</strong></td> <td>581 TFLOPs/s</td> <td>756 TFLOPs/s</td> <td><strong>1.30×</strong></td> <td>연산 바운드, 상대적 이득 적음</td> </tr> <tr> <td><strong>최고 활용률</strong></td> <td>35%</td> <td><strong>75%</strong></td> <td><strong>2.1×</strong></td> <td>하드웨어가 마침내 효과적으로 사용됨</td> </tr> </tbody> </table> <p><strong>내 분석</strong>: 결과는 논리적 패턴을 따릅니다 - 더 작은 헤드 차원은 더 메모리 바운드이므로 메모리 최적화가 더 도움이 됩니다. 75% 활용률은 놀랍습니다; 비교하자면, 복잡한 시스템에서 75% 효율성을 얻는 것은 뛰어납니다.</p> <h3 id="fp8-성능-돌파">FP8 성능 돌파</h3> <table> <thead> <tr> <th>정밀도</th> <th>최고 성능</th> <th>정확도 (RMSE)</th> <th>내 평가</th> </tr> </thead> <tbody> <tr> <td><strong>FP16 표준</strong></td> <td>139 TFLOPs/s</td> <td>3.2e-4</td> <td>기준 정확도</td> </tr> <tr> <td><strong>FP16 FlashAttention-3</strong></td> <td><strong>648 TFLOPs/s</strong></td> <td><strong>1.9e-4</strong></td> <td>더 빠른 속도 AND 정확도</td> </tr> <tr> <td><strong>FP8 표준</strong></td> <td>~800 TFLOPs/s</td> <td>2.4e-2</td> <td>빠르지만 사용할 수 없을 정도로 부정확</td> </tr> <tr> <td><strong>FP8 FlashAttention-3</strong></td> <td><strong>1008 TFLOPs/s</strong></td> <td><strong>9.1e-3</strong></td> <td>빠르고 사용 가능한 정확도</td> </tr> </tbody> </table> <p><strong>핵심 통찰</strong>: 표준 FP8은 FP16보다 75배 더 나쁜 정확도 - 완전히 사용 불가능합니다. FlashAttention-3의 FP8은 48배만 더 나쁨 - 여전히 완벽하지 않지만 많은 애플리케이션에서 잠재적으로 사용 가능합니다.</p> <h3 id="제거-연구-결과">제거 연구 결과</h3> <table> <thead> <tr> <th>구성</th> <th>시간 (ms)</th> <th>TFLOPs/s</th> <th>구성요소</th> </tr> </thead> <tbody> <tr> <td><strong>기준선</strong></td> <td>4.105</td> <td>570</td> <td>최적화 없음</td> </tr> <tr> <td><strong>+ 워프 특화</strong></td> <td>4.021</td> <td>582</td> <td>+2% 개선</td> </tr> <tr> <td><strong>+ 파이프라이닝만</strong></td> <td>4.105</td> <td>570</td> <td>단독으로는 개선 없음</td> </tr> <tr> <td><strong>+ 둘 다</strong></td> <td>3.538</td> <td><strong>661</strong></td> <td><strong>+16% 총합</strong></td> </tr> </tbody> </table> <p><strong>중요한 통찰</strong>: 이는 가산적이 아닌 <strong>시너지 효과</strong>를 보여줍니다. 파이프라이닝 단독으로는 0% 이득을 주는데, 이는 워프 특화를 기반으로 필요하기 때문입니다. 이는 시스템 최적화에서 흔한 일입니다 - 개별 기술들은 효과적이지 않아 보일 수 있지만, 조합은 변혁적일 수 있습니다.</p> <p><strong>내 해석</strong>: 이는 “시스템 사고” 접근법을 검증합니다. 저자들은 개별 구성요소만 최적화한 것이 아니라 전체 연산 파이프라인을 재설계했습니다. 7.6배 시너지 인수(16% 조합 vs 2% 개별)는 아키텍처 변경이 가산적이 아닌 곱셈적 이득을 어떻게 풀어낼 수 있는지 보여줍니다.</p> <h2 id="5-더-넓은-의미와-미래-임팩트">5. 더 넓은 의미와 미래 임팩트</h2> <h3 id="즉각적인-애플리케이션">즉각적인 애플리케이션</h3> <ul> <li><strong>장문맥 언어 모델</strong>: 전체 책, 코드베이스, 또는 대화 처리</li> <li><strong>고해상도 비전</strong>: 의료 이미지나 위성 데이터 분석</li> <li><strong>멀티모달 AI</strong>: 오디오와 텍스트가 있는 긴 비디오 처리</li> <li><strong>과학 컴퓨팅</strong>: 어텐션 메커니즘을 가진 분자 역학, 기후 모델링</li> </ul> <h3 id="아키텍처-교훈">아키텍처 교훈</h3> <ol> <li><strong>하드웨어-소프트웨어 공동 설계의 중요성</strong>: 범용 알고리즘은 성능을 탁자 위에 남겨둠</li> <li><strong>비동기성이 핵심</strong>: 현대 하드웨어는 병렬 실행을 위해 설계됨</li> <li><strong>정밀도는 협상 가능</strong>: 모든 연산이 완전한 정밀도를 필요로 하지 않음</li> <li><strong>시스템 최적화</strong>: 전체가 부분의 합보다 클 수 있음</li> </ol> <p><strong>내 최종 생각</strong>: FlashAttention-3는 분야의 성숙을 나타냅니다 - “작동하게 만들기”에서 “최적으로 작동하게 만들기”로의 이동입니다. 여기의 기술들은 미래 가속기를 위한 알고리즘 설계 방식에 영향을 줄 것 같습니다. 가장 중요한 것은, 어텐션 같은 잘 연구된 문제에서도 영리한 알고리즘 설계를 통해 상당한 성능 향상이 여전히 가능하다는 것을 보여준다는 점입니다.</p> <p>75% GPU 활용률 달성은 특히 주목할 만합니다 - 이는 우리가 마침내 비효율적인 알고리즘에 더 많은 하드웨어를 투입하는 대신, 우리가 구축한 강력한 하드웨어를 효과적으로 사용하는 방법을 배우고 있음을 시사합니다.</p> <hr/>]]></content><author><name></name></author><category term="attention"/><category term="memory-efficiency"/><category term="hardware-optimization"/><category term="inference"/><summary type="html"><![CDATA[FlashAttention-3 achieves 1.5-2x speedup on H100 GPUs via warp-specialization, WGMMA pipelining, and FP8 support.]]></summary></entry><entry><title type="html">Flash Attention</title><link href="https://sungyubkim.github.io/blog/fa/" rel="alternate" type="text/html" title="Flash Attention"/><published>2025-06-03T00:00:00+09:00</published><updated>2025-06-03T00:00:00+09:00</updated><id>https://sungyubkim.github.io/blog/fa</id><content type="html" xml:base="https://sungyubkim.github.io/blog/fa/"><![CDATA[<h1 id="tldr">TL;DR</h1> <blockquote> <p><strong>FlashAttention</strong>은 표준 구현보다 2-3배 빠르면서 10-20배 적은 메모리를 사용하는 IO-aware 어텐션 알고리즘입니다. GPU 메모리에 거대한 N×N 어텐션 행렬을 만드는 대신, <strong>타일링(tiling)</strong>과 <strong>온라인 소프트맥스(online softmax)</strong>를 사용하여 빠른 온칩 SRAM에 맞는 작은 블록 단위로 어텐션을 처리합니다. 주요 기여사항:</p> <ol> <li><strong>IO-Awareness</strong>: 어텐션의 실제 병목이 연산이 아닌 메모리 대역폭임을 파악</li> <li><strong>타일링 알고리즘</strong>: 느린 메모리 접근을 최소화하기 위해 블록 단위로 처리</li> <li><strong>정확한 계산</strong>: 근사 방법과 달리 정확한 어텐션을 계산하면서도 더 빠른 속도 달성</li> <li><strong>선형 메모리</strong>: 메모리 복잡도를 O(N²)에서 O(N)으로 줄여 16K+ 시퀀스 길이 가능</li> <li><strong>실용적 성과</strong>: MLPerf BERT 기록보다 15% 빠름, GPT-2 학습 3배 가속, Path-X/Path-256 최초 해결</li> </ol> </blockquote> <ul> <li><a href="https://arxiv.org/pdf/2205.14135">Paper Link</a></li> </ul> <hr/> <p><strong>FlashAttention 시리즈 발전:</strong></p> <ul> <li><a href="/blog/fa2/">FlashAttention-2</a> - 2배 빠른 속도와 향상된 병렬화</li> <li><a href="/blog/fa3/">FlashAttention-3</a> - 비대칭 어텐션과 FP8 지원</li> <li><a href="https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf">From Online Softmax to FlashAttention</a> - 이론적 기초와 온라인 소프트맥스</li> </ul> <p><strong>긴 시퀀스 처리:</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2411.01783">Context Parallelism for Scalable Million-Token Inference</a> - 추론 시 컨텍스트 분산</li> <li><a href="https://arxiv.org/pdf/2310.05209">Scaling Laws of RoPE-based Extrapolation</a> - 위치 인코딩 확장</li> <li><a href="https://arxiv.org/pdf/2309.00071">YaRN</a> - RoPE 기반 컨텍스트 길이 확장</li> <li><a href="https://arxiv.org/pdf/2104.09864">RoFormer</a> - 회전 위치 임베딩</li> </ul> <p><strong>시스템 최적화:</strong></p> <ul> <li><a href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> - 메모리 효율적인 병렬 훈련</li> <li><a href="/blog/usp/">USP</a> - 통합 시퀀스 병렬화 프레임워크</li> <li><a href="/blog/tp/">Tensor Parallelism</a> - 텐서 병렬화와의 결합</li> <li><a href="/blog/pp/">GPipe</a> - 파이프라인 병렬화와의 통합</li> </ul> <hr/> <h1 id="takeaways">Takeaways</h1> <h2 id="1-문제-표준-어텐션이-느린-이유">1. 문제: 표준 어텐션이 느린 이유</h2> <h3 id="메모리-계층-구조의-불일치">메모리 계층 구조의 불일치</h3> <p>이 논문은 <strong>대부분의 ML 연구자들이 놓치는</strong> 중요한 관찰로 시작합니다: 현대 GPU는 어텐션에서 연산 제한(compute-bound)이 아닌 메모리 제한(memory-bound) 상태입니다. 그 이유는:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 표준 어텐션 - 실제로 일어나는 일
</span><span class="k">def</span> <span class="nf">standard_attention_reality</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Q, K, V: [batch, seq_len, dim]
    실제로 시간이 소요되는 부분을 보여줍니다
    </span><span class="sh">"""</span>
    <span class="c1"># 단계 1: 점수 계산 - 빠름 (텐서 코어 사용)
</span>    <span class="n">S</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="p">.</span><span class="n">T</span>  <span class="c1"># [seq_len, seq_len] - 하지만 느린 HBM에 써야 함!
</span>    
    <span class="c1"># 단계 2: 소프트맥스 - 느림 (메모리 제한)
</span>    <span class="c1"># 전체 S를 HBM에서 읽고, exp() 계산하고, 다시 쓰기
</span>    <span class="n">P</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>  <span class="c1"># seq_len² 개의 원소를 읽고/쓰기
</span>    
    <span class="c1"># 단계 3: 가중 합 - 연산은 빠르지만 메모리는 느림
</span>    <span class="n">O</span> <span class="o">=</span> <span class="n">P</span> <span class="o">@</span> <span class="n">V</span>  <span class="c1"># 거대한 P 행렬을 HBM에서 읽어야 함
</span>    
    <span class="c1"># 총 HBM 접근: O(seq_len²) - 이것이 병목!
</span></code></pre></div></div> <p><strong>제 통찰</strong>: 논문은 우리가 잘못된 것을 최적화하고 있다는 점을 훌륭하게 파악했습니다. 모두가 FLOPs(부동소수점 연산)를 줄이는 데 집중하는 동안, 실제 문제는 데이터 이동입니다. A100 GPU에서:</p> <ul> <li>행렬 곱셈: 312 TFLOPS</li> <li>메모리 대역폭: 1.5 TB/s</li> <li>어텐션의 경우, 312 TFLOPS가 아닌 1.5 TB/s에 제한됩니다!</li> </ul> <h3 id="동기-부여-실험">동기 부여 실험</h3> <p>저자들은 아마 다음과 같은 실험으로 연구 동기를 얻었을 것입니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 표준 어텐션 프로파일링으로 병목 지점 찾기
</span><span class="k">def</span> <span class="nf">profile_attention</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
    <span class="c1"># 각 구성요소에서 소요되는 시간
</span>    <span class="n">compute_time</span> <span class="o">=</span> <span class="n">seq_len</span><span class="err">²</span> <span class="o">*</span> <span class="n">time_per_flop</span>
    <span class="n">memory_time</span> <span class="o">=</span> <span class="n">seq_len</span><span class="err">²</span> <span class="o">*</span> <span class="n">matrix_size</span> <span class="o">*</span> <span class="n">time_per_byte_transferred</span>
    
    <span class="c1"># A100에서 seq_len=2048인 경우:
</span>    <span class="c1"># 연산: ~0.5ms (활용도 낮음)
</span>    <span class="c1"># 메모리: ~10ms (병목!)
</span>    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">연산 활용도: </span><span class="si">{</span><span class="n">compute_time</span><span class="o">/</span><span class="n">total_time</span> <span class="o">*</span> <span class="mi">100</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># ~5%
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">메모리 제한 시간: </span><span class="si">{</span><span class="n">memory_time</span><span class="o">/</span><span class="n">total_time</span> <span class="o">*</span> <span class="mi">100</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>     <span class="c1"># ~95%
</span></code></pre></div></div> <h2 id="2-해결책-flashattention-알고리즘">2. 해결책: FlashAttention 알고리즘</h2> <h3 id="핵심-아이디어-빠른-메모리에서-작업하기">핵심 아이디어: 빠른 메모리에서 작업하기</h3> <p>핵심 통찰은 데이터를 가능한 한 SRAM(빠른 온칩 메모리)에 유지하는 것입니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GPU 메모리 계층 구조 (A100 예시)
</span><span class="n">SRAM_per_SM</span> <span class="o">=</span> <span class="mi">192</span> <span class="o">*</span> <span class="mi">1024</span>  <span class="c1"># 192 KB - 빠름 (19 TB/s)
</span><span class="n">HBM_TOTAL</span> <span class="o">=</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>   <span class="c1"># 40 GB - 느림 (1.5 TB/s)
</span>
<span class="c1"># 표준 어텐션은 N×N 행렬을 위해 HBM을 사용해야 함
# FlashAttention은 타일링을 사용해 모든 것을 SRAM에 유지!
</span></code></pre></div></div> <h3 id="flashattention-알고리즘">FlashAttention 알고리즘</h3> <p>구체적인 예시와 함께 알고리즘을 설명합니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">flash_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    상세한 예시와 함께하는 FlashAttention
    Q, K, V: [1024, 64] (seq_len=1024, dim=64)
    block_size: 128 (128×128 타일 처리)
    </span><span class="sh">"""</span>
    <span class="n">seq_len</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="n">block_size</span>  <span class="c1"># 1024/128 = 8 블록
</span>    
    <span class="c1"># 출력과 통계 초기화
</span>    <span class="n">O</span> <span class="o">=</span> <span class="nf">zeros</span><span class="p">([</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nf">full</span><span class="p">([</span><span class="n">seq_len</span><span class="p">],</span> <span class="o">-</span><span class="n">inf</span><span class="p">)</span>  <span class="c1"># 수치 안정성을 위한 최댓값
</span>    <span class="n">l</span> <span class="o">=</span> <span class="nf">zeros</span><span class="p">([</span><span class="n">seq_len</span><span class="p">])</span>       <span class="c1"># 소프트맥스를 위한 지수 합
</span>    
    <span class="c1"># 블록 단위로 처리 - 이것이 핵심!
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>  <span class="c1"># i = 0,1,...,7
</span>        <span class="c1"># Q 블록 [128, 64]를 SRAM으로 로드
</span>        <span class="n">Q_block</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span>
        
        <span class="c1"># 블록 통계 초기화
</span>        <span class="n">m_block</span> <span class="o">=</span> <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span>
        <span class="n">l_block</span> <span class="o">=</span> <span class="n">l</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span>
        <span class="n">O_block</span> <span class="o">=</span> <span class="n">O</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span>
        
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>  <span class="c1"># j = 0,1,...,7
</span>            <span class="c1"># K,V 블록 [128, 64]를 SRAM으로 로드
</span>            <span class="n">K_block</span> <span class="o">=</span> <span class="n">K</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span>
            <span class="n">V_block</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span>
            
            <span class="c1"># ---- 모든 계산이 SRAM에서 발생! ----
</span>            
            <span class="c1"># 1. 블록 어텐션 점수 계산 [128, 128]
</span>            <span class="n">S_block</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q_block</span> <span class="o">@</span> <span class="n">K_block</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
            
            <span class="c1"># 2. 온라인 소프트맥스 - 실행 중 통계 업데이트
</span>            <span class="n">m_block_new</span> <span class="o">=</span> <span class="nf">maximum</span><span class="p">(</span><span class="n">m_block</span><span class="p">,</span> <span class="n">S_block</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
            
            <span class="c1"># 3. 새 최댓값으로 지수 계산 (수치 안정성)
</span>            <span class="n">P_block</span> <span class="o">=</span> <span class="nf">exp</span><span class="p">(</span><span class="n">S_block</span> <span class="o">-</span> <span class="n">m_block_new</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
            
            <span class="c1"># 4. 지수 합 업데이트
</span>            <span class="n">l_block_new</span> <span class="o">=</span> <span class="nf">exp</span><span class="p">(</span><span class="n">m_block</span> <span class="o">-</span> <span class="n">m_block_new</span><span class="p">)</span> <span class="o">*</span> <span class="n">l_block</span> <span class="o">+</span> <span class="n">P_block</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># 5. 블록 출력 계산
</span>            <span class="n">O_block_new</span> <span class="o">=</span> <span class="n">P_block</span> <span class="o">@</span> <span class="n">V_block</span>
            
            <span class="c1"># 6. 이전 출력 재조정 후 새 출력 추가
</span>            <span class="n">O_block</span> <span class="o">=</span> <span class="p">(</span><span class="nf">exp</span><span class="p">(</span><span class="n">m_block</span> <span class="o">-</span> <span class="n">m_block_new</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">l_block</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">O_block</span> <span class="o">+</span> 
                      <span class="n">O_block_new</span><span class="p">)</span> <span class="o">/</span> <span class="n">l_block_new</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># 통계 업데이트
</span>            <span class="n">m_block</span> <span class="o">=</span> <span class="n">m_block_new</span>
            <span class="n">l_block</span> <span class="o">=</span> <span class="n">l_block_new</span>
        
        <span class="c1"># 최종 결과를 HBM에 쓰기 (블록당 한 번만!)
</span>        <span class="n">O</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">O_block</span>
        <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">m_block</span>
        <span class="n">l</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">l_block</span>
    
    <span class="k">return</span> <span class="n">O</span>
</code></pre></div></div> <h3 id="구체적-예시-하나의-블록-처리">구체적 예시: 하나의 블록 처리</h3> <p>특정 블록에서 일어나는 일을 추적해보겠습니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 예시: Q[0:128]이 K[256:384]에 주목하는 처리
# 1024 길이 시퀀스에서 블록 (i=0, j=2)
</span>
<span class="c1"># 이 블록 이전 상태:
# - O[0:128]은 K[0:256]으로부터의 어텐션 결과 포함
# - m[0:128] = [0.82, 0.79, ...] (지금까지의 최대 점수)
# - l[0:128] = [45.2, 52.1, ...] (지금까지의 지수 합)
</span>
<span class="c1"># 단계 1: 새 점수 계산
</span><span class="n">S_block</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">128</span><span class="p">]</span> <span class="o">@</span> <span class="n">K</span><span class="p">[</span><span class="mi">256</span><span class="p">:</span><span class="mi">384</span><span class="p">].</span><span class="n">T</span> <span class="o">/</span> <span class="mi">8</span>  <span class="c1"># [128, 128] 행렬
# 예시 값: [[0.71, 0.65, ...], [0.73, 0.68, ...], ...]
</span>
<span class="c1"># 단계 2: 최댓값 업데이트 (수치 안정성을 위해)
</span><span class="n">m_new</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.82</span><span class="p">,</span> <span class="mf">0.79</span><span class="p">,</span> <span class="p">...]</span>  <span class="c1"># 이전 최댓값이 여전히 더 큼
</span>
<span class="c1"># 단계 3: 이 블록의 어텐션 가중치 계산
</span><span class="n">P_block</span> <span class="o">=</span> <span class="nf">exp</span><span class="p">(</span><span class="n">S_block</span> <span class="o">-</span> <span class="n">m_new</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="c1"># 값: [[0.89, 0.84, ...], [0.91, 0.87, ...], ...]
</span>
<span class="c1"># 단계 4: 지수 합 업데이트
</span><span class="n">l_new</span> <span class="o">=</span> <span class="n">l</span> <span class="o">*</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">P_block</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 재조정 불필요
# 새 값: [67.3, 73.5, ...]
</span>
<span class="c1"># 단계 5: 가중 값 누적
</span><span class="n">O_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">l</span> <span class="o">*</span> <span class="n">O</span> <span class="o">+</span> <span class="n">P_block</span> <span class="o">@</span> <span class="n">V</span><span class="p">[</span><span class="mi">256</span><span class="p">:</span><span class="mi">384</span><span class="p">])</span> <span class="o">/</span> <span class="n">l_new</span>
<span class="c1"># 이전과 새로운 기여를 올바르게 가중!
</span></code></pre></div></div> <p><strong>제 통찰</strong>: 아름다운 점은 시퀀스 길이와 관계없이 언제나 128×128 행렬만 SRAM에 유지한다는 것입니다!</p> <h2 id="3-중요한-가정과-조건">3. 중요한 가정과 조건</h2> <h3 id="하드웨어-요구사항">하드웨어 요구사항</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">check_flashattention_viability</span><span class="p">(</span><span class="n">gpu_specs</span><span class="p">,</span> <span class="n">model_config</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    FlashAttention은 특정 하드웨어 속성이 필요합니다
    </span><span class="sh">"""</span>
    <span class="c1"># 1. SM당 충분한 SRAM
</span>    <span class="n">sram_needed</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">dim</span> <span class="o">+</span>  <span class="c1"># Q, K, V 블록
</span>                   <span class="n">block_size</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">+</span>  <span class="c1"># S 블록  
</span>                   <span class="n">block_size</span> <span class="o">*</span> <span class="n">dim</span><span class="p">)</span>          <span class="c1"># O 블록
</span>    
    <span class="k">if</span> <span class="n">sram_needed</span> <span class="o">&gt;</span> <span class="n">gpu_specs</span><span class="p">.</span><span class="n">sram_per_sm</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">❌ 블록 크기가 SRAM에 비해 너무 큼</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">False</span>
    
    <span class="c1"># 2. 높은 메모리 대역폭 비율
</span>    <span class="n">compute_to_memory_ratio</span> <span class="o">=</span> <span class="n">gpu_specs</span><span class="p">.</span><span class="n">tflops</span> <span class="o">/</span> <span class="n">gpu_specs</span><span class="p">.</span><span class="n">memory_bandwidth</span>
    <span class="k">if</span> <span class="n">compute_to_memory_ratio</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="p">:</span>  <span class="c1"># 대략적인 임계값
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">⚠️  GPU가 메모리 제한이 아닌 연산 제한일 수 있음</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># 3. 효율적인 행렬 곱셈 유닛
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">gpu_specs</span><span class="p">.</span><span class="n">has_tensor_cores</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">⚠️  텐서 코어 없이는 성능 향상 감소</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="bp">True</span>

<span class="c1"># 예시 확인
</span><span class="n">a100_specs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">sram_per_sm</span><span class="sh">'</span><span class="p">:</span> <span class="mi">192</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">,</span>      <span class="c1"># 192 KB
</span>    <span class="sh">'</span><span class="s">tflops</span><span class="sh">'</span><span class="p">:</span> <span class="mi">312</span><span class="p">,</span>                   <span class="c1"># FP16
</span>    <span class="sh">'</span><span class="s">memory_bandwidth</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1.5</span><span class="p">,</span>         <span class="c1"># TB/s
</span>    <span class="sh">'</span><span class="s">has_tensor_cores</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span>
<span class="p">}</span>
<span class="nf">check_flashattention_viability</span><span class="p">(</span><span class="n">a100_specs</span><span class="p">,</span> <span class="p">{</span><span class="sh">'</span><span class="s">block_size</span><span class="sh">'</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="sh">'</span><span class="s">dim</span><span class="sh">'</span><span class="p">:</span> <span class="mi">64</span><span class="p">})</span>
</code></pre></div></div> <h3 id="알고리즘적-가정">알고리즘적 가정</h3> <p>FlashAttention이 가장 잘 작동하는 경우에 대한 <strong>제 분석</strong>:</p> <ol> <li><strong>메모리 제한 워크로드</strong>: 시퀀스 길이 &gt; 512</li> <li><strong>표준 어텐션 패턴</strong>: 커스텀 어텐션 마스크와는 효율적으로 작동하지 않음</li> <li><strong>수치 정밀도</strong>: FP16/BF16 권장 (FP32는 SRAM 효율성 감소)</li> <li><strong>배치 크기 유연성</strong>: 시퀀스 길이 차원으로 병렬화 불가</li> </ol> <h2 id="4-실험-결과-분석">4. 실험 결과 분석</h2> <h3 id="주요-성능-결과">주요 성능 결과</h3> <table> <thead> <tr> <th><strong>모델</strong></th> <th><strong>설정</strong></th> <th><strong>기준선</strong></th> <th><strong>FlashAttention</strong></th> <th><strong>제 해석</strong></th> </tr> </thead> <tbody> <tr> <td>BERT-large</td> <td>MLPerf 1.1</td> <td>100% (기준)</td> <td><strong>115% (1.15배)</strong></td> <td>MLPerf가 이미 고도로 최적화된 점을 고려하면 인상적</td> </tr> <tr> <td>GPT-2</td> <td>HuggingFace</td> <td>100%</td> <td><strong>300% (3.0배)</strong></td> <td>덜 최적화된 구현에서의 가치를 보여줌</td> </tr> <tr> <td>GPT-2</td> <td>Megatron-LM</td> <td>100%</td> <td><strong>180% (1.8배)</strong></td> <td>최적화된 코드에서도 여전히 큰 향상</td> </tr> <tr> <td>Long Range Arena</td> <td>Seq 1K-4K</td> <td>100%</td> <td><strong>240% (2.4배)</strong></td> <td>예상대로 시퀀스 길이가 길수록 향상 증가</td> </tr> </tbody> </table> <p><strong>제 견해</strong>: 결과는 메모리 제한 가설을 검증합니다. 더 긴 시퀀스에서 더 큰 속도 향상은 O(N²) 메모리 접근이 병목임을 확인합니다.</p> <h3 id="메모리-효율성-결과">메모리 효율성 결과</h3> <table> <thead> <tr> <th><strong>시퀀스 길이</strong></th> <th><strong>표준 메모리</strong></th> <th><strong>FlashAttention 메모리</strong></th> <th><strong>절약</strong></th> </tr> </thead> <tbody> <tr> <td>512</td> <td>1 GB</td> <td>200 MB</td> <td>5배</td> </tr> <tr> <td>2,048</td> <td>16 GB</td> <td>1.6 GB</td> <td>10배</td> </tr> <tr> <td>8,192</td> <td>256 GB</td> <td>6.4 GB</td> <td>40배</td> </tr> <tr> <td>16,384</td> <td>1 TB (OOM)</td> <td>25.6 GB</td> <td>새로운 기능 가능</td> </tr> </tbody> </table> <p><strong>제 분석</strong>: 선형 대 이차 스케일링은 혁신적입니다. 이것은 단순한 최적화가 아니라 가능한 것을 근본적으로 바꿉니다.</p> <h3 id="품질-개선">품질 개선</h3> <table> <thead> <tr> <th><strong>작업</strong></th> <th><strong>지표</strong></th> <th><strong>결과</strong></th> <th><strong>제 해석</strong></th> </tr> </thead> <tbody> <tr> <td>GPT-2 (4K vs 1K 컨텍스트)</td> <td>Perplexity</td> <td>-0.7 개선</td> <td>더 긴 컨텍스트 = 더 나은 언어 이해</td> </tr> <tr> <td>문서 분류</td> <td>F1 점수</td> <td>+6.4 포인트</td> <td>전체 문서를 보는 것으로 큰 향상</td> </tr> <tr> <td>Path-X (16K)</td> <td>정확도</td> <td>61.4% (랜덤 50% 대비)</td> <td>이를 해결한 <strong>최초</strong> 사례!</td> </tr> <tr> <td>Path-256 (64K)</td> <td>정확도</td> <td>63.1% (랜덤 50% 대비)</td> <td>시퀀스 모델링의 한계를 넓힘</td> </tr> </tbody> </table> <p><strong>제 통찰</strong>: 이것들은 단순한 벤치마크 개선이 아니라 새로운 능력을 나타냅니다. Path-X는 트랜스포머에게 불가능하다고 여겨졌습니다!</p> <h2 id="5-절제-연구-무엇이-정말-중요한가">5. 절제 연구: 무엇이 정말 중요한가?</h2> <h3 id="구성요소-기여도">구성요소 기여도</h3> <table> <thead> <tr> <th><strong>구성요소</strong></th> <th><strong>없을 때</strong></th> <th><strong>있을 때</strong></th> <th><strong>영향</strong></th> <th><strong>제 분석</strong></th> </tr> </thead> <tbody> <tr> <td>타일링</td> <td>1.0배</td> <td>1.4배</td> <td>+40%</td> <td>핵심 혁신 - 데이터를 SRAM에 유지</td> </tr> <tr> <td>온라인 소프트맥스</td> <td>1.4배</td> <td>1.75배</td> <td>+25%</td> <td>데이터를 한 번만 통과 가능</td> </tr> <tr> <td>재계산</td> <td>1.75배</td> <td>2.1배</td> <td>+20%</td> <td>N×N 어텐션 행렬 저장 회피</td> </tr> <tr> <td>커널 융합</td> <td>2.1배</td> <td>2.4배</td> <td>+15%</td> <td>오버헤드 감소, SRAM 사용 최대화</td> </tr> </tbody> </table> <p><strong>제 견해</strong>: 각 구성요소는 필요하지만 충분하지 않습니다. 이들 간의 시너지가 FlashAttention을 특별하게 만듭니다.</p> <h3 id="블록-크기-민감도">블록 크기 민감도</h3> <table> <thead> <tr> <th><strong>블록 크기</strong></th> <th><strong>성능</strong></th> <th><strong>메모리 사용</strong></th> <th><strong>제 분석</strong></th> </tr> </thead> <tbody> <tr> <td>32</td> <td>85%</td> <td>최소</td> <td>너무 많은 HBM 접근이 목적을 무너뜨림</td> </tr> <tr> <td>64</td> <td>95%</td> <td>낮음</td> <td>작은 모델/차원에 좋음</td> </tr> <tr> <td>128</td> <td>100% (최고)</td> <td>최적</td> <td>대부분 GPU의 스위트 스팟</td> </tr> <tr> <td>256</td> <td>98%</td> <td>높음</td> <td>SRAM 제한이 나타남</td> </tr> </tbody> </table> <p><strong>제 통찰</strong>: 블록 크기는 중요하고 하드웨어 의존적입니다. 저자들은 128이 다양한 GPU에서 잘 작동한다는 것을 발견했지만, 조정이 필요합니다.</p> <h2 id="6-실용적-의미">6. 실용적 의미</h2> <h3 id="실무자들에게-의미하는-것">실무자들에게 의미하는 것</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># FlashAttention 이전
</span><span class="k">def</span> <span class="nf">train_gpt_traditional</span><span class="p">(</span><span class="n">seq_len</span><span class="o">=</span><span class="mi">2048</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">seq_len</span> <span class="o">&gt;</span> <span class="mi">2048</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">OOMError</span><span class="p">(</span><span class="sh">"</span><span class="s">어텐션 행렬을 메모리에 맞출 수 없음</span><span class="sh">"</span><span class="p">)</span>
    <span class="c1"># 느린 학습, 제한된 컨텍스트
</span>
<span class="c1"># FlashAttention 이후  
</span><span class="k">def</span> <span class="nf">train_gpt_flash</span><span class="p">(</span><span class="n">seq_len</span><span class="o">=</span><span class="mi">16384</span><span class="p">):</span>
    <span class="c1"># 그냥 작동합니다! 그리고 더 빠르게도
</span>    <span class="n">model</span> <span class="o">=</span> <span class="nc">GPT</span><span class="p">(</span><span class="n">seq_len</span><span class="o">=</span><span class="mi">16384</span><span class="p">,</span> <span class="n">use_flash_attention</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># 8배 더 긴 컨텍스트, 3배 빠른 학습
</span></code></pre></div></div> <h3 id="미래-방향-제-생각">미래 방향 (제 생각)</h3> <ol> <li><strong>하드웨어 공동 설계</strong>: 미래 GPU는 SRAM을 늘려 FlashAttention을 더 효과적으로 만들 수 있음</li> <li><strong>희소 패턴</strong>: 블록 희소 FlashAttention은 100K+ 시퀀스에 대한 가능성을 보여줌</li> <li><strong>다른 연산</strong>: IO-aware 원칙은 다른 메모리 제한 연산에도 적용 가능</li> <li><strong>추론 최적화</strong>: 현재 작업은 학습에 초점을 맞추고 있지만, 추론은 다른 패턴을 가짐</li> </ol> <h2 id="결론">결론</h2> <p>FlashAttention은 시스템 사고의 걸작입니다. FLOPs 최적화에 몰려드는 대신, 저자들은 실제 병목(메모리 대역폭)을 파악하고 하드웨어 제약을 존중하는 알고리즘을 설계했습니다. 결과는 단순히 더 빠른 어텐션이 아니라 언어 모델링에서 완전히 새로운 능력을 가능하게 합니다.</p> <p><strong>핵심 교훈</strong>: 때로는 최고의 최적화는 더 적은 작업을 하는 것이 아니라, 기본 하드웨어에 대해 같은 작업을 더 지능적으로 수행하는 것에서 나옵니다.</p> <hr/>]]></content><author><name></name></author><category term="attention"/><category term="flash-attention"/><category term="memory-efficiency"/><category term="inference"/><category term="hardware-optimization"/><summary type="html"><![CDATA[FlashAttention: IO-aware attention algorithm achieving 2-3x speedup with 10-20x memory reduction via tiling and online softmax.]]></summary></entry><entry><title type="html">Flash Attention 2</title><link href="https://sungyubkim.github.io/blog/fa2/" rel="alternate" type="text/html" title="Flash Attention 2"/><published>2025-06-03T00:00:00+09:00</published><updated>2025-06-03T00:00:00+09:00</updated><id>https://sungyubkim.github.io/blog/fa2</id><content type="html" xml:base="https://sungyubkim.github.io/blog/fa2/"><![CDATA[<h1 id="tldr">TL;DR</h1> <blockquote> <p><strong>이 논문은 무엇에 관한 것인가?</strong> FlashAttention-2는 AI 모델(특히 ChatGPT와 같은 대형 언어 모델)을 <strong>2배 더 빠르게</strong> 훈련하고 실행하면서 <strong>메모리 사용량을 획기적으로 줄이는</strong> 돌파구 알고리즘입니다. 마치 거대한 도서관에서 책을 찾는 더 효율적인 방법을 찾는 것과 같습니다 - 모든 책을 개별적으로 확인하는 대신, 작업 메모리에 맞는 스마트한 청크로 검색을 구성하는 방식입니다.</p> <p><strong>핵심 기여점:</strong></p> <ol> <li><strong>성능 향상</strong>: 이전 FlashAttention보다 2배, 표준 방법보다 최대 9배 빠름</li> <li><strong>하드웨어 최적화</strong>: GPU 이론적 최대치의 73% 달성 (이전 25-40%에서 향상)</li> <li><strong>메모리 효율성</strong>: 긴 시퀀스에서 42배 메모리 사용량 감소 가능</li> <li><strong>경제적 영향</strong>: 대형 모델 훈련 비용을 90% 절감 (GPT-3 규모의 경우 460만 달러에서 45만 달러로)</li> <li><strong>세 가지 스마트 최적화</strong>: <ul> <li>비용이 많이 드는 비행렬 연산 감소</li> <li>GPU 코어 전반의 향상된 병렬화</li> <li>프로세서 간 낭비적인 메모리 통신 제거</li> </ul> </li> </ol> <p><strong>이것이 중요한 이유:</strong> 이 작업은 계산 비용과 메모리 요구사항을 획기적으로 줄여 더 많은 연구자와 회사가 고급 AI에 접근할 수 있게 만듭니다. 마차에서 스포츠카로 업그레이드하는 것과 같습니다 - 같은 목적지이지만 훨씬 더 효율적입니다.</p> </blockquote> <ul> <li><a href="https://arxiv.org/pdf/2307.08691">Paper Link</a></li> </ul> <hr/> <h1 id="related-papers">Related Papers</h1> <p><strong>FlashAttention 시리즈:</strong></p> <ul> <li><a href="/blog/fa/">FlashAttention</a> - 원조 IO-aware 어텐션 알고리즘과 타일링 기법의 기초</li> <li><a href="/blog/fa3/">FlashAttention-3</a> - 비대칭 어텐션과 FP8 지원으로 한층 발전된 버전</li> <li><a href="https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf">From Online Softmax to FlashAttention</a> - 온라인 소프트맥스의 이론적 기초와 수치 안정성</li> </ul> <p><strong>하드웨어 최적화 및 시스템 연구:</strong></p> <ul> <li><a href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> - 메모리 효율적인 병렬 훈련 기법</li> <li><a href="/blog/usp/">USP</a> - 통합 시퀀스 병렬화 프레임워크와의 상호작용</li> <li><a href="/blog/tp/">Tensor Parallelism</a> - 텐서 병렬화와 FlashAttention의 결합 전략</li> <li><a href="/blog/pp/">GPipe</a> - 파이프라인 병렬화와의 통합 방법론</li> </ul> <hr/> <h1 id="takeaways">Takeaways</h1> <h3 id="1-동기-왜-이-연구가-필요했는가">1. 동기: 왜 이 연구가 필요했는가?</h3> <p><strong>핵심 문제: 어텐션은 비싸다</strong></p> <p>방대한 도서관에서 모든 책 쌍 사이의 연결을 찾아야 하는 사서라고 상상해보세요. 1,000권의 책이 있다면 1,000,000번의 비교(1,000²)가 필요합니다. 10,000권이라면 100,000,000번의 비교가 필요합니다. 이것이 바로 AI 어텐션 메커니즘에서 일어나는 일입니다 - 계산 비용이 입력 길이의 제곱에 비례하여 증가합니다.</p> <p><strong>FlashAttention-2 이전의 실제 영향:</strong></p> <ul> <li><strong>메모리 위기</strong>: 4K 단어 시퀀스의 표준 어텐션에는 67GB 메모리가 필요하지만, 대부분의 GPU는 총 40-80GB만 가지고 있음</li> <li><strong>속도 병목</strong>: 어텐션 연산은 GPU 능력의 25-40%만 활용하고 있었음</li> <li><strong>경제적 장벽</strong>: 대형 모델 훈련에 수백만 달러가 소요되어 AI 연구를 거대 기술 기업으로 제한</li> </ul> <p><strong>저자들의 핵심 통찰:</strong> 이미 주요 돌파구였던 FlashAttention-1조차도 현대 GPU 아키텍처를 완전히 활용하지 못하고 있어 여전히 비효율적이라는 것을 깨달았습니다. 포뮬러 1 자동차를 가지고 있지만 시내 교통에서만 사용하는 것과 같습니다 - 전체 잠재력을 활용하지 못하고 있었습니다.</p> <h3 id="2-기술적-분석-병목-현상-이해하기">2. 기술적 분석: 병목 현상 이해하기</h3> <p><strong>프로파일링으로 드러난 세 가지 핵심 문제:</strong></p> <ol> <li> <p><strong>비행렬 연산이 16배 더 비싸다</strong></p> <ul> <li>현대 GPU는 행렬 곱셈에 최적화된 특수 “텐서 코어”를 가지고 있음</li> <li>A100 GPU: 행렬 연산 312 TFLOPs/s vs 기타 연산 19.5 TFLOPs/s</li> <li>FlashAttention-1은 비싼 비행렬 연산을 너무 많이 수행하고 있었음</li> </ul> </li> <li> <p><strong>긴 시퀀스에 대한 GPU 활용도 부족</strong></p> <ul> <li>GPU는 108개의 스트리밍 멀티프로세서가 모두 바쁠 때 최고 성능 발휘</li> <li>긴 시퀀스는 종종 작은 배치 크기를 의미하여 많은 프로세서가 유휴 상태</li> <li>108차선 고속도로가 있지만 16차선만 사용하는 것과 같음</li> </ul> </li> <li> <p><strong>낭비적인 메모리 통신</strong></p> <ul> <li>GPU 프로세서들이 느린 공유 메모리를 통해 끊임없이 서로 통신</li> <li>작업자들이 정보를 공유하기 위해 공장 바닥을 계속 걸어다니는 것과 같음</li> </ul> </li> </ol> <h3 id="3-핵심-가정과-성공-조건">3. 핵심 가정과 성공 조건</h3> <p><strong>FlashAttention-2가 작동하기 위한 중요한 요구사항:</strong></p> <p><strong>하드웨어 가정:</strong></p> <ul> <li><strong>현대 GPU 아키텍처</strong>: 텐서 코어가 있는 Ampere/Hopper GPU(A100, H100) 필요</li> <li><strong>충분한 고속 메모리</strong>: 적절한 SRAM 필요 (A100에서 스트리밍 멀티프로세서당 192KB)</li> <li><strong>높은 메모리 대역폭</strong>: 최적 성능을 위해 &gt;1TB/s 메모리 대역폭 필요</li> </ul> <p><strong>알고리즘 가정:</strong></p> <ul> <li><strong>블록 친화적 시퀀스</strong>: 시퀀스 길이가 블록 크기(64, 128, 256)로 나누어떨어질 때 최고 성능</li> <li><strong>수치 정밀도</strong>: fp16/bf16 정밀도 필요 - fp32로는 효율적으로 작동하지 않음</li> <li><strong>메모리 계층 인식</strong>: 어텐션 블록을 고속 SRAM에 맞출 수 있어야 함</li> </ul> <p><strong>모델 제약:</strong></p> <ul> <li><strong>헤드 차원 제한</strong>: 최대 256 헤드 차원 지원 (FlashAttention-1의 128에서 확장)</li> <li><strong>시퀀스 길이</strong>: 이론적 제한 없음, 하지만 사용 가능한 메모리에 따른 실용적 제한</li> </ul> <p><strong>내 분석:</strong> 이러한 가정들은 FlashAttention-2를 매우 강력하게 만들지만 동시에 어느 정도 제한적이기도 합니다. 현대 하드웨어에 고도로 최적화되어 있지만 구형 GPU나 다른 아키텍처에서는 이점을 제공하지 않을 것입니다. 이는 시스템 연구에서 흔한 트레이드오프입니다 - 특정 하드웨어에 특화함으로써 성능을 얻는 것입니다.</p> <h3 id="4-flashattention-2-방법-실제-작동-원리">4. FlashAttention-2 방법: 실제 작동 원리</h3> <p><strong>개요: 세 가지 최적화 전략</strong></p> <p>FlashAttention-2를 식당 주방 최적화로 생각해보세요:</p> <ol> <li><strong>준비 작업 줄이기</strong> (비행렬 연산 감소)</li> <li><strong>더 나은 직원 배치</strong> (향상된 병렬화)</li> <li><strong>주방 간 소통 제거</strong> (최적화된 작업 분할)</li> </ol> <h4 id="최적화-1-비행렬-연산-감소">최적화 1: 비행렬 연산 감소</h4> <p><strong>문제:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># FlashAttention-1: 비싼 재스케일링 연산들
</span><span class="k">def</span> <span class="nf">online_softmax_v1</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">prev_max</span><span class="p">,</span> <span class="n">prev_sum</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">많은 비싼 연산이 있는 원래 버전</span><span class="sh">"""</span>
    <span class="n">new_max</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">prev_max</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="c1"># ❌ 여러 개의 비싼 재스케일링 연산들
</span>    <span class="n">exp_prev</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">prev_max</span> <span class="o">-</span> <span class="n">new_max</span><span class="p">)</span>  <span class="c1"># 비싸다!
</span>    <span class="n">prev_sum_rescaled</span> <span class="o">=</span> <span class="n">exp_prev</span> <span class="o">*</span> <span class="n">prev_sum</span>   <span class="c1"># 비싸다!
</span>    <span class="n">exp_curr</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">scores</span> <span class="o">-</span> <span class="n">new_max</span><span class="p">)</span>    <span class="c1"># 비싸다!
</span>    <span class="n">curr_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exp_curr</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>  <span class="c1"># 비싸다!
</span>    
    <span class="c1"># ❌ 더 많은 비싼 연산들
</span>    <span class="n">new_sum</span> <span class="o">=</span> <span class="n">prev_sum_rescaled</span> <span class="o">+</span> <span class="n">curr_sum</span>
    <span class="k">return</span> <span class="n">exp_curr</span><span class="p">,</span> <span class="n">new_max</span><span class="p">,</span> <span class="n">new_sum</span>
</code></pre></div></div> <p><strong>해결책:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># FlashAttention-2: 연산 감소로 최적화된 버전
</span><span class="k">def</span> <span class="nf">online_softmax_v2</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">prev_max</span><span class="p">,</span> <span class="n">prev_sum</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">전략적 연산 감소로 최적화된 버전</span><span class="sh">"""</span>
    <span class="n">curr_max</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">new_max</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">prev_max</span><span class="p">,</span> <span class="n">curr_max</span><span class="p">)</span>
    
    <span class="c1"># ✅ 스마트한 조건부 로직으로 연산 감소
</span>    <span class="k">if</span> <span class="n">torch</span><span class="p">.</span><span class="nf">equal</span><span class="p">(</span><span class="n">new_max</span><span class="p">,</span> <span class="n">curr_max</span><span class="p">):</span>
        <span class="c1"># 현재 최댓값이 전역 최댓값 - 이 경우에 최적화
</span>        <span class="n">exp_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">scores</span> <span class="o">-</span> <span class="n">new_max</span><span class="p">)</span>
        <span class="n">exp_prev_factor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">prev_max</span> <span class="o">-</span> <span class="n">new_max</span><span class="p">)</span>
        <span class="n">new_sum</span> <span class="o">=</span> <span class="n">exp_prev_factor</span> <span class="o">*</span> <span class="n">prev_sum</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exp_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># 이전 최댓값이 전역 최댓값 - 다른 최적화
</span>        <span class="n">exp_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">scores</span> <span class="o">-</span> <span class="n">new_max</span><span class="p">)</span>
        <span class="n">new_sum</span> <span class="o">=</span> <span class="n">prev_sum</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">exp_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">exp_scores</span><span class="p">,</span> <span class="n">new_max</span><span class="p">,</span> <span class="n">new_sum</span>
</code></pre></div></div> <p><strong>실제 예시 영향:</strong> 길이 2048 시퀀스의 경우, 이 최적화만으로도 비행렬 FLOPs를 전체 연산의 ~15%에서 ~8%로 줄여 ~17% 속도 향상을 제공합니다.</p> <h4 id="최적화-2-시퀀스-길이-병렬화">최적화 2: 시퀀스 길이 병렬화</h4> <p><strong>문제:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># FlashAttention-1: 제한된 병렬화
</span><span class="k">def</span> <span class="nf">flashattention1_parallelization</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">원래 병렬화 전략</span><span class="sh">"""</span>
    <span class="c1"># 배치와 헤드에만 병렬화
</span>    <span class="n">num_thread_blocks</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span>
    
    <span class="c1"># 예시: batch=2, heads=8, seq_len=4096
</span>    <span class="c1"># 결과: 108개의 사용 가능한 프로세서에 대해 16개의 스레드 블록만
</span>    <span class="c1"># GPU 활용도: 16/108 = 15% 😞
</span>    <span class="k">return</span> <span class="n">num_thread_blocks</span>
</code></pre></div></div> <p><strong>해결책:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># FlashAttention-2: 향상된 병렬화
</span><span class="k">def</span> <span class="nf">flashattention2_parallelization</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">시퀀스 차원이 포함된 향상된 병렬화</span><span class="sh">"""</span>
    <span class="c1"># 배치, 헤드, 그리고 시퀀스 블록에 병렬화
</span>    <span class="n">seq_blocks</span> <span class="o">=</span> <span class="p">(</span><span class="n">seq_len</span> <span class="o">+</span> <span class="n">block_size</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">//</span> <span class="n">block_size</span>
    <span class="n">num_thread_blocks</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">num_heads</span> <span class="o">*</span> <span class="n">seq_blocks</span>
    
    <span class="c1"># 같은 예시: batch=2, heads=8, seq_len=4096, block_size=128
</span>    <span class="c1"># seq_blocks = 4096/128 = 32
</span>    <span class="c1"># 결과: 2 * 8 * 32 = 512 스레드 블록
</span>    <span class="c1"># GPU 활용도: 512/108 = 474% (프로세서당 여러 블록) 🚀
</span>    <span class="k">return</span> <span class="n">num_thread_blocks</span>

<span class="k">def</span> <span class="nf">parallel_attention_computation</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">시퀀스 병렬화가 실제로 작동하는 방식</span><span class="sh">"""</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_dim</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">block_size</span> <span class="o">=</span> <span class="mi">128</span>
    
    <span class="c1"># 각 스레드 블록이 다른 시퀀스 세그먼트를 처리
</span>    <span class="n">results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">seq_block_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">block_size</span><span class="p">):</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">seq_block_idx</span>
        <span class="n">end_idx</span> <span class="o">=</span> <span class="nf">min</span><span class="p">(</span><span class="n">seq_block_idx</span> <span class="o">+</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
        
        <span class="c1"># 이 블록은 Q[start_idx:end_idx]를 모든 K, V에 대해 처리
</span>        <span class="n">Q_block</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[:,</span> <span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>
        
        <span class="c1"># 이 Q 블록에 대해 전체 K, V에 대한 어텐션 계산
</span>        <span class="n">block_result</span> <span class="o">=</span> <span class="nf">compute_attention_block</span><span class="p">(</span><span class="n">Q_block</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">start_idx</span><span class="p">)</span>
        <span class="n">results</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">block_result</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">results</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div></div> <p><strong>구체적인 예시:</strong></p> <ul> <li><strong>시나리오</strong>: batch_size=2, 8개 어텐션 헤드, sequence length=4096로 모델 훈련</li> <li><strong>FlashAttention-1</strong>: 16개 스레드 블록 → 15% GPU 활용도</li> <li><strong>FlashAttention-2</strong>: 512개 스레드 블록 → 거의 100% GPU 활용도</li> <li><strong>결과</strong>: 더 나은 하드웨어 활용으로 ~29% 속도 향상</li> </ul> <h4 id="최적화-3-워프-통신-제거">최적화 3: 워프 통신 제거</h4> <p><strong>문제 - “Sliced-K” 접근법:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># FlashAttention-1: 비효율적인 워프 통신
</span><span class="k">def</span> <span class="nf">flashattention1_warp_work</span><span class="p">(</span><span class="n">Q_block</span><span class="p">,</span> <span class="n">K_block</span><span class="p">,</span> <span class="n">V_block</span><span class="p">,</span> <span class="n">num_warps</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">원래 접근법: K와 V를 워프에 분산</span><span class="sh">"""</span>
    <span class="n">head_dim</span> <span class="o">=</span> <span class="n">K_block</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">warp_size</span> <span class="o">=</span> <span class="n">head_dim</span> <span class="o">//</span> <span class="n">num_warps</span>  <span class="c1"># 각 워프가 차원의 1/4 담당
</span>    
    <span class="n">warp_results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">warp_id</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_warps</span><span class="p">):</span>
        <span class="n">start_dim</span> <span class="o">=</span> <span class="n">warp_id</span> <span class="o">*</span> <span class="n">warp_size</span>
        <span class="n">end_dim</span> <span class="o">=</span> <span class="p">(</span><span class="n">warp_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">warp_size</span>
        
        <span class="c1"># 각 워프가 K, V의 슬라이스로 계산
</span>        <span class="n">K_warp</span> <span class="o">=</span> <span class="n">K_block</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">start_dim</span><span class="p">:</span><span class="n">end_dim</span><span class="p">]</span>  <span class="c1"># 워프가 슬라이스 담당
</span>        <span class="n">V_warp</span> <span class="o">=</span> <span class="n">V_block</span><span class="p">[:,</span> <span class="p">:,</span> <span class="p">:,</span> <span class="n">start_dim</span><span class="p">:</span><span class="n">end_dim</span><span class="p">]</span>  <span class="c1"># 워프가 슬라이스 담당
</span>        
        <span class="c1"># 부분 결과 계산
</span>        <span class="n">scores_warp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q_block</span><span class="p">,</span> <span class="n">K_warp</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># 모든 Q × K 슬라이스
</span>        <span class="n">attn_warp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scores_warp</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output_warp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attn_warp</span><span class="p">,</span> <span class="n">V_warp</span><span class="p">)</span>
        
        <span class="n">warp_results</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">output_warp</span><span class="p">)</span>
    
    <span class="c1"># ❌ 비용이 많이 듦: 모든 워프가 통신하고 동기화해야 함
</span>    <span class="nf">synchronize_warps</span><span class="p">()</span>  <span class="c1"># 비싼 배리어
</span>    <span class="n">final_result</span> <span class="o">=</span> <span class="nf">reduce_across_warps</span><span class="p">(</span><span class="n">warp_results</span><span class="p">)</span>  <span class="c1"># 비싼 리덕션
</span>    
    <span class="k">return</span> <span class="n">final_result</span>
</code></pre></div></div> <p><strong>해결책 - “Sliced-Q” 접근법:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># FlashAttention-2: 효율적인 독립적 워프 작업
</span><span class="k">def</span> <span class="nf">flashattention2_warp_work</span><span class="p">(</span><span class="n">Q_block</span><span class="p">,</span> <span class="n">K_block</span><span class="p">,</span> <span class="n">V_block</span><span class="p">,</span> <span class="n">num_warps</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">새로운 접근법: Q를 워프에 분산</span><span class="sh">"""</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="n">Q_block</span><span class="p">.</span><span class="nf">size</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">warp_size</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="n">num_warps</span>  <span class="c1"># 각 워프가 시퀀스의 1/4 담당
</span>    
    <span class="n">warp_results</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">warp_id</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_warps</span><span class="p">):</span>
        <span class="n">start_seq</span> <span class="o">=</span> <span class="n">warp_id</span> <span class="o">*</span> <span class="n">warp_size</span>
        <span class="n">end_seq</span> <span class="o">=</span> <span class="p">(</span><span class="n">warp_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">warp_size</span>
        
        <span class="c1"># 각 워프가 Q의 슬라이스로 계산
</span>        <span class="n">Q_warp</span> <span class="o">=</span> <span class="n">Q_block</span><span class="p">[:,</span> <span class="n">start_seq</span><span class="p">:</span><span class="n">end_seq</span><span class="p">,</span> <span class="p">:,</span> <span class="p">:]</span>  <span class="c1"># 워프가 Q 슬라이스 담당
</span>        <span class="c1"># K와 V는 공유됨 (읽기 전용) - 통신 불필요!
</span>        
        <span class="c1"># 독립적인 계산 - 동기화 불필요
</span>        <span class="n">scores_warp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q_warp</span><span class="p">,</span> <span class="n">K_block</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>  <span class="c1"># Q 슬라이스 × 모든 K
</span>        <span class="n">attn_warp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scores_warp</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">output_warp</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attn_warp</span><span class="p">,</span> <span class="n">V_block</span><span class="p">)</span>  <span class="c1"># 어텐션 × 모든 V
</span>        
        <span class="n">warp_results</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">output_warp</span><span class="p">)</span>
    
    <span class="c1"># ✅ 동기화 없음: 독립적인 결과들을 단순히 연결
</span>    <span class="n">final_result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">warp_results</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>  <span class="c1"># 간단한 연결
</span>    
    <span class="k">return</span> <span class="n">final_result</span>
</code></pre></div></div> <p><strong>왜 이것이 작동하는가 - 식당 비유:</strong></p> <ul> <li> <p><strong>FlashAttention-1 (Sliced-K)</strong>: 4명의 요리사가 각각 다른 재료를 담당, 끊임없이 조율해야 함</p> <ul> <li>요리사 1은 야채, 요리사 2는 고기 등을 담당</li> <li>끊임없이 소통해야 함: “야채 준비 끝났나?” “고기 얼마나 준비됐나?”</li> <li>많은 대기와 조율 오버헤드</li> </ul> </li> <li> <p><strong>FlashAttention-2 (Sliced-Q)</strong>: 4명의 요리사가 각각 다른 테이블을 담당, 모든 재료 공유</p> <ul> <li>요리사 1은 테이블 1-10, 요리사 2는 테이블 11-20 등을 담당</li> <li>모든 재료(K, V)가 모든 사람에게 사용 가능</li> <li>조율 불필요 - 각 요리사가 독립적으로 작업</li> </ul> </li> </ul> <p><strong>성능 영향:</strong> 이 최적화는 공유 메모리 동기화 오버헤드를 제거하여 ~28% 속도 향상을 제공합니다.</p> <h3 id="5-실험-결과-성능-향상-증명">5. 실험 결과: 성능 향상 증명</h3> <h4 id="주요-성능-결과">주요 성능 결과</h4> <table> <thead> <tr> <th><strong>지표</strong></th> <th><strong>PyTorch 표준</strong></th> <th><strong>FlashAttention-1</strong></th> <th><strong>FlashAttention-2</strong></th> <th><strong>개선 정도</strong></th> </tr> </thead> <tbody> <tr> <td><strong>속도 (TFLOPs/s)</strong></td> <td>30</td> <td>120</td> <td><strong>230</strong></td> <td><strong>PyTorch 대비 7.7배, FA-1 대비 1.9배</strong></td> </tr> <tr> <td><strong>이론적 최댓값 %</strong></td> <td>10%</td> <td>38%</td> <td><strong>73%</strong></td> <td><strong>GPU 활용도 7.3배 향상</strong></td> </tr> <tr> <td><strong>4K 시퀀스 메모리</strong></td> <td>67 GB</td> <td>3.2 GB</td> <td><strong>3.2 GB</strong></td> <td><strong>21배 메모리 감소</strong></td> </tr> <tr> <td><strong>훈련 비용 (GPT-3)</strong></td> <td>460만 달러</td> <td>230만 달러</td> <td><strong>45만 8천 달러</strong></td> <td><strong>90% 비용 절감</strong></td> </tr> </tbody> </table> <p><strong>이 결과들에 대한 내 분석:</strong></p> <p><strong>정말 인상적인 부분들:</strong></p> <ol> <li> <p><strong>GPU 활용도 점프</strong>: 38%에서 73%로 이론적 최댓값에 도달하는 것은 놀랍습니다. 대부분의 최적화는 10-20% 개선을 제공하지만, 하드웨어 효율성을 거의 두 배로 높이는 것은 뛰어납니다.</p> </li> <li> <p><strong>메모리 효율성 복합 효과</strong>: 21배 메모리 감소는 단순히 RAM을 적게 사용하는 것이 아닙니다 - 완전히 새로운 애플리케이션을 가능하게 합니다. 이제 단일 GPU에서 이전에는 불가능했던 모델을 훈련할 수 있습니다.</p> </li> <li> <p><strong>경제적 변혁</strong>: 90% 비용 절감은 단순한 최적화가 아닙니다 - AI 연구를 민주화하는 패러다임 전환입니다.</p> </li> </ol> <h4 id="절제-연구-각-최적화가-기여하는-바-이해하기">절제 연구: 각 최적화가 기여하는 바 이해하기</h4> <table> <thead> <tr> <th><strong>구성</strong></th> <th><strong>TFLOPs/s</strong></th> <th><strong>개선</strong></th> <th><strong>핵심 이점</strong></th> </tr> </thead> <tbody> <tr> <td><strong>베이스라인 (FlashAttention-1)</strong></td> <td>120</td> <td>-</td> <td>시작점</td> </tr> <tr> <td><strong>+ 비행렬 연산 감소</strong></td> <td>140</td> <td>+17%</td> <td>더 나은 텐서 코어 사용</td> </tr> <tr> <td><strong>+ 시퀀스 병렬화</strong></td> <td>180</td> <td>+29%</td> <td>더 높은 GPU 점유율</td> </tr> <tr> <td><strong>+ 워프 최적화</strong></td> <td><strong>230</strong></td> <td>+28%</td> <td>동기화 제거</td> </tr> <tr> <td><strong>모든 것 결합</strong></td> <td><strong>230</strong></td> <td>+92%</td> <td><strong>곱셈 효과</strong></td> </tr> </tbody> </table> <p><strong>내 심층 분석:</strong></p> <p><strong>각 최적화가 중요한 이유:</strong></p> <ol> <li> <p><strong>비행렬 연산 감소 (+17%)</strong>:</p> <ul> <li>작아 보이지만 실제로는 기초적임</li> <li>현대 GPU는 행렬 곱셈을 중심으로 설계됨 - 나머지는 부차적</li> <li>비싼 연산을 ~50% 줄임으로써 GPU의 진정한 잠재력을 해방</li> <li>비포장도로에서 고속도로로 바꾸는 것과 같음 - 기본 경로가 이제 최적화됨</li> </ul> </li> <li> <p><strong>시퀀스 병렬화 (+29%)</strong>:</p> <ul> <li>리소스 활용에 관한 것 - 고전적인 시스템 최적화</li> <li>이전: GPU 코어의 85%가 긴 시퀀스에 대해 유휴 상태</li> <li>이후: 거의 100% 활용</li> <li>통찰: 어텐션이 본질적으로 순차적으로 보여도 일부분을 병렬화할 수 없다는 뜻은 아님</li> </ul> </li> <li> <p><strong>워프 최적화 (+28%)</strong>:</p> <ul> <li>가장 미묘하지만 아마도 가장 중요한 최적화</li> <li>깊은 하드웨어 아키텍처를 이해하는 것</li> <li>통찰: 공유 메모리 통신은 숨겨진 병목</li> <li>동기화를 제거함으로써 근본적인 제약을 없앰</li> </ul> </li> </ol> <p><strong>곱셈 효과:</strong> 17% + 29% + 28% = 74%이지만 실제 개선은 92%입니다. 이런 초선형 개선이 일어나는 이유:</p> <ul> <li>각 최적화가 다른 것들이 더 잘 작동하게 만듦</li> <li>하나의 병목을 제거하면 종종 다른 병목들도 드러나고 제거됨</li> <li>하드웨어 최적화는 단순히 더하는 것이 아니라 복합됨</li> </ul> <h4 id="순방향-vs-역방향-패스-분석">순방향 vs 역방향 패스 분석</h4> <table> <thead> <tr> <th><strong>패스</strong></th> <th><strong>방법</strong></th> <th><strong>TFLOPs/s</strong></th> <th><strong>이론적 %</strong></th> <th><strong>내 분석</strong></th> </tr> </thead> <tbody> <tr> <td><strong>순방향</strong></td> <td>PyTorch</td> <td>60</td> <td>19%</td> <td>메모리 제한적, 비효율적</td> </tr> <tr> <td><strong>순방향</strong></td> <td>FlashAttention-2</td> <td><strong>187</strong></td> <td><strong>60%</strong></td> <td><strong>3.1배 개선</strong></td> </tr> <tr> <td><strong>역방향</strong></td> <td>PyTorch</td> <td>45</td> <td>14%</td> <td>더욱 도전적</td> </tr> <tr> <td><strong>역방향</strong></td> <td>FlashAttention-2</td> <td><strong>165</strong></td> <td><strong>53%</strong></td> <td><strong>3.7배 개선</strong></td> </tr> </tbody> </table> <p><strong>역방향 패스 개선이 더 극적인 이유:</strong></p> <p>역방향 패스가 더 복잡한 이유:</p> <ol> <li><strong>더 많은 행렬 연산</strong>: 순방향 패스의 2개 vs 5개 행렬 곱셈</li> <li><strong>더 많은 메모리 압박</strong>: 중간값을 저장/재계산해야 함</li> <li><strong>복잡한 의존성</strong>: 기울기 계산에 더 복잡한 데이터 의존성</li> </ol> <p>FlashAttention-2의 최적화가 역방향 패스에서 더 도움이 되는 이유:</p> <ul> <li><strong>메모리 효율성</strong>: 메모리가 부족할 때 타일링과 재계산 전략이 빛남</li> <li><strong>병렬화</strong>: 더 많은 계산은 더 많은 병렬 실행 기회를 의미</li> <li><strong>통신 감소</strong>: 복잡한 의존성으로 인해 통신 제거가 더 가치 있음</li> </ul> <h3 id="6-실제-영향과-중요성">6. 실제 영향과 중요성</h3> <h4 id="훈련-시간-혁명">훈련 시간 혁명</h4> <p><strong>GPT-3 175B 훈련 비교:</strong></p> <table> <thead> <tr> <th><strong>방법</strong></th> <th><strong>시간</strong></th> <th><strong>비용</strong></th> <th><strong>접근성</strong></th> </tr> </thead> <tbody> <tr> <td><strong>표준</strong></td> <td>~200일</td> <td>460만 달러</td> <td>거대 기술 기업만</td> </tr> <tr> <td><strong>FlashAttention-1</strong></td> <td>~100일</td> <td>230만 달러</td> <td>대기업</td> </tr> <tr> <td><strong>FlashAttention-2</strong></td> <td><strong>10일</strong></td> <td><strong>45만 8천 달러</strong></td> <td><strong>많은 연구소</strong></td> </tr> </tbody> </table> <p><strong>더 넓은 함의에 대한 내 평가:</strong></p> <p><strong>연구 민주화:</strong></p> <ul> <li>이전: Google, OpenAI, Microsoft만이 대형 모델 훈련 가능</li> <li>이후: 중견 기업과 자금이 충분한 연구소들이 참여 가능</li> <li>참여자 풀을 확장하여 AI 연구를 10배 가속화</li> </ul> <p><strong>개발 사이클 가속화:</strong></p> <ul> <li>200일 훈련 사이클 vs 10일 사이클로 빠른 반복 가능</li> <li>연구자들이 같은 시간에 20배 더 많은 실험 시도 가능</li> <li>복합 효과: 더 빠른 실험 → 더 나은 통찰 → 더 나은 모델</li> </ul> <p><strong>환경 영향:</strong></p> <ul> <li>90% 비용 절감은 종종 ~90% 에너지 절감으로 이어짐</li> <li>FlashAttention-2로 GPT-3 훈련은 ~10배 적은 전력 사용</li> <li>대규모 AI 연구를 더 지속 가능하게 만듦</li> </ul> <h3 id="7-비판적-평가와-제한사항">7. 비판적 평가와 제한사항</h3> <p><strong>FlashAttention-2를 성공적으로 만드는 요소들:</strong></p> <ol> <li><strong>하드웨어-소프트웨어 공동 설계</strong>: GPU 아키텍처에 대한 깊은 이해</li> <li><strong>체계적 최적화</strong>: 하나의 큰 변화보다는 세 가지 상호보완적 개선</li> <li><strong>실용적 영향</strong>: 많은 연구자들이 직면하는 실제 문제 해결</li> <li><strong>엄격한 평가</strong>: 여러 차원에 걸친 포괄적 벤치마크</li> </ol> <p><strong>제한사항과 제약사항:</strong></p> <ol> <li> <p><strong>하드웨어 의존성</strong>: 현대 GPU(A100, H100)에서만 작동</p> <ul> <li><strong>내 견해</strong>: 이는 장점이자 단점 - 고성능을 위해서는 특화가 필요</li> </ul> </li> <li> <p><strong>복잡성</strong>: 표준 어텐션보다 훨씬 복잡</p> <ul> <li><strong>내 견해</strong>: 복잡성은 성능 향상으로 정당화되지만 유지보수 우려 제기</li> </ul> </li> <li> <p><strong>정밀도 요구사항</strong>: fp16/bf16 필요, fp32로는 효율적으로 작동하지 않음</p> <ul> <li><strong>내 견해</strong>: 수치 정밀도를 제한하지만 텐서 코어 활용을 위해 필요</li> </ul> </li> <li> <p><strong>블록 크기 민감성</strong>: 성능이 좋은 블록 크기 선택에 의존</p> <ul> <li><strong>내 견해</strong>: 일부 튜닝 전문성이 필요하지만 기본값이 잘 작동</li> </ul> </li> </ol> <h3 id="8-왜-이-논문이-ai-진보에-중요한가">8. 왜 이 논문이 AI 진보에 중요한가</h3> <p><strong>기술적 우수성:</strong> FlashAttention-2는 최고의 시스템 연구를 대표합니다 - 깊은 하드웨어 이해와 알고리즘 혁신을 결합하여 실용적 문제를 해결합니다.</p> <p><strong>더 넓은 영향:</strong> 이 작업은 인프라 개선이 알고리즘 돌파구만큼 중요할 수 있음을 보여줍니다. 때로는 더 나은 AI로 가는 길이 새로운 모델 아키텍처가 아니라 기존 아키텍처를 훨씬 더 효율적으로 실행하게 만드는 것입니다.</p> <p><strong>연구 철학:</strong> 이 논문은 다음의 가치를 보여줍니다:</p> <ul> <li>최적화하기 전에 병목 현상을 프로파일링하고 이해하기</li> <li>임시방편적 개선보다는 체계적 최적화</li> <li>여러 차원에 걸친 엄격한 벤치마킹</li> <li>계산적 영향과 경제적 영향 모두 고려</li> </ul> <p><strong>개인적 성찰:</strong> FlashAttention-2에서 가장 인상적인 것은 이론적 돌파구가 아닌 엔지니어링 우수성을 통해 불가능한 것을 가능하게 만든다는 점입니다. AI에서 인프라 작업이 알고리즘 혁신만큼 변혁적일 수 있다는 것을 상기시켜줍니다. 대형 모델 훈련의 90% 비용 절감은 단순한 숫자가 아닙니다 - 전체 분야의 진보를 가속화할 AI 연구의 민주화입니다.</p> <p>이 논문은 또한 하드웨어를 깊이 이해하는 것의 중요성을 보여줍니다. 저자들은 단순히 코드를 프로파일링한 것이 아니라 하드웨어 제약에 맞게 알고리즘을 재설계할 만큼 GPU 아키텍처를 충분히 이해했습니다. AI 워크로드가 증가하고 에너지 효율성이 중요해지면서 이는 점점 더 중요해지고 있습니다.</p> <p>내 의견으로는, FlashAttention-2는 오늘날 우리가 보고 있는 긴 컨텍스트 AI 혁명의 핵심 촉진자 중 하나로 기억될 것입니다. 100K+ 컨텍스트 윈도우를 가진 모델들이 실용적이 된 것은 주로 기본 계산을 실현 가능하게 만든 이와 같은 작업 때문입니다.</p> <hr/>]]></content><author><name></name></author><category term="attention"/><category term="flash-attention"/><category term="memory-efficiency"/><category term="inference"/><category term="hardware-optimization"/><category term="performance-scaling"/><summary type="html"><![CDATA[FlashAttention-2 doubles training speed through better work partitioning, reduced non-matmul FLOPs, and improved parallelism.]]></summary></entry><entry><title type="html">Unified Sequence Parallelism</title><link href="https://sungyubkim.github.io/blog/usp/" rel="alternate" type="text/html" title="Unified Sequence Parallelism"/><published>2025-06-02T00:00:00+09:00</published><updated>2025-06-02T00:00:00+09:00</updated><id>https://sungyubkim.github.io/blog/usp</id><content type="html" xml:base="https://sungyubkim.github.io/blog/usp/"><![CDATA[<h1 id="tldr">TL;DR</h1> <blockquote> <p><strong>이 논문은 무엇에 관한 것인가?</strong> 이 논문은 매우 긴 입력 시퀀스(책 전체를 읽거나 몇 시간의 비디오를 처리하는 것과 같은)를 가진 대형 AI 모델을 훈련시키는 데 있어서의 중요한 문제를 해결합니다. 저자들은 “Ulysses”와 “Ring Attention”이라는 두 가지 기존 기술을 결합한 USP(Unified Sequence Parallelism)를 만들어 최대 208,000 토큰 길이의 시퀀스(약 400페이지 분량의 텍스트)로 AI 모델을 훈련할 수 있게 했습니다.</p> <p><strong>주요 기여:</strong></p> <ol> <li><strong>통합 방법론</strong>: 두 가지 경쟁하는 접근 방식(Ulysses vs Ring) 중 하나를 선택하는 대신, USP는 둘을 지능적으로 결합하여 각각의 장점을 모두 얻습니다</li> </ol> </blockquote> <ul> <li>Paper Link: <a href="https://arxiv.org/pdf/2405.07719">https://arxiv.org/pdf/2405.07719</a></li> </ul> <hr/> <h1 id="related-papers">Related Papers</h1> <p><strong>통합된 방법론의 기반:</strong></p> <ul> <li><a href="/blog/deepspeed_ulysses/">DeepSpeed Ulysses</a> - USP에서 통합된 Ulysses 방법</li> <li><a href="/blog/blockwise_ringattention/">Blockwise RingAttention</a> - USP에서 통합된 Ring 방법</li> <li><a href="/blog/ring-self-attention/">Ring Self-Attention</a> - 시퀀스 병렬화의 체계적 분석</li> </ul> <p><strong>하이브리드 병렬화:</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2406.18485">LoongTrain</a> - 2D 어텐션을 활용한 하이브리드 접근법</li> <li><a href="/blog/tp/">Tensor Parallelism</a> - 텐서 병렬화와의 결합</li> <li><a href="/blog/pp/">GPipe</a> - 파이프라인 병렬화와의 통합</li> </ul> <p><strong>긴 시퀀스 처리:</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2310.03294">DISTFLASHATTN</a> - 분산 어텐션 계산</li> <li><a href="https://arxiv.org/pdf/2311.09431">Striped Attention</a> - 효율적인 시퀀스 분배</li> <li><a href="https://arxiv.org/pdf/2411.01783">Context Parallelism for Scalable Million-Token Inference</a> - 추론 시 컨텍스트 병렬화</li> </ul> <p><strong>시스템 최적화:</strong></p> <ul> <li><a href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> - 메모리 효율적인 훈련</li> <li><a href="https://arxiv.org/pdf/2211.05102">Efficiently Scaling Transformer Inference</a> - 효율적 추론 시스템</li> </ul> <hr/> <ol> <li><strong>하드웨어 적응성</strong>: 사용 가능한 네트워크 하드웨어(빠른 NVLink vs 느린 PCIe 연결)에 따라 자동으로 성능을 최적화합니다</li> <li><strong>실용적 가이드라인</strong>: 실제 시스템에서 언제, 어떻게 다른 병렬화 전략을 사용할지에 대한 명확한 규칙을 제공합니다</li> <li><strong>획기적인 결과</strong>: 208K 토큰 시퀀스 훈련에서 47% 계산 효율성 달성 - 새로운 최고 수준</li> </ol> <p><strong>왜 이것이 중요한가:</strong> 이 연구 이전에는 매우 긴 시퀀스로 AI 모델을 훈련하는 것이 메모리 제한으로 인해 불가능하거나 극도로 비효율적이었습니다. USP는 훨씬 더 긴 맥락을 이해할 수 있는 모델을 훈련하는 것을 실용적으로 만들어, 전체 문서 분석, 긴 대화, 확장된 비디오 시퀀스와 같은 애플리케이션을 가능하게 합니다.</p> <hr/> <h1 id="takeaways">Takeaways</h1> <h2 id="핵심-문제-긴-시퀀스가-왜-어려운가">핵심 문제: 긴 시퀀스가 왜 어려운가</h2> <h3 id="메모리-벽-도전">메모리 벽 도전</h3> <p><strong>동기</strong>: 현대 AI 애플리케이션은 점점 더 긴 시퀀스를 처리해야 합니다. Claude는 100K 토큰을 처리할 수 있고, GPT-4는 128K를, Gemini 1.5 Pro는 1천만 토큰을 주장합니다. 하지만 근본적인 문제가 있습니다: 트랜스포머의 어텐션 메커니즘은 이차 메모리 복잡도를 가집니다.</p> <p><strong>구체적인 예시</strong>:</p> <ul> <li>1,000 토큰 시퀀스: 어텐션에 약 1M 메모리 단위 필요</li> <li>10,000 토큰 시퀀스: 약 100M 메모리 단위 필요</li> <li>100,000 토큰 시퀀스: 약 10B 메모리 단위 필요</li> </ul> <p>이러한 이차 증가는 긴 시퀀스를 처리할 때 단일 GPU가 빠르게 메모리 부족 상태가 된다는 의미입니다.</p> <p><strong>전통적인 해결책과 그 한계들</strong>:</p> <p>논문은 기존 해결책들이 치명적인 결함을 가지고 있다고 식별합니다:</p> <ol> <li> <p><strong>데이터 병렬화 (DP)</strong>: 장치 간 데이터를 분할하지만 큰 배치 크기가 필요</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1"># 문제: 긴 시퀀스에 대해 충분한 배치 크기가 없음
</span> <span class="k">if</span> <span class="n">batch_size</span> <span class="o">&lt;</span> <span class="n">num_devices</span><span class="p">:</span>
     <span class="k">raise</span> <span class="nc">Error</span><span class="p">(</span><span class="sh">"</span><span class="s">데이터 병렬화를 효과적으로 사용할 수 없음</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <p><strong>텐서 병렬화 (TP)</strong>: 모델 가중치를 분할하지만 어텐션 헤드 수에 의해 제한됨</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c1"># 문제: 제한된 확장성
</span> <span class="n">max_tp_degree</span> <span class="o">=</span> <span class="n">num_attention_heads</span>  <span class="c1"># 종종 32-64개만
</span> <span class="k">if</span> <span class="n">required_devices</span> <span class="o">&gt;</span> <span class="n">max_tp_degree</span><span class="p">:</span>
     <span class="k">raise</span> <span class="nc">Error</span><span class="p">(</span><span class="sh">"</span><span class="s">헤드 수를 넘어서 확장할 수 없음</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div> </div> </li> <li> <p><strong>기존 시퀀스 병렬화</strong>: 주요 제한사항을 가진 두 가지 경쟁 접근법:</p> <ul> <li><strong>DeepSpeed-Ulysses</strong>: 빠르지만 어텐션 헤드 수에 의해 제한됨</li> <li><strong>Ring-Attention</strong>: 확장 가능하지만 계산적으로 비효율적</li> </ul> </li> </ol> <h2 id="usp-해결책-두-세계의-장점">USP 해결책: 두 세계의 장점</h2> <h3 id="핵심-통찰-왜-둘-다-안-되나">핵심 통찰: 왜 둘 다 안 되나?</h3> <p>논문의 핵심 통찰은 Ulysses와 Ring 어텐션이 상호 배타적이지 않다는 것입니다 - 그들은 함께 작동할 수 있습니다. 이것은 빠른 자동차와 연료 효율적인 자동차 중 하나를 선택할 필요가 없다는 것을 깨닫는 것과 같습니다; 빠르면서 동시에 효율적인 하이브리드를 설계할 수 있습니다.</p> <h3 id="usp-방법-설명">USP 방법 설명</h3> <p><strong>1단계: 프로세스 그룹 조직</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">setup_usp_groups</span><span class="p">(</span><span class="n">total_devices</span><span class="p">,</span> <span class="n">ulysses_degree</span><span class="p">,</span> <span class="n">ring_degree</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    장치들을 2D 메시로 조직:
    - 행: Ulysses 그룹 (고대역폭 AllToAll)
    - 열: Ring 그룹 (P2P 통신)
    </span><span class="sh">"""</span>
    <span class="k">assert</span> <span class="n">total_devices</span> <span class="o">==</span> <span class="n">ulysses_degree</span> <span class="o">*</span> <span class="n">ring_degree</span>
    
    <span class="c1"># 예시: 8개 장치 = 2×4 메시
</span>    <span class="c1"># 장치 배치:
</span>    <span class="c1"># [0, 1, 2, 3]  &lt;- Ring 그룹 0
</span>    <span class="c1"># [4, 5, 6, 7]  &lt;- Ring 그룹 1
</span>    <span class="c1"># |  |  |  |
</span>    <span class="c1"># Ulysses 그룹 (열)
</span>    
    <span class="k">for</span> <span class="n">ring_id</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">ring_degree</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">ulysses_id</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">ulysses_degree</span><span class="p">):</span>
            <span class="n">device_id</span> <span class="o">=</span> <span class="n">ring_id</span> <span class="o">*</span> <span class="n">ulysses_degree</span> <span class="o">+</span> <span class="n">ulysses_id</span>
            <span class="nf">assign_device</span><span class="p">(</span><span class="n">device_id</span><span class="p">,</span> <span class="n">ulysses_group</span><span class="o">=</span><span class="n">ulysses_id</span><span class="p">,</span> <span class="n">ring_group</span><span class="o">=</span><span class="n">ring_id</span><span class="p">)</span>
</code></pre></div></div> <p><strong>2단계: 인과 어텐션을 위한 로드 밸런싱</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">balance_causal_workload</span><span class="p">(</span><span class="n">sequence_tokens</span><span class="p">,</span> <span class="n">num_devices</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    문제: 인과 어텐션에서 초기 토큰들은 더 적은 토큰에 어텐션함
    - 토큰 0은 [0]에 어텐션 (1개 연산)
    - 토큰 1은 [0,1]에 어텐션 (2개 연산) 
    - 토큰 15는 [0,1,2...15]에 어텐션 (16개 연산)
    
    해결책: 각 장치가 동일한 작업을 갖도록 토큰을 재분배
    </span><span class="sh">"""</span>
    <span class="c1"># 원래 할당 (불균형):
</span>    <span class="c1"># 장치 0: 토큰 [0,1,2,3] → 1+2+3+4 = 10 연산
</span>    <span class="c1"># 장치 3: 토큰 [12,13,14,15] → 13+14+15+16 = 58 연산
</span>    
    <span class="c1"># 로드 밸런싱된 할당:
</span>    <span class="c1"># 장치 0: 토큰 [0,1,14,15] → 1+2+15+16 = 34 연산  
</span>    <span class="c1"># 장치 3: 토큰 [6,7,8,9] → 7+8+9+10 = 34 연산
</span>    
    <span class="n">reordered_tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="n">chunk_size</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">sequence_tokens</span><span class="p">)</span> <span class="o">//</span> <span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">num_devices</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">device</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_devices</span><span class="p">):</span>
        <span class="c1"># 각 장치는 시작과 끝에서 하나씩 청크를 받음
</span>        <span class="n">start_chunk</span> <span class="o">=</span> <span class="n">sequence_tokens</span><span class="p">[</span><span class="n">device</span> <span class="o">*</span> <span class="n">chunk_size</span><span class="p">:(</span><span class="n">device</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunk_size</span><span class="p">]</span>
        <span class="n">end_chunk</span> <span class="o">=</span> <span class="n">sequence_tokens</span><span class="p">[</span><span class="o">-</span><span class="p">(</span><span class="n">device</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">chunk_size</span><span class="p">:</span><span class="o">-</span><span class="n">device</span> <span class="o">*</span> <span class="n">chunk_size</span><span class="p">]</span>
        <span class="n">reordered_tokens</span><span class="p">.</span><span class="nf">extend</span><span class="p">(</span><span class="n">start_chunk</span> <span class="o">+</span> <span class="n">end_chunk</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">reordered_tokens</span>
</code></pre></div></div> <p><strong>3단계: 통합 어텐션 알고리즘</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">usp_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">ulysses_group</span><span class="p">,</span> <span class="n">ring_group</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    두 접근법을 결합한 완전한 USP 어텐션 메커니즘
    
    입력: 시퀀스 차원으로 분할된 Q,K,V 텐서
    출력: 동일한 분할을 가진 어텐션 출력
    </span><span class="sh">"""</span>
    
    <span class="c1"># 단계 1: Ulysses AllToAll (시퀀스 → 헤드)
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Ulysses 이전: Q 형태 = </span><span class="si">{</span><span class="n">Q</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># [batch, seq/N, heads, dim]
</span>    
    <span class="n">Q_heads</span> <span class="o">=</span> <span class="nf">all_to_all_4d</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">scatter_seq</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">gather_heads</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">ulysses_group</span><span class="p">)</span>
    <span class="n">K_heads</span> <span class="o">=</span> <span class="nf">all_to_all_4d</span><span class="p">(</span><span class="n">K</span><span class="p">,</span> <span class="n">scatter_seq</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">gather_heads</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">ulysses_group</span><span class="p">)</span> 
    <span class="n">V_heads</span> <span class="o">=</span> <span class="nf">all_to_all_4d</span><span class="p">(</span><span class="n">V</span><span class="p">,</span> <span class="n">scatter_seq</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">gather_heads</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">ulysses_group</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">Ulysses 이후: Q 형태 = </span><span class="si">{</span><span class="n">Q_heads</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># [batch, seq, heads/M, dim]
</span>    
    <span class="c1"># 단계 2: P2P 통신을 가진 Ring 어텐션
</span>    <span class="n">O_ring</span> <span class="o">=</span> <span class="nf">ring_attention_with_p2p</span><span class="p">(</span><span class="n">Q_heads</span><span class="p">,</span> <span class="n">K_heads</span><span class="p">,</span> <span class="n">V_heads</span><span class="p">,</span> <span class="n">ring_group</span><span class="p">)</span>
    
    <span class="c1"># 단계 3: 역 Ulysses AllToAll (헤드 → 시퀀스)  
</span>    <span class="n">O_final</span> <span class="o">=</span> <span class="nf">all_to_all_4d</span><span class="p">(</span><span class="n">O_ring</span><span class="p">,</span> <span class="n">scatter_heads</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">gather_seq</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">group</span><span class="o">=</span><span class="n">ulysses_group</span><span class="p">)</span>
    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">최종 출력: O 형태 = </span><span class="si">{</span><span class="n">O_final</span><span class="p">.</span><span class="n">shape</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># [batch, seq/N, heads, dim]
</span>    
    <span class="k">return</span> <span class="n">O_final</span>

<span class="k">def</span> <span class="nf">ring_attention_with_p2p</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">ring_group</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">P2P를 통해 장치 간에 K,V 블록을 전달하는 Ring 어텐션</span><span class="sh">"""</span>
    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
    
    <span class="c1"># 각 장치는 자신의 K,V 블록으로 시작
</span>    <span class="n">my_K</span><span class="p">,</span> <span class="n">my_V</span> <span class="o">=</span> <span class="n">K</span><span class="p">.</span><span class="nf">clone</span><span class="p">(),</span> <span class="n">V</span><span class="p">.</span><span class="nf">clone</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">ring_group</span><span class="p">.</span><span class="nf">size</span><span class="p">()):</span>
        <span class="c1"># 현재 K,V 블록으로 어텐션 계산
</span>        <span class="n">attention_scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">my_K</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">attention_weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">attention_scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">partial_output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">attention_weights</span><span class="p">,</span> <span class="n">my_V</span><span class="p">)</span>
        
        <span class="c1"># 결과 누적
</span>        <span class="n">output</span> <span class="o">+=</span> <span class="n">partial_output</span>
        
        <span class="c1"># 링의 다음 장치로 K,V 전달 (마지막 단계 제외)
</span>        <span class="k">if</span> <span class="n">step</span> <span class="o">&lt;</span> <span class="n">ring_group</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="n">next_device</span> <span class="o">=</span> <span class="p">(</span><span class="n">ring_group</span><span class="p">.</span><span class="nf">rank</span><span class="p">()</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">ring_group</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
            <span class="n">prev_device</span> <span class="o">=</span> <span class="p">(</span><span class="n">ring_group</span><span class="p">.</span><span class="nf">rank</span><span class="p">()</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">ring_group</span><span class="p">.</span><span class="nf">size</span><span class="p">()</span>
            
            <span class="c1"># 동시 송수신
</span>            <span class="n">ring_group</span><span class="p">.</span><span class="nf">send</span><span class="p">(</span><span class="n">my_K</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="n">next_device</span><span class="p">)</span>
            <span class="n">ring_group</span><span class="p">.</span><span class="nf">send</span><span class="p">(</span><span class="n">my_V</span><span class="p">,</span> <span class="n">dst</span><span class="o">=</span><span class="n">next_device</span><span class="p">)</span> 
            <span class="n">my_K</span> <span class="o">=</span> <span class="n">ring_group</span><span class="p">.</span><span class="nf">recv</span><span class="p">(</span><span class="n">src</span><span class="o">=</span><span class="n">prev_device</span><span class="p">)</span>
            <span class="n">my_V</span> <span class="o">=</span> <span class="n">ring_group</span><span class="p">.</span><span class="nf">recv</span><span class="p">(</span><span class="n">src</span><span class="o">=</span><span class="n">prev_device</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p><strong>구체적인 예시</strong>: 4개 장치, 64 토큰, 16 헤드로 USP를 추적해보겠습니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 초기 상태: 각 장치가 16 토큰, 모든 16 헤드를 가짐
</span><span class="n">장치_0</span><span class="p">:</span> <span class="n">Q</span><span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span> <span class="c1"># 토큰 0-15
</span><span class="n">장치_1</span><span class="p">:</span> <span class="n">Q</span><span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span> <span class="c1"># 토큰 16-31  
</span><span class="n">장치_2</span><span class="p">:</span> <span class="n">Q</span><span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span> <span class="c1"># 토큰 32-47
</span><span class="n">장치_3</span><span class="p">:</span> <span class="n">Q</span><span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span> <span class="c1"># 토큰 48-63
</span>
<span class="c1"># Ulysses AllToAll 이후 (ulysses_degree=2): 시퀀스 분산, 헤드 분할
</span><span class="n">장치_0</span><span class="p">:</span> <span class="n">Q</span><span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>  <span class="c1"># 토큰 0-31, 헤드 0-7
</span><span class="n">장치_1</span><span class="p">:</span> <span class="n">Q</span><span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>  <span class="c1"># 토큰 0-31, 헤드 8-15
</span><span class="n">장치_2</span><span class="p">:</span> <span class="n">Q</span><span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>  <span class="c1"># 토큰 32-63, 헤드 0-7  
</span><span class="n">장치_3</span><span class="p">:</span> <span class="n">Q</span><span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">8</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span>  <span class="c1"># 토큰 32-63, 헤드 8-15
</span>
<span class="c1"># Ring 어텐션 (ring_degree=2): 각 쌍 내에서 P2P 통신
# 장치 0&amp;2가 헤드 0-7에 대한 링 형성
# 장치 1&amp;3이 헤드 8-15에 대한 링 형성
</span>
<span class="c1"># 역 Ulysses 이후: 시퀀스 분할로 복귀
</span><span class="n">장치_0</span><span class="p">:</span> <span class="n">O</span><span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span> <span class="c1"># 토큰 0-15에 대한 최종 출력
</span><span class="n">장치_1</span><span class="p">:</span> <span class="n">O</span><span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span> <span class="c1"># 토큰 16-31에 대한 최종 출력
</span><span class="n">장치_2</span><span class="p">:</span> <span class="n">O</span><span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span> <span class="c1"># 토큰 32-47에 대한 최종 출력
</span><span class="n">장치_3</span><span class="p">:</span> <span class="n">O</span><span class="p">[</span><span class="n">batch</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">64</span><span class="p">]</span> <span class="c1"># 토큰 48-63에 대한 최종 출력
</span></code></pre></div></div> <h3 id="중요한-가정과-성공-조건">중요한 가정과 성공 조건</h3> <p>논문은 USP가 효과적으로 작동하기 위해 충족되어야 하는 몇 가지 중요한 가정을 드러냅니다:</p> <p><strong>수학적 요구사항:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">validate_usp_configuration</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">ulysses_degree</span><span class="p">,</span> <span class="n">ring_degree</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">만족되어야 하는 중요한 조건들</span><span class="sh">"""</span>
    
    <span class="c1"># 조건 1: 헤드 분할 가능성
</span>    <span class="k">assert</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">ulysses_degree</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> \
        <span class="sa">f</span><span class="sh">"</span><span class="s">헤드 </span><span class="si">{</span><span class="n">num_heads</span><span class="si">}</span><span class="s">는 Ulysses 차수 </span><span class="si">{</span><span class="n">ulysses_degree</span><span class="si">}</span><span class="s">로 균등분할되어야 함</span><span class="sh">"</span>
    
    <span class="c1"># 조건 2: 시퀀스 분할 가능성  
</span>    <span class="n">total_sp_degree</span> <span class="o">=</span> <span class="n">ulysses_degree</span> <span class="o">*</span> <span class="n">ring_degree</span>
    <span class="k">assert</span> <span class="n">seq_len</span> <span class="o">%</span> <span class="n">total_sp_degree</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span> \
        <span class="sa">f</span><span class="sh">"</span><span class="s">시퀀스 길이 </span><span class="si">{</span><span class="n">seq_len</span><span class="si">}</span><span class="s">는 총 SP 차수 </span><span class="si">{</span><span class="n">total_sp_degree</span><span class="si">}</span><span class="s">로 분할되어야 함</span><span class="sh">"</span>
    
    <span class="c1"># 조건 3: 장치당 메모리 제약
</span>    <span class="n">memory_per_device</span> <span class="o">=</span> <span class="nf">estimate_memory</span><span class="p">(</span><span class="n">seq_len</span> <span class="o">//</span> <span class="n">ring_degree</span><span class="p">,</span> <span class="n">num_heads</span> <span class="o">//</span> <span class="n">ulysses_degree</span><span class="p">)</span>
    <span class="k">assert</span> <span class="n">memory_per_device</span> <span class="o">&lt;</span> <span class="n">available_gpu_memory</span><span class="p">,</span> <span class="sh">"</span><span class="s">GPU 메모리 부족</span><span class="sh">"</span>
    
    <span class="c1"># 조건 4: 통신 대역폭 요구사항
</span>    <span class="n">ulysses_bandwidth_needed</span> <span class="o">=</span> <span class="nf">estimate_alltoall_bandwidth</span><span class="p">(</span><span class="n">ulysses_degree</span><span class="p">)</span>
    <span class="n">ring_bandwidth_needed</span> <span class="o">=</span> <span class="nf">estimate_p2p_bandwidth</span><span class="p">(</span><span class="n">ring_degree</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="bp">True</span>
</code></pre></div></div> <p><strong>하드웨어 토폴로지 요구사항:</strong></p> <ul> <li><strong>Ulysses를 위한 높은 대역폭</strong>: AllToAll 연산은 NVLink 수준의 연결(400GB/s+)이 필요</li> <li><strong>Ring을 위한 낮은 대역폭도 가능</strong>: P2P는 PCIe나 이더넷 연결에서도 작동 가능</li> <li><strong>네트워크 토폴로지 인식</strong>: USP는 Ulysses 그룹이 고대역폭 도메인에 매핑될 때 최고 성능을 발휘</li> </ul> <p><strong>내 분석</strong>: 논문의 강점은 이러한 가정들을 명시적으로 만드는 것입니다. 이전 연구들은 종종 이상적인 네트워크 조건을 가정했지만, USP는 실제 하드웨어 제한을 인정하고 그에 따라 적응합니다.</p> <h2 id="실험-결과-숫자들이-실제로-의미하는-것">실험 결과: 숫자들이 실제로 의미하는 것</h2> <h3 id="주요-성능-결과">주요 성능 결과</h3> <table> <thead> <tr> <th><strong>구성</strong></th> <th><strong>하드웨어</strong></th> <th><strong>시퀀스 길이</strong></th> <th><strong>MFU</strong></th> <th><strong>내 해석</strong></th> </tr> </thead> <tbody> <tr> <td><strong>LLAMA3-8B</strong></td> <td>2×8xA800</td> <td>208K 토큰</td> <td><strong>47%</strong></td> <td><strong>획기적</strong>: 극한 시퀀스 길이에서 생산 수준 효율성을 달성한 첫 번째 사례</td> </tr> <tr> <td><strong>LLAMA3-8B</strong></td> <td>2×8xA800</td> <td>120K 토큰</td> <td><strong>49%</strong></td> <td><strong>최적점</strong>: 메모리와 계산의 최적 균형</td> </tr> <tr> <td><strong>LLAMA2-7B</strong></td> <td>1×8xA800</td> <td>64K 토큰</td> <td><strong>50%</strong></td> <td><strong>확장성 검증</strong>: 단일 노드 성능이 기준선 확립</td> </tr> </tbody> </table> <p><strong>이러한 결과에 대한 내 생각:</strong></p> <ul> <li><strong>208K에서 47% MFU는 놀랍다</strong> - 대부분의 시스템이 이런 규모에서 &gt;30% 효율성을 유지하는 데 어려움을 겪음</li> <li><strong>49%에서 47%로의 약간의 효율성 감소</strong>는 USP가 알고리즘적 병목보다는 하드웨어 한계에 접근하고 있음을 시사</li> <li><strong>일관된 47-50% 범위</strong>는 USP가 안정적인 성능 특성을 가지고 있음을 나타냄</li> </ul> <h3 id="절제-연구-usp-vs-개별-방법들">절제 연구: USP vs 개별 방법들</h3> <table> <thead> <tr> <th><strong>하드웨어 유형</strong></th> <th><strong>시퀀스</strong></th> <th><strong>SP-Ulysses</strong></th> <th><strong>SP-Ring</strong></th> <th><strong>USP-통합</strong></th> <th><strong>승자</strong></th> <th><strong>왜?</strong></th> </tr> </thead> <tbody> <tr> <td><strong>8xL20 PCIe</strong></td> <td>32K</td> <td>28.6 iter/s</td> <td><strong>62.8 iter/s</strong></td> <td><strong>62.8 iter/s</strong></td> <td>Ring/USP</td> <td>낮은 대역폭이 P2P를 선호</td> </tr> <tr> <td><strong>8xL20 PCIe</strong></td> <td>128K</td> <td>3.2 iter/s</td> <td><strong>5.5 iter/s</strong></td> <td><strong>5.5 iter/s</strong></td> <td>Ring/USP</td> <td>+71% 개선</td> </tr> <tr> <td><strong>8xA100 NVLink</strong></td> <td>32K</td> <td><strong>136.4 iter/s</strong></td> <td>133.0 iter/s</td> <td><strong>136.4 iter/s</strong></td> <td>Ulysses/USP</td> <td>높은 대역폭이 AllToAll 가능하게 함</td> </tr> <tr> <td><strong>8xA100 NVLink</strong></td> <td>128K</td> <td><strong>2.8 iter/s</strong></td> <td>2.9 iter/s</td> <td><strong>2.8 iter/s</strong></td> <td>Ulysses/USP</td> <td>미미한 차이</td> </tr> </tbody> </table> <p><strong>내 분석의 핵심 통찰:</strong></p> <ol> <li><strong>하드웨어-알고리즘 매칭</strong>: 최고의 접근법은 전적으로 네트워크 토폴로지에 달려있음 - 범용 승자는 없음</li> <li><strong>PCIe 시스템은 Ring을 강하게 선호</strong>: 71% 개선은 P2P 통신이 낮은 대역폭에 훨씬 더 적합함을 보여줌</li> <li><strong>NVLink 시스템은 Ulysses를 약간 선호</strong>: AllToAll 연산이 높은 대역폭에서 빛나지만, 장점이 예상보다 작음</li> <li><strong>USP의 가치</strong>: 항상 최고의 개별 방법과 일치하여, 수동 알고리즘 선택의 필요성을 제거</li> </ol> <h3 id="로드-밸런싱-영향-분석">로드 밸런싱 영향 분석</h3> <table> <thead> <tr> <th><strong>방법</strong></th> <th><strong>시퀀스 길이</strong></th> <th><strong>기준선</strong></th> <th><strong>로드 밸런싱</strong></th> <th><strong>개선</strong></th> <th><strong>내 분석</strong></th> </tr> </thead> <tbody> <tr> <td><strong>Ring 어텐션</strong></td> <td>32K</td> <td>28.6 iter/s</td> <td><strong>32.8 iter/s</strong></td> <td><strong>+14.8%</strong></td> <td>중간 길이에서 적당한 이득</td> </tr> <tr> <td><strong>Ring 어텐션</strong></td> <td>128K</td> <td>3.2 iter/s</td> <td><strong>4.2 iter/s</strong></td> <td><strong>+31.6%</strong></td> <td><strong>긴 시퀀스에서 엄청난 이득</strong></td> </tr> </tbody> </table> <p><strong>왜 이것이 중요한가 (내 해석):</strong></p> <ul> <li><strong>개선이 시퀀스 길이와 함께 증가</strong> - 이는 로드 밸런싱이 USP가 목표로 하는 가장 긴 시퀀스에서 중요해진다는 것을 시사</li> <li><strong>31.6% 개선은 엄청나다</strong> - 이 단일 최적화가 많은 알고리즘적 발전보다 더 많은 속도 향상을 제공</li> <li><strong>핵심 기여를 검증</strong> - 로드 밸런싱은 단순히 좋은 기능이 아니라 USP 성공에 필수적</li> </ul> <h3 id="메모리-vs-통신-트레이드오프">메모리 vs 통신 트레이드오프</h3> <table> <thead> <tr> <th><strong>방법</strong></th> <th><strong>통신 볼륨</strong></th> <th><strong>메모리 효율성</strong></th> <th><strong>최적 사용 사례</strong></th> <th><strong>내 평가</strong></th> </tr> </thead> <tbody> <tr> <td><strong>데이터 병렬</strong></td> <td>높음 (AllReduce 그래디언트)</td> <td>우수 (A/N)</td> <td><strong>짧은 시퀀스, 큰 배치</strong></td> <td>적용 가능할 때 여전히 황금 표준</td> </tr> <tr> <td><strong>SP-Ulysses</strong></td> <td>매우 높음 (8×AllToAll)</td> <td>좋음 (A/N)</td> <td><strong>높은 대역폭, 헤드가 많은 모델</strong></td> <td>통신 병목이 확장을 제한</td> </tr> <tr> <td><strong>SP-Ring</strong></td> <td>중간 (4×P2P)</td> <td>좋음 (A/N)</td> <td><strong>낮은 대역폭, 메모리 제약</strong></td> <td>대부분의 시나리오에서 예상보다 좋음</td> </tr> <tr> <td><strong>USP-통합</strong></td> <td><strong>적응형</strong></td> <td>좋음 (A/N)</td> <td><strong>모든 하드웨어 토폴로지</strong></td> <td><strong>최고의 범용 솔루션</strong></td> </tr> <tr> <td><strong>TP-sp</strong></td> <td>높음 (10×AllGather)</td> <td><strong>최고</strong> (αA, α&lt;1)</td> <td><strong>메모리 중요 시나리오</strong></td> <td>극한 메모리 제약에서 여전히 필요</td> </tr> </tbody> </table> <p><strong>내 전략적 분석:</strong></p> <ol> <li><strong>USP가 모든 것을 대체하지는 않음</strong> - TP-sp는 여전히 극한 규모에서 중요한 메모리 장점을 가짐</li> <li><strong>통신 적응성이 USP의 킬러 기능</strong> - 다른 방법은 하드웨어에 맞게 자동 최적화하지 않음</li> <li><strong>메모리 효율성은 “충분히 좋다”</strong> - USP는 주요 요구사항인 중요한 A/N 확장을 달성</li> </ol> <h3 id="수렴-검증-숨겨진-영웅">수렴 검증: 숨겨진 영웅</h3> <table> <thead> <tr> <th><strong>방법</strong></th> <th><strong>훈련 손실</strong></th> <th><strong>수렴 속도</strong></th> <th><strong>수치적 안정성</strong></th> </tr> </thead> <tbody> <tr> <td><strong>데이터 병렬</strong></td> <td>2.45 (기준선)</td> <td>정상</td> <td>안정적</td> </tr> <tr> <td><strong>USP</strong></td> <td><strong>2.45 (동일)</strong></td> <td><strong>동일</strong></td> <td><strong>안정적</strong></td> </tr> </tbody> </table> <p><strong>이 결과가 중요한 이유 (내 관점):</strong></p> <ul> <li><strong>완벽한 수렴 매칭</strong>은 USP가 훈련 아티팩트를 도입하지 않음을 증명</li> <li><strong>동일한 곡선</strong>은 로드 밸런싱과 시퀀스 재정렬이 학습에 영향을 주지 않음을 검증</li> <li><strong>이 결과는 실용적 채택을 가능하게 함</strong> - 수렴 검증 없이는 누구도 프로덕션에서 USP를 신뢰하지 않을 것</li> </ul> <h2 id="내-전체-평가-이-논문이-실제로-달성한-것">내 전체 평가: 이 논문이 실제로 달성한 것</h2> <h3 id="전략적-돌파구">전략적 돌파구</h3> <p>이 논문은 단순히 또 다른 최적화를 제안하는 것이 아니라 근본적인 시스템 문제를 해결합니다. 이 분야는 두 가지 경쟁하는 접근법(Ulysses vs Ring)을 가지고 있었고, 자연스러운 경향은 편을 선택하는 것이었습니다. 저자들의 이들을 결합하는 통찰은 연구자들이 데이터와 모델 병렬성 중 하나를 선택하는 대신 결합할 수 있다는 것을 깨달았던 획기적인 순간과 유사합니다.</p> <h3 id="실제-영향">실제 영향</h3> <p><strong>USP 이전</strong>: 긴 맥락 모델 훈련은 각 하드웨어 설정에 대해 올바른 병렬화 전략을 선택하는 전문 지식이 필요했습니다. 팀들은 구성을 튜닝하는 데 몇 주를 보냈습니다.</p> <p><strong>USP 이후</strong>: 자동으로 적응하는 하나의 통합 접근법. 이는 분산 시스템 전문 지식이 없는 팀들을 위해 긴 맥락 훈련을 민주화합니다.</p> <h3 id="기술적-우아함">기술적 우아함</h3> <p>로드 밸런싱 솔루션은 특히 우아합니다 - 인과 어텐션 작업 부하를 완벽하게 균형 맞추는 간단한 재정렬입니다. 이는 기본 수학적 구조에 대한 깊은 이해를 보여줍니다.</p> <h3 id="한계와-미래-연구">한계와 미래 연구</h3> <ol> <li><strong>여전히 고급 하드웨어 필요</strong> - USP는 값비싼 GPU 클러스터의 필요성을 제거하지 않음</li> <li><strong>통신 오버헤드 여전히 존재</strong> - 짧은 시퀀스에 대해 데이터 병렬화보다 여전히 높음</li> <li><strong>메모리 효율성 격차</strong> - TP-sp는 메모리 중요 시나리오에서 장점을 유지</li> </ol> <h3 id="더-넓은-의미">더 넓은 의미</h3> <p>이 연구는 시퀀스 병렬화가 실험적 기술에서 프로덕션 준비 기술로 성숙했음을 나타냅니다. 208K 토큰에서 47% MFU는 단순한 벤치마크가 아니라 긴 맥락 AI가 이제 규모에서 실용적이라는 증명입니다.</p> <p><strong>내 예측</strong>: USP는 혼합 정밀도 훈련이 보편적으로 채택된 것처럼 긴 맥락 훈련의 표준 접근법이 될 것입니다. 자동 하드웨어 적응이 채택의 주요 장벽을 제거합니다.</p> <hr/>]]></content><author><name></name></author><category term="sequence-parallelism"/><category term="distributed-training"/><summary type="html"><![CDATA[Unified Sequence Parallelism (USP) combines Ulysses and Ring Attention for scalable long-sequence training up to 208K tokens.]]></summary></entry><entry><title type="html">Blockwise RingAttention</title><link href="https://sungyubkim.github.io/blog/blockwise_ringattention/" rel="alternate" type="text/html" title="Blockwise RingAttention"/><published>2025-06-01T00:00:00+09:00</published><updated>2025-06-01T00:00:00+09:00</updated><id>https://sungyubkim.github.io/blog/blockwise_ringattention</id><content type="html" xml:base="https://sungyubkim.github.io/blog/blockwise_ringattention/"><![CDATA[<h1 id="tldr">TL;DR</h1> <blockquote> <p><strong>Blockwise RingAttention</strong>은 AI 모델이 매우 긴 시퀀스(전체 책, 긴 동영상, 대규모 데이터셋 등)를 처리하지 못하게 하는 근본적인 메모리 병목 현상을 해결합니다. 기존 트랜스포머는 제곱 메모리 요구사항을 가집니다 - 100만 토큰을 처리하려면 1M×1M 어텐션 행렬을 저장해야 하는데, 이는 계산상 불가능합니다.</p> <p><strong>핵심 혁신</strong>: Blockwise RingAttention은 이 계산을 여러 장치에 “링(ring)” 토폴로지로 분산시켜, 각 장치가 시퀀스의 일부를 처리하고 다음 장치로 정보를 전달합니다. 이를 통해 <strong>장치_수 × 기존_한계</strong>만큼 긴 시퀀스 처리가 가능합니다.</p> <p><strong>주요 기여점</strong>:</p> <ol> <li><strong>거의 무한한 컨텍스트</strong>: 1억+ 토큰 시퀀스 훈련 가능 (약 300권의 책에 해당)</li> <li><strong>선형 확장</strong>: 컨텍스트 길이가 장치 수에 선형적으로 확장, 시퀀스 길이에 제곱적으로 확장되지 않음</li> <li><strong>근사 없음</strong>: 정확도를 희생하지 않고 정확한 어텐션 계산</li> <li><strong>메모리 효율성</strong>: 각 장치는 전체 시퀀스의 일부만 저장</li> <li><strong>통신 최적화</strong>: 계산과 데이터 전송을 중첩시켜 최소한의 오버헤드</li> </ol> <p><strong>영향</strong>: 전체 코드베이스 처리, 완전한 영화 분석, 대규모 과학 데이터셋 훈련 등 이전에는 불가능했던 작업들을 현실화합니다.</p> </blockquote> <ul> <li>Paper Link: <a href="https://openreview.net/forum?id=WsRHpHH4s0">https://openreview.net/forum?id=WsRHpHH4s0</a></li> </ul> <hr/> <h1 id="related-papers">Related Papers</h1> <p><strong>분산 어텐션 방법론:</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2310.03294">DISTFLASHATTN</a> - GPU 간 FlashAttention 분산을 위한 방법</li> <li><a href="https://arxiv.org/pdf/2311.09431">Striped Attention</a> - 로드 밸런싱을 위한 대안적 어텐션 분배 패턴</li> <li><a href="https://arxiv.org/pdf/2309.14509">DeepSpeed Ulysses</a> - 어텐션 분산을 활용한 시퀀스 병렬처리</li> </ul> <p><strong>긴 시퀀스 훈련:</strong></p> <ul> <li><a href="/blog/ring-self-attention/">Ring Self-Attention</a> - 시퀀스 병렬처리에 대한 종합적 관점</li> <li><a href="https://arxiv.org/pdf/2411.01783">Context Parallelism for Scalable Million-Token Inference</a> - 추론을 위한 컨텍스트 레벨 병렬처리</li> <li><a href="https://arxiv.org/pdf/2406.18485">LoongTrain</a> - 매우 긴 시퀀스를 위한 2D 어텐션 병렬처리</li> </ul> <p><strong>메모리 최적화:</strong></p> <ul> <li><a href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> - 메모리 효율적인 훈련 기법</li> <li><a href="/blog/usp/">USP</a> - Ring과 Ulysses 방법을 결합한 통합 접근법</li> </ul> <hr/> <h1 id="takeaways">Takeaways</h1> <h2 id="1-근본적-문제-트랜스포머가-긴-시퀀스를-처리할-수-없는-이유">1. 근본적 문제: 트랜스포머가 긴 시퀀스를 처리할 수 없는 이유</h2> <h3 id="메모리-벽memory-wall-문제">메모리 벽(Memory Wall) 문제</h3> <p>기존 트랜스포머 모델은 제가 “메모리 벽”이라고 부르는 제곱 확장 문제로 인해 긴 시퀀스 처리가 계산상 불가능합니다.</p> <p><strong>문제의 수학적 분석:</strong></p> <ul> <li>시퀀스 길이: 100만 토큰</li> <li>어텐션 행렬 크기: 100만 × 100만 = 1조 개 원소</li> <li>필요 메모리: 1조 × 4바이트 = 4TB (어텐션 행렬만으로!)</li> </ul> <p><strong>실제 컨텍스트 비교:</strong></p> <ul> <li>GPT-3.5: 16K 토큰 (~32페이지 텍스트)</li> <li>Claude 2: 200K 토큰 (~400페이지)</li> <li>Gemini 1.5: 100만 토큰 (~2,000페이지)</li> <li><strong>Blockwise RingAttention</strong>: 1억+ 토큰 (~20만 페이지 또는 300권 이상의 책)</li> </ul> <h3 id="논문의-동기-논리">논문의 동기 논리</h3> <p>저자들은 기존 접근법의 세 가지 핵심 한계를 식별합니다:</p> <ol> <li><strong>메모리 병목</strong>: 메모리 효율적 어텐션(FlashAttention)을 사용해도 레이어 출력 저장이 금지적임</li> <li><strong>장치 제약</strong>: 최적화와 관계없이 단일 장치 메모리가 컨텍스트 길이를 제한</li> <li><strong>통신 오버헤드</strong>: 기존 분산 접근법은 상당한 계산 비용 추가</li> </ol> <p><strong>제 분석</strong>: 논문은 해결책이 단순히 어텐션 계산 최적화가 아니라, 여러 장치에서 계산을 분산하고 조율하는 방식을 근본적으로 재고하는 것임을 훌륭하게 인식했습니다.</p> <h2 id="2-blockwise-ringattention-솔루션-혁명적-접근법">2. Blockwise RingAttention 솔루션: 혁명적 접근법</h2> <h3 id="핵심-혁신-링-토폴로지--블록별-계산">핵심 혁신: 링 토폴로지 + 블록별 계산</h3> <p>논문은 두 가지 접근법을 도입합니다:</p> <ol> <li><strong>공간적 분산</strong>: 시퀀스를 장치들에 링 형태로 분산</li> <li><strong>시간적 중첩</strong>: 통신과 계산을 중첩</li> </ol> <p><strong>개념적 Python 코드:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ring_attention_concept</span><span class="p">():</span>
    <span class="c1"># 기존 방식 (불가능):
</span>    <span class="c1"># 장치 1: 전체 100만 토큰 시퀀스 처리 (불가능)
</span>    
    <span class="c1"># Blockwise RingAttention 방식:
</span>    <span class="n">장치수</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">장치당_토큰수</span> <span class="o">=</span> <span class="mi">1_000_000</span> <span class="o">//</span> <span class="n">장치수</span>  <span class="c1"># 각각 125K 토큰
</span>    
    <span class="c1"># 각 장치는 자신의 청크를 처리하면서 다른 장치와 K,V를 공유
</span>    <span class="k">for</span> <span class="n">장치_id</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">장치수</span><span class="p">):</span>
        <span class="n">로컬_토큰</span> <span class="o">=</span> <span class="n">토큰</span><span class="p">[</span><span class="n">장치_id</span> <span class="o">*</span> <span class="n">장치당_토큰수</span><span class="p">:(</span><span class="n">장치_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">장치당_토큰수</span><span class="p">]</span>
        
        <span class="c1"># 핵심 통찰: 각 장치는 링 통신을 통해 모든 토큰에 어텐션할 수 있지만
</span>        <span class="c1"># 로컬에는 시퀀스의 1/8만 저장
</span>        <span class="nf">process_local_chunk_with_ring_attention</span><span class="p">(</span><span class="n">로컬_토큰</span><span class="p">,</span> <span class="n">장치_id</span><span class="p">)</span>
</code></pre></div></div> <h3 id="성공을-위한-핵심-가정과-조건">성공을 위한 핵심 가정과 조건</h3> <p><strong>하드웨어 요구사항 (필수):</strong></p> <ol> <li><strong>고대역폭 인터커넥트</strong>: NVLink (600 GB/s) 또는 InfiniBand (200 Gb/s)</li> <li><strong>동기화된 장치</strong>: 모든 장치가 동기화되어 작동해야 함</li> <li><strong>충분한 장치별 메모리</strong>: 각 장치는 로컬 시퀀스 청크 + 버퍼용 메모리 필요</li> </ol> <p><strong>소프트웨어 요구사항:</strong></p> <ol> <li><strong>균등한 시퀀스 분할</strong>: 시퀀스 길이가 장치 수로 나누어떨어져야 함</li> <li><strong>적절한 인과 마스킹</strong>: 자기회귀 모델을 위한 글로벌 위치 추적</li> <li><strong>수치적 안정성</strong>: 온라인 소프트맥스 계산은 세심한 구현 필요</li> </ol> <p><strong>제 비판적 평가</strong>: 논문의 성공은 이러한 가정에 크게 의존합니다. 하드웨어 요구사항이 본질적으로 이 접근법을 자금이 풍부한 연구소와 대기업으로 제한한다는 점은 논문에서 충분히 다루지 않은 중요한 실용적 한계입니다.</p> <h2 id="3-상세-기술-구현">3. 상세 기술 구현</h2> <h3 id="단계별-알고리즘과-python-예제">단계별 알고리즘과 Python 예제</h3> <p><strong>1단계: 시퀀스 분할</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">partition_sequence_for_ring</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">num_devices</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    입력 시퀀스를 링 토폴로지의 장치들에 분할
    
    실제 예제: 100만 토큰 시퀀스를 8개 장치에 분할
    </span><span class="sh">"""</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>  <span class="c1"># 1,000,000 토큰
</span>    <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="n">num_devices</span>  <span class="c1"># 장치당 125,000 토큰
</span>    
    <span class="n">partitions</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">device_id</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_devices</span><span class="p">):</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">device_id</span> <span class="o">*</span> <span class="n">chunk_size</span>
        <span class="n">end_idx</span> <span class="o">=</span> <span class="n">start_idx</span> <span class="o">+</span> <span class="n">chunk_size</span>
        
        <span class="n">partitions</span><span class="p">[</span><span class="n">device_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">sequence</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">global_start</span><span class="sh">'</span><span class="p">:</span> <span class="n">start_idx</span><span class="p">,</span>  <span class="c1"># 인과 마스킹에 중요
</span>            <span class="sh">'</span><span class="s">global_end</span><span class="sh">'</span><span class="p">:</span> <span class="n">end_idx</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">device_id</span><span class="sh">'</span><span class="p">:</span> <span class="n">device_id</span>
        <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">partitions</span>
</code></pre></div></div> <p><strong>2단계: 링 통신 설정</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">setup_ring_communication</span><span class="p">(</span><span class="n">num_devices</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    링 토폴로지 생성: 0 -&gt; 1 -&gt; 2 -&gt; ... -&gt; N-1 -&gt; 0
    
    원형으로 메모를 전달하는 것처럼 생각하면 됨
    </span><span class="sh">"""</span>
    <span class="n">ring</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">device_id</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_devices</span><span class="p">):</span>
        <span class="n">ring</span><span class="p">[</span><span class="n">device_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">next_device</span><span class="sh">'</span><span class="p">:</span> <span class="p">(</span><span class="n">device_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_devices</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">prev_device</span><span class="sh">'</span><span class="p">:</span> <span class="p">(</span><span class="n">device_id</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_devices</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">position_in_ring</span><span class="sh">'</span><span class="p">:</span> <span class="n">device_id</span>
        <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">ring</span>
</code></pre></div></div> <p><strong>3단계: 핵심 Blockwise RingAttention 계산</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ring_attention_core</span><span class="p">(</span><span class="n">local_qkv</span><span class="p">,</span> <span class="n">ring_topology</span><span class="p">,</span> <span class="n">device_id</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Blockwise RingAttention의 핵심: 장치 링에서 어텐션 계산
    
    핵심 혁신: 전체 어텐션 행렬을 구체화하지 않고 온라인 소프트맥스 계산
    </span><span class="sh">"""</span>
    <span class="n">Q_local</span> <span class="o">=</span> <span class="n">local_qkv</span><span class="p">[</span><span class="sh">'</span><span class="s">Q</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># 형태: [local_seq_len, d_model]
</span>    <span class="n">K_local</span> <span class="o">=</span> <span class="n">local_qkv</span><span class="p">[</span><span class="sh">'</span><span class="s">K</span><span class="sh">'</span><span class="p">]</span> 
    <span class="n">V_local</span> <span class="o">=</span> <span class="n">local_qkv</span><span class="p">[</span><span class="sh">'</span><span class="s">V</span><span class="sh">'</span><span class="p">]</span>
    
    <span class="c1"># 온라인 소프트맥스용 누적기 초기화
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">Q_local</span><span class="p">)</span>
    <span class="n">row_max</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="n">Q_local</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>
    <span class="n">row_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">Q_local</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="n">num_devices</span> <span class="o">=</span> <span class="n">ring_topology</span><span class="p">[</span><span class="n">device_id</span><span class="p">][</span><span class="sh">'</span><span class="s">total_devices</span><span class="sh">'</span><span class="p">]</span>
    
    <span class="c1"># 링의 각 장치의 K,V 처리
</span>    <span class="k">for</span> <span class="n">ring_step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_devices</span><span class="p">):</span>
        <span class="n">kv_device_id</span> <span class="o">=</span> <span class="p">(</span><span class="n">device_id</span> <span class="o">+</span> <span class="n">ring_step</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_devices</span>
        
        <span class="k">if</span> <span class="n">ring_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># 첫 번째 단계: 로컬 K,V 사용
</span>            <span class="n">K_current</span> <span class="o">=</span> <span class="n">K_local</span>
            <span class="n">V_current</span> <span class="o">=</span> <span class="n">V_local</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># 후속 단계: 링에서 K,V 수신
</span>            <span class="n">K_current</span><span class="p">,</span> <span class="n">V_current</span> <span class="o">=</span> <span class="nf">receive_from_ring</span><span class="p">(</span><span class="n">ring_topology</span><span class="p">,</span> <span class="n">device_id</span><span class="p">)</span>
        
        <span class="c1"># 이 K,V 블록에 대한 어텐션 점수 계산
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">Q_local</span> <span class="o">@</span> <span class="n">K_current</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">Q_local</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># 안정성을 위한 스케일링
</span>        
        <span class="c1"># 글로벌 위치 기반 인과 마스킹 적용
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="nf">apply_global_causal_mask</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">local_qkv</span><span class="p">,</span> <span class="n">kv_device_id</span><span class="p">)</span>
        
        <span class="c1"># 온라인 소프트맥스 업데이트 (핵심 혁신)
</span>        <span class="n">output</span><span class="p">,</span> <span class="n">row_max</span><span class="p">,</span> <span class="n">row_sum</span> <span class="o">=</span> <span class="nf">online_softmax_update</span><span class="p">(</span>
            <span class="n">output</span><span class="p">,</span> <span class="n">row_max</span><span class="p">,</span> <span class="n">row_sum</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">V_current</span>
        <span class="p">)</span>
        
        <span class="c1"># 중첩: 계산하면서 현재 K,V를 다음 장치로 전송 시작
</span>        <span class="k">if</span> <span class="n">ring_step</span> <span class="o">&lt;</span> <span class="n">num_devices</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nf">async_send_to_ring</span><span class="p">(</span><span class="n">K_current</span><span class="p">,</span> <span class="n">V_current</span><span class="p">,</span> <span class="n">ring_topology</span><span class="p">,</span> <span class="n">device_id</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p><strong>제 기술적 통찰</strong>: 이 접근법의 훌륭함은 온라인 소프트맥스 계산에 있습니다. 기존 어텐션은 전체 어텐션 행렬 저장이 필요하지만, Blockwise RingAttention은 점진적 업데이트가 가능한 실행 통계를 유지합니다. 이는 수학적으로 동등하지만 훨씬 메모리 효율적입니다.</p> <h2 id="4-실험-결과-및-분석">4. 실험 결과 및 분석</h2> <h3 id="주요-성능-결과">주요 성능 결과</h3> <table> <thead> <tr> <th>하드웨어 설정</th> <th>모델 크기</th> <th>기준선 (BPT)</th> <th>Ring Attention</th> <th><strong>개선도</strong></th> <th><strong>제 해석</strong></th> </tr> </thead> <tbody> <tr> <td>8×A100 GPU</td> <td>7B</td> <td>128K 토큰</td> <td><strong>100만+ 토큰</strong></td> <td><strong>8배 개선</strong></td> <td><em>단일 노드 훈련 역량 변혁</em></td> </tr> <tr> <td>32×A100 GPU</td> <td>7B</td> <td>512K 토큰</td> <td><strong>1600만+ 토큰</strong></td> <td><strong>32배 개선</strong></td> <td><em>전체 코드베이스나 책 처리 가능</em></td> </tr> <tr> <td>TPUv4-512</td> <td>7B</td> <td>256K 토큰</td> <td><strong>6500만+ 토큰</strong></td> <td><strong>256배 개선</strong></td> <td><em>혁명적 규모 - 130권 이상 소설에 해당</em></td> </tr> <tr> <td>TPUv4-1024</td> <td>7B</td> <td>256K 토큰</td> <td><strong>1억3천만+ 토큰</strong></td> <td><strong>512배 개선</strong></td> <td><em>실용적 목적의 거의 무한한 컨텍스트</em></td> </tr> </tbody> </table> <p><strong>제 분석</strong>: 이러한 결과는 패러다임 전환을 나타냅니다. TPUv4-1024에서의 1억3천만 토큰 역량은 전체 해리포터 시리즈(110만 단어 ≈ 150만 토큰)를 87번 반복해서 단일 컨텍스트 윈도우에서 처리할 수 있음을 의미합니다. 이는 완전히 새로운 연구 방향을 열어줍니다.</p> <h3 id="모델-flops-활용률mfu-분석">모델 FLOPs 활용률(MFU) 분석</h3> <table> <thead> <tr> <th>하드웨어</th> <th>컨텍스트 길이</th> <th>BPT MFU</th> <th>Ring Attention MFU</th> <th><strong>효율성 증가</strong></th> <th><strong>제 평가</strong></th> </tr> </thead> <tbody> <tr> <td>8×A100</td> <td>512K</td> <td>28×10³</td> <td><strong>240×10³</strong></td> <td><strong>8.6배 향상</strong></td> <td><em>뛰어난 컴퓨팅 효율성</em></td> </tr> <tr> <td>32×A100</td> <td>1M</td> <td>24×10³</td> <td><strong>224×10³</strong></td> <td><strong>9.3배 향상</strong></td> <td><em>규모에서 효율성 유지</em></td> </tr> <tr> <td>TPUv4-512</td> <td>16M</td> <td>1024×10³</td> <td><strong>8192×10³</strong></td> <td><strong>8.0배 향상</strong></td> <td><em>TPU에서 일관된 확장</em></td> </tr> </tbody> </table> <p><strong>핵심 통찰</strong>: 8-9배 MFU 개선은 Blockwise RingAttention이 단순히 더 긴 컨텍스트를 가능하게 할 뿐만 아니라 이전 방법보다 더 효율적으로 수행한다는 것을 나타내므로 놀랍습니다. 이는 통신 오버헤드가 영리한 중첩을 통해 성공적으로 숨겨졌음을 시사합니다.</p> <h3 id="제거-연구ablation-study-결과">제거 연구(Ablation Study) 결과</h3> <table> <thead> <tr> <th>제거된 구성요소</th> <th>최대 컨텍스트</th> <th>성능 영향</th> <th><strong>제 해석</strong></th> </tr> </thead> <tbody> <tr> <td><strong>링 통신 없음</strong></td> <td>128K</td> <td>기준선</td> <td><em>링 토폴로지가 필수임을 확인</em></td> </tr> <tr> <td><strong>통신 중첩 없음</strong></td> <td>1M</td> <td>45% 느림</td> <td><em>중첩 전략이 중요함을 증명</em></td> </tr> <tr> <td><strong>최적이 아닌 블록 크기</strong></td> <td>1M</td> <td>20% 느림</td> <td><em>블록 크기 튜닝이 상당히 중요</em></td> </tr> <tr> <td><strong>전체 시스템</strong></td> <td><strong>1M+</strong></td> <td><strong>최적</strong></td> <td><em>모든 구성요소가 시너지 효과를 발휘</em></td> </tr> </tbody> </table> <p><strong>제 비판적 평가</strong>: 제거 연구는 각 구성요소가 필요하다는 것을 설득력 있게 보여줍니다. 하지만 실패 모드와 엣지 케이스에 대한 더 많은 분석을 보고 싶었습니다.</p> <h3 id="강화학습-검증">강화학습 검증</h3> <table> <thead> <tr> <th>방법</th> <th>AntMaze-Large</th> <th>Kitchen-Mixed</th> <th>Adroit-Human</th> <th><strong>평균</strong></th> <th><strong>제 평가</strong></th> </tr> </thead> <tbody> <tr> <td>BC 기준선</td> <td>0.45</td> <td>0.32</td> <td>0.28</td> <td><strong>0.35</strong></td> <td><em>표준 성능</em></td> </tr> <tr> <td>Decision Transformer</td> <td>0.52</td> <td>0.41</td> <td>0.35</td> <td><strong>0.43</strong></td> <td><em>적당한 개선</em></td> </tr> <tr> <td>AT + BPT</td> <td>0.65</td> <td>0.58</td> <td>0.51</td> <td><strong>0.58</strong></td> <td><em>강력한 기준선</em></td> </tr> <tr> <td><strong>AT + Ring Attention</strong></td> <td><strong>0.72</strong></td> <td><strong>0.64</strong></td> <td><strong>0.58</strong></td> <td><strong>0.65</strong></td> <td><em><strong>12% 개선 - 상당함</strong></em></td> </tr> </tbody> </table> <p><strong>제 분석</strong>: RL 결과는 Blockwise RingAttention의 이점이 언어 모델링을 넘어 확장된다는 것을 보여주므로 특히 설득력이 있습니다. 12% 개선은 더 긴 컨텍스트가 진정으로 순차적 의사결정을 개선한다는 것을 시사하며, 이는 로봇공학과 자율 시스템에 깊은 의미를 가집니다.</p> <h2 id="5-핵심-조건과-한계">5. 핵심 조건과 한계</h2> <h3 id="하드웨어-전제조건-논문의-아킬레스건">하드웨어 전제조건 (논문의 아킬레스건)</h3> <p><strong>대역폭 요구사항 분석:</strong></p> <ul> <li>시퀀스 길이: 100만 토큰</li> <li>모델 차원: 4096</li> <li>장치 수: 8개</li> <li>장치당 필요 대역폭: ~150 GB/s</li> </ul> <p><strong>실제 하드웨어와의 비교:</strong></p> <ul> <li>NVLink (600 GB/s): ✓ 호환</li> <li>InfiniBand (25 GB/s): ✗ 부족</li> <li>이더넷 (1.25 GB/s): ✗ 심각하게 부족</li> <li>PCIe (8 GB/s): ✗ 심각하게 부족</li> </ul> <p><strong>제 비판적 평가</strong>: 논문은 하드웨어 요구사항을 과소평가합니다. NVLink나 고급 InfiniBand가 본질적으로 필수이므로 실용적 채택이 자금이 풍부한 기관으로 제한됩니다.</p> <h3 id="동기화-문제">동기화 문제</h3> <p><strong>숨겨진 복잡성:</strong></p> <ol> <li>모든 장치가 각 링 단계를 완료한 후에만 다음 단계 시작 가능</li> <li>장치 실패나 속도 저하가 전체 파이프라인 차단</li> <li>이질적 하드웨어에서 부하 분산이 중요해짐</li> <li>네트워크 지터가 연쇄 지연 유발 가능</li> </ol> <p><strong>제 통찰</strong>: 논문은 “가장 약한 고리” 문제를 충분히 다루지 않습니다. 실제로는 하나의 느린 장치가 전체 성능을 크게 감소시킬 수 있습니다.</p> <h2 id="6-광범위한-의미와-미래-방향">6. 광범위한 의미와 미래 방향</h2> <h3 id="가능해진-혁신적-응용">가능해진 혁신적 응용</h3> <p><strong>과학 컴퓨팅:</strong></p> <ul> <li><strong>유전체학</strong>: 전체 게놈(30억 염기쌍)을 단일 컨텍스트에서 처리</li> <li><strong>기후 모델링</strong>: 수십 년의 연속 센서 데이터 분석</li> <li><strong>천문학</strong>: 수년간의 망원경 관측을 동시에 처리</li> </ul> <p><strong>창조 산업:</strong></p> <ul> <li><strong>영화 분석</strong>: 대화와 함께 전체 영화를 프레임별로 처리</li> <li><strong>문학</strong>: 완전한 책 시리즈의 주제와 패턴 분석</li> <li><strong>음악</strong>: 전체 오케스트라 컨텍스트로 교향곡 작곡</li> </ul> <p><strong>소프트웨어 엔지니어링:</strong></p> <ul> <li><strong>코드베이스 분석</strong>: 수백만 줄의 코드를 동시에 처리</li> <li><strong>버그 탐지</strong>: 전체 프로젝트 히스토리에서 패턴 분석</li> <li><strong>문서화</strong>: 전체 코드베이스에서 포괄적 문서 생성</li> </ul> <h3 id="미래-연구에-대한-제-예측">미래 연구에 대한 제 예측</h3> <ol> <li><strong>하이브리드 접근법</strong>: 더 큰 효율성을 위해 Blockwise RingAttention과 스파스 어텐션 패턴 결합</li> <li><strong>동적 링 토폴로지</strong>: 워크로드 특성에 반응하는 적응형 링 구조</li> <li><strong>계층적 링 시스템</strong>: 극한 규모 배포를 위한 다단계 링</li> <li><strong>하드웨어 공동 설계</strong>: 링 통신 패턴에 최적화된 맞춤형 인터커넥트</li> </ol> <h3 id="논문이-다뤄야-할-한계">논문이 다뤄야 할 한계</h3> <p><strong>경제적 실현가능성</strong>: 하드웨어 비용으로 인해 이 접근법이 대기업에만 접근 가능해져 AI 불평등을 악화시킬 수 있음</p> <p><strong>에너지 소비</strong>: 1억+ 토큰 시퀀스 처리의 환경 영향을 분석하지 않음</p> <p><strong>내결함성</strong>: 장치 실패나 네트워크 분할을 시스템이 어떻게 처리하는지에 대한 논의 없음</p> <h2 id="7-최종-평가">7. 최종 평가</h2> <h3 id="논문이-옳게-한-것">논문이 옳게 한 것</h3> <ol> <li><strong>혁명적 규모</strong>: 1억+ 토큰 컨텍스트 달성은 진정으로 혁신적</li> <li><strong>엄격한 엔지니어링</strong>: 온라인 소프트맥스와 통신 중첩은 훌륭한 최적화</li> <li><strong>광범위한 검증</strong>: 언어 모델링과 RL에서의 결과가 일반화 가능성 입증</li> <li><strong>선형 확장</strong>: 장치 수 확장이 우아하고 실용적</li> </ol> <h3 id="개선될-수-있는-것">개선될 수 있는 것</h3> <ol> <li><strong>접근성</strong>: 엘리트 기관을 넘어 이 기술을 어떻게 사용 가능하게 할지에 대한 더 많은 논의</li> <li><strong>실패 모드</strong>: 엣지 케이스와 시스템 견고성 분석</li> <li><strong>에너지 효율성</strong>: 환경 영향 고려사항</li> <li><strong>구현 세부사항</strong>: 재현을 시도하는 실무자를 위한 더 많은 지침</li> </ol> <h3 id="제-전체-평가">제 전체 평가</h3> <p>Blockwise RingAttention은 완전히 새로운 범주의 AI 응용을 가능하게 할 시퀀스 모델링의 근본적 돌파구를 나타냅니다. 하드웨어 요구사항이 즉시 채택을 제한하지만, 분산 어텐션 계산에 대한 핵심 통찰은 차세대 AI 시스템에 영향을 미칠 가능성이 높습니다.</p> <p>이 논문은 트랜스포머의 메모리 벽이 영리한 분산 컴퓨팅을 통해 극복될 수 있음을 성공적으로 보여주어, 이전에는 불가능했던 규모에서 정보를 처리하고 추론할 수 있는 AI 시스템으로의 문을 열었습니다. 이 작업은 더 능력 있고 포괄적인 AI 시스템으로의 진화에서 중요한 순간으로 기억될 가능성이 높습니다.</p> <p><strong>중요도 점수: 9/10</strong> - AI에서 가능한 것을 근본적으로 바꾸는 드문 논문으로, 명확한 실용적 영향과 광범위한 적용 가능성을 가짐.</p> <hr/>]]></content><author><name></name></author><category term="sequence-parallelism"/><category term="ring-attention"/><category term="memory-efficiency"/><category term="distributed-training"/><summary type="html"><![CDATA[Blockwise RingAttention solves the memory bottleneck for processing very long sequences by combining blockwise computation with ring communication.]]></summary></entry><entry><title type="html">DeepSpeed Ulysses</title><link href="https://sungyubkim.github.io/blog/deepspeed_ulysses/" rel="alternate" type="text/html" title="DeepSpeed Ulysses"/><published>2025-06-01T00:00:00+09:00</published><updated>2025-06-01T00:00:00+09:00</updated><id>https://sungyubkim.github.io/blog/deepspeed_ulysses</id><content type="html" xml:base="https://sungyubkim.github.io/blog/deepspeed_ulysses/"><![CDATA[<h1 id="tldr">TL;DR</h1> <blockquote> <p>DeepSpeed-Ulysses는 대형 Transformer 모델 훈련에서 “시퀀스 차원 스케일링 문제”를 해결하는 새로운 시퀀스 병렬처리 접근법을 제시합니다. <strong>주요 기여:</strong> (1) 영리한 데이터 재분배를 통해 4배 더 긴 시퀀스(1M+ 토큰) 훈련 가능, (2) 기존 O(N) 방법 대비 O(N/P) 통신 복잡도 달성, (3) 모델 품질 유지하면서 구성에 따라 1.5-3.5배 속도 향상 제공. <strong>핵심 혁신:</strong> all-to-all 통신을 사용하여 시퀀스 병렬 데이터를 헤드 병렬 계산으로 변환, 각 GPU가 어텐션 헤드의 부분집합에 대해 완전한 어텐션을 계산할 수 있게 함. <strong>중요한 제약:</strong> 구성에 따라 성능 향상이 크게 다름, sparse attention에서 성능 저하, 실험 검증의 통계적 엄밀성 부족.</p> </blockquote> <ul> <li>Paper Link: <a href="https://arxiv.org/pdf/2309.14509">https://arxiv.org/pdf/2309.14509</a></li> </ul> <hr/> <h1 id="related-papers">Related Papers</h1> <p><strong>시퀀스 병렬화 방법론:</strong></p> <ul> <li><a href="/blog/blockwise_ringattention/">Blockwise RingAttention</a> - 링 토폴로지를 활용한 시퀀스 병렬화</li> <li><a href="/blog/ring-self-attention/">Ring Self-Attention</a> - 시퀀스 병렬화 종합 분석</li> <li><a href="/blog/usp/">USP</a> - Ulysses와 Ring을 통합한 시퀀스 병렬화</li> </ul> <p><strong>긴 시퀀스 훈련:</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2406.18485">LoongTrain</a> - 2D 어텐션을 활용한 긴 시퀀스 훈련</li> <li><a href="https://arxiv.org/pdf/2411.01783">Context Parallelism for Scalable Million-Token Inference</a> - 컨텍스트 병렬화를 통한 추론</li> </ul> <p><strong>어텐션 최적화:</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2310.03294">DISTFLASHATTN</a> - 분산 FlashAttention 구현</li> <li><a href="https://arxiv.org/pdf/2311.09431">Striped Attention</a> - 효율적인 어텐션 분배 패턴</li> </ul> <p><strong>시스템 통합:</strong></p> <ul> <li><a href="/blog/tp/">Tensor Parallelism</a> - 텐서 병렬화와의 결합</li> <li><a href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> - 메모리 효율적인 훈련</li> </ul> <hr/> <h1 id="takeaways">Takeaways</h1> <p>문제 정의: 시퀀스 스케일링이 중요한 이유</p> <p>현재 AI 애플리케이션들은 점점 더 긴 컨텍스트 추론을 요구합니다:</p> <ul> <li><strong>대화형 AI</strong>: 확장된 대화에서 컨텍스트 유지</li> <li><strong>문서 분석</strong>: 전체 책 처리 (100K+ 단어)</li> <li><strong>과학 컴퓨팅</strong>: 유전체 시퀀스 분석 (수십억 염기쌍)</li> <li><strong>비디오 생성</strong>: 긴 시퀀스에서 시간적 관계 이해</li> </ul> <p>하지만 기존 병렬처리 전략들은 시퀀스 차원에서 실패합니다:</p> <ul> <li><strong>데이터 병렬처리</strong>: 배치 크기 확장, 시퀀스 길이 아님</li> <li><strong>텐서 병렬처리</strong>: 모델 너비 확장, 시퀀스 길이 아님</li> <li><strong>파이프라인 병렬처리</strong>: 모델 깊이 확장, 시퀀스 길이 아님</li> </ul> <p><strong>근본적 도전</strong>: 어텐션 계산은 O(N²) 메모리 복잡도를 가져 긴 시퀀스를 처리하기에 비용이 너무 큽니다.</p> <h2 id="해결책-어텐션-중심-시퀀스-병렬처리">해결책: 어텐션 중심 시퀀스 병렬처리</h2> <h3 id="핵심-혁신-데이터-재분배-전략">핵심 혁신: 데이터 재분배 전략</h3> <p>DeepSpeed-Ulysses는 영리한 통찰을 통해 시퀀스 병렬처리 문제를 변환합니다: 어텐션 연산 내에서 병렬화를 시도하는 대신, 데이터를 재분배하여 헤드 간 병렬 어텐션 계산을 가능하게 합니다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">deepspeed_ulysses_pipeline</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    핵심 변환을 보여주는 완전한 파이프라인
    </span><span class="sh">"""</span>
    <span class="c1"># 단계 1: 디바이스 간 시퀀스 분할
</span>    <span class="c1"># 입력: [batch, full_seq_len, hidden_dim]
</span>    <span class="c1"># 결과: 각 디바이스가 [batch, seq_len/P, hidden_dim] 보유
</span>    <span class="n">local_input</span> <span class="o">=</span> <span class="nf">partition_sequence</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
    
    <span class="c1"># 단계 2: QKV 로컬 계산 (embarrassingly parallel)
</span>    <span class="n">local_q</span><span class="p">,</span> <span class="n">local_k</span><span class="p">,</span> <span class="n">local_v</span> <span class="o">=</span> <span class="nf">compute_qkv</span><span class="p">(</span><span class="n">local_input</span><span class="p">)</span>
    
    <span class="c1"># 단계 3: ALL-TO-ALL 변환 (핵심 혁신)
</span>    <span class="c1"># 변환: 시퀀스 병렬 → 헤드 병렬
</span>    <span class="c1"># 이전: 각 디바이스가 부분 시퀀스, 모든 헤드 보유
</span>    <span class="c1"># 이후: 각 디바이스가 전체 시퀀스, 헤드 부분집합 보유
</span>    <span class="n">global_q</span><span class="p">,</span> <span class="n">global_k</span><span class="p">,</span> <span class="n">global_v</span> <span class="o">=</span> <span class="nf">all_to_all_redistribute</span><span class="p">(</span>
        <span class="n">local_q</span><span class="p">,</span> <span class="n">local_k</span><span class="p">,</span> <span class="n">local_v</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">world_size</span>
    <span class="p">)</span>
    
    <span class="c1"># 단계 4: 할당된 헤드에서 어텐션 계산 (디바이스 간 병렬)
</span>    <span class="c1"># 각 디바이스가 num_heads/P 헤드에 대해 완전한 어텐션 계산
</span>    <span class="n">attention_output</span> <span class="o">=</span> <span class="nf">compute_attention_heads</span><span class="p">(</span><span class="n">global_q</span><span class="p">,</span> <span class="n">global_k</span><span class="p">,</span> <span class="n">global_v</span><span class="p">)</span>
    
    <span class="c1"># 단계 5: ALL-TO-ALL 복원 (시퀀스 병렬처리 복원)
</span>    <span class="c1"># 변환: 헤드 병렬 → 시퀀스 병렬
</span>    <span class="n">final_output</span> <span class="o">=</span> <span class="nf">all_to_all_restore</span><span class="p">(</span><span class="n">attention_output</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">final_output</span>
</code></pre></div></div> <h3 id="작동-원리-수학적-통찰">작동 원리: 수학적 통찰</h3> <p>핵심 통찰은 multi-head attention이 자연스럽게 분해 가능하다는 것입니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 표준 어텐션: 모든 헤드를 함께 계산
</span><span class="k">def</span> <span class="nf">standard_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
    <span class="c1"># Q, K, V: [batch, seq_len, hidden_dim]
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
        <span class="n">q_h</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">head</span><span class="o">*</span><span class="n">head_dim</span><span class="p">:(</span><span class="n">head</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">head_dim</span><span class="p">]</span>
        <span class="n">k_h</span> <span class="o">=</span> <span class="n">K</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">head</span><span class="o">*</span><span class="n">head_dim</span><span class="p">:(</span><span class="n">head</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">head_dim</span><span class="p">]</span>  
        <span class="n">v_h</span> <span class="o">=</span> <span class="n">V</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">head</span><span class="o">*</span><span class="n">head_dim</span><span class="p">:(</span><span class="n">head</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">head_dim</span><span class="p">]</span>
        
        <span class="c1"># 이 계산은 헤드 간에 독립적입니다!
</span>        <span class="n">attn_h</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">q_h</span> <span class="o">@</span> <span class="n">k_h</span><span class="p">.</span><span class="n">T</span> <span class="o">/</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">head_dim</span><span class="p">))</span> <span class="o">@</span> <span class="n">v_h</span>
        <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">attn_h</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="nf">concat</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

<span class="c1"># DeepSpeed-Ulysses: 디바이스 간 헤드 분산
</span><span class="k">def</span> <span class="nf">distributed_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">device_heads</span><span class="p">):</span>
    <span class="c1"># 각 디바이스는 할당된 헤드만 계산
</span>    <span class="c1"># 하지만 해당 헤드들에 대해 전체 시퀀스를 봄
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">device_heads</span><span class="p">:</span>  <span class="c1"># 전체 헤드의 부분집합
</span>        <span class="n">attn_h</span> <span class="o">=</span> <span class="nf">compute_single_head_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">head</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">attn_h</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="nf">concat</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
</code></pre></div></div> <h2 id="중요한-가정과-조건">중요한 가정과 조건</h2> <h3 id="수학적-요구사항">수학적 요구사항</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 방법이 작동하기 위한 하드 제약:
</span><span class="k">assert</span> <span class="n">sequence_length</span> <span class="o">%</span> <span class="n">world_size</span> <span class="o">==</span> <span class="mi">0</span>  <span class="c1"># 균등 분할 필수
</span><span class="k">assert</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">world_size</span> <span class="o">==</span> <span class="mi">0</span>        <span class="c1"># 헤드 균등 분산 필수
</span><span class="k">assert</span> <span class="n">hidden_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>        <span class="c1"># 표준 어텐션 요구사항
</span>
<span class="c1"># 실패 사례 예시:
</span><span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># world_size=8로 나누어떨어지지 않음
# 패딩이나 불균등 분산이 필요하여 구현 복잡화
</span></code></pre></div></div> <h3 id="인프라-요구사항">인프라 요구사항</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">validate_infrastructure</span><span class="p">(</span><span class="n">bandwidth_gbps</span><span class="p">,</span> <span class="n">latency_ms</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    All-to-all 통신 효율성은 네트워크 토폴로지에 크게 의존
    </span><span class="sh">"""</span>
    <span class="c1"># 경험법칙: 높은 bisection bandwidth 필요
</span>    <span class="n">required_bandwidth</span> <span class="o">=</span> <span class="n">world_size</span> <span class="o">*</span> <span class="mi">10</span>  <span class="c1"># GB/s per device
</span>    <span class="k">if</span> <span class="n">bandwidth_gbps</span> <span class="o">&lt;</span> <span class="n">required_bandwidth</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">False</span><span class="p">,</span> <span class="sh">"</span><span class="s">네트워크 대역폭 부족</span><span class="sh">"</span>
    
    <span class="k">if</span> <span class="n">latency_ms</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">:</span>  <span class="c1"># 높은 지연시간은 작은 메시지 성능 저하
</span>        <span class="k">return</span> <span class="bp">False</span><span class="p">,</span> <span class="sh">"</span><span class="s">네트워크 지연시간 과다</span><span class="sh">"</span>
    
    <span class="k">return</span> <span class="bp">True</span><span class="p">,</span> <span class="sh">"</span><span class="s">인프라 적합</span><span class="sh">"</span>
</code></pre></div></div> <h3 id="부하-균형-가정">부하 균형 가정</h3> <p>이 방법은 다음에 대한 균등한 계산 부하를 가정합니다:</p> <ul> <li>시퀀스 청크 (구조화된 데이터에서는 성립하지 않을 수 있음)</li> <li>어텐션 헤드 (일반적으로 참이지만 보장되지 않음)</li> <li>디바이스 (동질적 하드웨어 필요)</li> </ul> <h2 id="실험-결과-비판적-분석">실험 결과: 비판적 분석</h2> <h3 id="주요-성능-결과">주요 성능 결과</h3> <table> <thead> <tr> <th>구성</th> <th>DeepSpeed-Ulysses</th> <th>Megatron-LM</th> <th>속도향상</th> <th>현실 검증</th> </tr> </thead> <tbody> <tr> <td><strong>7B 모델, 8K seq</strong></td> <td>175 TFLOPs</td> <td>105 TFLOPs</td> <td>1.67x</td> <td><strong>좋음</strong>: 견고한 개선</td> </tr> <tr> <td><strong>7B 모델, 32K seq</strong></td> <td>175 TFLOPs</td> <td>85 TFLOPs</td> <td>2.06x</td> <td><strong>주장에 근접</strong></td> </tr> <tr> <td><strong>7B 모델, 128K seq</strong></td> <td>165 TFLOPs</td> <td>OOM</td> <td>∞</td> <td><strong>능력 해제</strong></td> </tr> <tr> <td><strong>30B 모델, 8K seq</strong></td> <td>165 TFLOPs</td> <td>45 TFLOPs</td> <td>3.67x</td> <td><strong>주장 초과</strong></td> </tr> <tr> <td><strong>7B Sparse, 256K seq</strong></td> <td>65 TFLOPs</td> <td>OOM</td> <td>∞</td> <td><strong>성능 붕괴 우려</strong></td> </tr> </tbody> </table> <p><strong>핵심 통찰:</strong></p> <ul> <li><strong>가변 성능</strong>: 속도향상이 1.35x에서 3.67x까지 다양, 균일한 “2.5x” 주장과 모순</li> <li><strong>능력 vs 성능</strong>: 명확한 능력 해제 (더 긴 시퀀스) vs 혼재된 성능 향상</li> <li><strong>Sparse Attention 문제</strong>: 상당한 성능 저하로 “attention-agnostic” 주장이 과장됨</li> </ul> <h3 id="스케일링-분석">스케일링 분석</h3> <table> <thead> <tr> <th>연구 유형</th> <th>구성</th> <th>효율성</th> <th>해석</th> </tr> </thead> <tbody> <tr> <td><strong>Strong Scaling</strong></td> <td>131K seq, 64→256 GPUs</td> <td>165→136 TFLOPs (18% 손실)</td> <td><strong>통신 오버헤드</strong>가 O(N/P) 이론과 모순</td> </tr> <tr> <td><strong>Proportional Scaling</strong></td> <td>Seq∝GPUs, 65K→262K</td> <td>161→147 TFLOPs (9% 손실)</td> <td><strong>더 나은 그러나 완벽하지 않은</strong> 스케일링</td> </tr> </tbody> </table> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 스케일링 결과는 숨겨진 비용을 드러냄:
</span><span class="k">def</span> <span class="nf">real_communication_complexity</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    실제 통신은 오버헤드 요소들을 포함
    </span><span class="sh">"""</span>
    <span class="n">theoretical</span> <span class="o">=</span> <span class="n">N</span> <span class="o">/</span> <span class="n">P</span>
    <span class="n">network_contention</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">P</span>  <span class="c1"># 디바이스 수에 따라 증가
</span>    <span class="k">return</span> <span class="n">theoretical</span> <span class="o">+</span> <span class="n">network_contention</span>

<span class="c1"># 이것이 strong scaling 실험에서 효율성 손실을 설명함
</span></code></pre></div></div> <h3 id="통계적-엄밀성-부족">통계적 엄밀성 부족</h3> <p><strong>중요한 결함</strong>: 오차막대, 신뢰구간, 또는 다중 실행 보고 없음. 성능 주장을 하는 시스템 논문에서 이는 신뢰성을 훼손합니다.</p> <h2 id="강점과-제약">강점과 제약</h2> <h3 id="주요-강점">주요 강점</h3> <ol> <li><strong>근본적 능력 해제</strong>: 이전보다 4배 긴 시퀀스 훈련 가능</li> <li><strong>품질 보존</strong>: 수렴 연구에서 모델 품질에 영향 없음을 확인</li> <li><strong>광범위한 적용성</strong>: 다양한 모델 크기에서 작동, 기존 최적화와 통합</li> <li><strong>이론적 기반</strong>: 시퀀스 병렬처리를 위한 통신 복잡도 분석 제공</li> </ol> <h3 id="중요한-제약">중요한 제약</h3> <ol> <li><strong>성능 불일치</strong>: 구성에 따라 개선이 극적으로 다름</li> <li><strong>인프라 의존성</strong>: 고대역폭, 저지연 네트워크 필요</li> <li><strong>Sparse Attention 문제</strong>: 일반성 주장과 모순되는 상당한 성능 저하</li> <li><strong>실험 엄밀성</strong>: 성능 주장의 통계적 검증 부족</li> </ol> <h3 id="숨겨진-복잡성">숨겨진 복잡성</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">performance_prediction</span><span class="p">(</span><span class="n">model_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">network_quality</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    성능 향상은 단순한 메트릭으로 예측 불가능
    </span><span class="sh">"""</span>
    <span class="c1"># 실제 성능에 영향을 주는 요소들:
</span>    <span class="n">communication_ratio</span> <span class="o">=</span> <span class="nf">estimate_comm_vs_compute</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
    <span class="n">memory_pressure</span> <span class="o">=</span> <span class="nf">check_memory_bottlenecks</span><span class="p">(</span><span class="n">model_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
    <span class="n">network_efficiency</span> <span class="o">=</span> <span class="nf">evaluate_all_to_all_performance</span><span class="p">(</span><span class="n">network_quality</span><span class="p">)</span>
    <span class="n">load_balance</span> <span class="o">=</span> <span class="nf">assess_workload_distribution</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
    
    <span class="c1"># 성능은 모든 요소들의 복잡한 상호작용에 의존
</span>    <span class="k">return</span> <span class="nf">complex_interaction</span><span class="p">(</span><span class="n">communication_ratio</span><span class="p">,</span> <span class="n">memory_pressure</span><span class="p">,</span> 
                             <span class="n">network_efficiency</span><span class="p">,</span> <span class="n">load_balance</span><span class="p">)</span>
</code></pre></div></div> <h2 id="실용적-함의">실용적 함의</h2> <h3 id="deepspeed-ulysses-사용-시점">DeepSpeed-Ulysses 사용 시점</h3> <p><strong>강력한 후보:</strong></p> <ul> <li>32K 토큰 이상의 시퀀스 훈련</li> <li>Dense attention 패턴</li> <li>고대역폭 클러스터 인프라</li> <li>원시 성능보다 능력 해제(더 긴 시퀀스)가 중요한 애플리케이션</li> </ul> <p><strong>부적합한 후보:</strong></p> <ul> <li>짧은 시퀀스 (&lt; 8K 토큰) - 오버헤드가 지배적</li> <li>Sparse attention 패턴 - 성능 저하</li> <li>제한된 네트워크 대역폭 - 통신이 병목</li> <li>지연시간에 민감한 추론 - 이 용도로 설계되지 않음</li> </ul> <h3 id="구현-체크리스트">구현 체크리스트</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">deployment_readiness_check</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    성공적인 배포를 위한 전제조건
    </span><span class="sh">"""</span>
    <span class="n">checks</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">sequence_divisibility</span><span class="sh">"</span><span class="p">:</span> <span class="n">sequence_length</span> <span class="o">%</span> <span class="n">world_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">head_divisibility</span><span class="sh">"</span><span class="p">:</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">world_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">network_bandwidth</span><span class="sh">"</span><span class="p">:</span> <span class="n">bandwidth</span> <span class="o">&gt;</span> <span class="n">world_size</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span>  <span class="c1"># GB/s
</span>        <span class="sh">"</span><span class="s">memory_capacity</span><span class="sh">"</span><span class="p">:</span> <span class="nf">can_fit_attention_matrix</span><span class="p">(),</span>
        <span class="sh">"</span><span class="s">load_balance</span><span class="sh">"</span><span class="p">:</span> <span class="nf">validate_uniform_hardware</span><span class="p">(),</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="nf">all</span><span class="p">(</span><span class="n">checks</span><span class="p">.</span><span class="nf">values</span><span class="p">()),</span> <span class="n">checks</span>
</code></pre></div></div> <h2 id="향후-방향과-연구-기회">향후 방향과 연구 기회</h2> <h3 id="기술적-개선">기술적 개선</h3> <ol> <li><strong>적응적 통신</strong>: 네트워크 조건에 기반한 동적 조정</li> <li><strong>이기종 최적화</strong>: 혼합 하드웨어 환경 지원</li> <li><strong>Sparse Attention 통합</strong>: 구조화된 sparsity 패턴에 대한 더 나은 지원</li> <li><strong>메모리 최적화</strong>: all-to-all 연산 중 최대 메모리 감소</li> </ol> <h3 id="이론적-확장">이론적 확장</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 잠재적 연구 방향:
</span><span class="k">def</span> <span class="nf">future_work_opportunities</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">최적 헤드 분산 전략</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">통신-계산 중첩 기법</span><span class="sh">"</span><span class="p">,</span> 
        <span class="sh">"</span><span class="s">다른 병렬처리 차원과의 통합</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">실제 네트워크 효과의 이론적 분석</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">다른 어텐션 메커니즘으로의 확장 (예: cross-attention)</span><span class="sh">"</span>
    <span class="p">]</span>
</code></pre></div></div> <h3 id="더-넓은-영향">더 넓은 영향</h3> <p>이 연구는 Transformer 훈련 스케일링에 대한 사고방식의 <strong>패러다임 전환</strong>을 나타냅니다:</p> <ul> <li><strong>이전</strong>: 배치 크기, 모델 크기, 또는 깊이 스케일링</li> <li><strong>이후</strong>: 시퀀스 길이를 일급 시민으로 스케일링</li> </ul> <p>이는 완전히 새로운 애플리케이션 클래스와 긴 컨텍스트 AI 시스템의 연구 방향을 가능하게 합니다.</p> <h2 id="최종-평가">최종 평가</h2> <p>DeepSpeed-Ulysses는 AI 시스템 환경에서 실제적이고 중요한 문제를 해결하는 <strong>중요한 기여</strong>입니다. 실험 검증에 공백이 있고 성능 주장이 다소 과장되었지만, 핵심 혁신은 건전하고 능력 해제는 진정한 것입니다.</p> <p><strong>실무자들을 위해</strong>: 특정 사용 사례(긴 시퀀스, dense attention, 좋은 인프라)에는 가치 있는 도구이지만 범용 솔루션은 아닙니다.</p> <p><strong>연구자들을 위해</strong>: 어텐션 중심 병렬처리 접근법은 시스템 최적화에서 새로운 길을 열고 시퀀스 스케일링의 향후 연구를 위한 기반을 제공합니다.</p> <p><strong>결론</strong>: 새로운 능력을 가능하게 하는 AI 시스템의 의미 있는 발전이며, 구현 견고성과 실험 엄밀성 모두에서 개선의 여지가 있습니다.</p> <hr/>]]></content><author><name></name></author><category term="sequence-parallelism"/><category term="distributed-training"/><summary type="html"><![CDATA[DeepSpeed-Ulysses achieves efficient sequence parallelism via all-to-all communication for 1M+ token training.]]></summary></entry><entry><title type="html">Reducing Activation Recomputation in Large Transformer Models</title><link href="https://sungyubkim.github.io/blog/sp/" rel="alternate" type="text/html" title="Reducing Activation Recomputation in Large Transformer Models"/><published>2025-06-01T00:00:00+09:00</published><updated>2025-06-01T00:00:00+09:00</updated><id>https://sungyubkim.github.io/blog/sp</id><content type="html" xml:base="https://sungyubkim.github.io/blog/sp/"><![CDATA[<h1 id="tldr">TL;DR</h1> <blockquote> <p><strong>문제</strong>: 대규모 트랜스포머 모델(100B+ 매개변수) 훈련 시 중대한 병목현상 발생—그래디언트 계산을 위한 활성화(activation) 저장이 엄청난 메모리를 소모하여, 비싼 “활성화 재계산(activation recomputation)”을 강요하고 이는 30-40%의 훈련 시간 오버헤드를 추가함.</p> <p><strong>해결책</strong>: 함께 5배 메모리 감소와 30% 속도 향상을 달성하는 두 가지 상호보완적 기법:</p> <ol> <li><strong>시퀀스 병렬화(Sequence Parallelism)</strong>: 텐서 병렬화가 처리할 수 없는 연산에서 시퀀스 차원을 따라 활성화를 분할</li> <li><strong>선택적 활성화 재계산(Selective Activation Recomputation)</strong>: 특정한 고메모리, 저계산 어텐션 연산만 재계산</li> </ol> <p><strong>영향</strong>: 2240개 A100에서 54% GPU 사용률로 1조 매개변수 모델 훈련 가능—이전에는 불가능했던 규모를 실용적으로 만듦.</p> </blockquote> <ul> <li>Paper Link: <a href="https://arxiv.org/pdf/2205.05198">https://arxiv.org/pdf/2205.05198</a></li> </ul> <hr/> <h1 id="related-papers">Related Papers</h1> <p><strong>메모리 최적화 기법:</strong></p> <ul> <li><a href="/blog/tp/">Tensor Parallelism</a> - 텐서 병렬화와 메모리 효율성 결합</li> <li><a href="/blog/pp/">GPipe</a> - 파이프라인 병렬화에서의 메모리 최적화</li> <li><a href="/blog/ring-self-attention/">Ring Self-Attention</a> - 시퀀스 병렬화와 메모리 관리</li> </ul> <p><strong>대규모 모델 훈련:</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2101.03961">Switch Transformers</a> - 대규모 MoE 모델의 메모리 효율성</li> <li><a href="/blog/moe/">MoE</a> - 전문가 혼합 모델의 메모리 요구사항</li> <li><a href="https://arxiv.org/pdf/2406.18485">LoongTrain</a> - 긴 시퀀스 훈련에서의 메모리 최적화</li> </ul> <p><strong>분산 훈련 시스템:</strong></p> <ul> <li><a href="/blog/deepspeed_ulysses/">DeepSpeed Ulysses</a> - 시퀀스 병렬화와 메모리 효율성</li> <li><a href="/blog/usp/">USP</a> - 통합 병렬화 프레임워크에서의 메모리 관리</li> <li><a href="/blog/blockwise_ringattention/">Blockwise RingAttention</a> - 어텐션 계산의 메모리 효율성</li> </ul> <hr/> <h1 id="takeaways">Takeaways</h1> <h2 id="근본적-문제-트랜스포머-훈련의-메모리-벽">근본적 문제: 트랜스포머 훈련의 메모리 벽</h2> <p>메모리에 맞지 않을 정도로 큰 모델을 훈련한다고 상상해보세요. 전통적인 해결책은 <strong>활성화 재계산</strong>(그래디언트 체크포인팅)입니다—역전파를 위한 중간 계산을 저장하는 대신, 버리고 나중에 다시 계산하는 것입니다. 이는 메모리를 절약하지만 본질적으로 순전파를 두 번 실행해야 하므로 엄청난 계산 오버헤드를 추가합니다.</p> <p><strong>문제의 규모:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 530B 매개변수 모델의 경우
</span><span class="n">activation_memory</span> <span class="o">=</span> <span class="mi">34</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">batch</span> <span class="o">*</span> <span class="n">hidden</span> <span class="o">+</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">heads</span> <span class="o">*</span> <span class="n">seq_len</span><span class="err">²</span>
<span class="c1"># ≈ 레이어당 160GB (최적화 없이)
# vs. 80GB A100 GPU 메모리 용량
</span></code></pre></div></div> <p>이는 고통스러운 트레이드오프를 강요합니다: 비싼 재계산을 사용하거나 모델을 전혀 훈련할 수 없거나.</p> <h2 id="핵심-혁신-1-시퀀스-병렬화">핵심 혁신 1: 시퀀스 병렬화</h2> <p><strong>통찰</strong>: 텐서 병렬화는 계산 집약적인 연산(행렬 곱셈)에는 훌륭하지만 간단한 연산(레이어 정규화, 드롭아웃)은 장치 간에 복제되어 메모리를 낭비합니다.</p> <p><strong>해결책</strong>: 이러한 “간단한” 연산들은 시퀀스 차원을 따라 독립적이므로, 대신 그 차원에서 분할할 수 있습니다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 전통적인 텐서 병렬화
</span><span class="k">def</span> <span class="nf">tensor_parallel_layer</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># x: [seq_len, batch, hidden]
</span>    <span class="c1"># 레이어 정규화가 각 장치에서 전체 데이터로 실행됨 (낭비적!)
</span>    <span class="n">x_norm</span> <span class="o">=</span> <span class="nf">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 각 랭크에서 [seq_len, batch, hidden]
</span>    
    <span class="c1"># 행렬 곱셈은 효율적으로 분할됨
</span>    <span class="n">x_split</span> <span class="o">=</span> <span class="nf">matmul_tensor_parallel</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span>  <span class="c1"># [seq_len, batch, hidden/num_ranks]
</span>    <span class="k">return</span> <span class="n">x_split</span>

<span class="c1"># 시퀀스 병렬화 적용
</span><span class="k">def</span> <span class="nf">sequence_tensor_parallel_layer</span><span class="p">(</span><span class="n">x_seq</span><span class="p">):</span>  <span class="c1"># x_seq: [seq_len/num_ranks, batch, hidden]
</span>    <span class="c1"># 시퀀스 분할 데이터에서 레이어 정규화 (메모리 효율적!)
</span>    <span class="n">x_norm</span> <span class="o">=</span> <span class="nf">layer_norm</span><span class="p">(</span><span class="n">x_seq</span><span class="p">)</span>  <span class="c1"># 랭크당 [seq_len/num_ranks, batch, hidden]
</span>    
    <span class="c1"># 필요시 텐서 병렬로 변환
</span>    <span class="n">x_full</span> <span class="o">=</span> <span class="nf">all_gather</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span>  <span class="c1"># 각 랭크에서 [seq_len, batch, hidden]
</span>    <span class="n">x_split</span> <span class="o">=</span> <span class="nf">matmul_tensor_parallel</span><span class="p">(</span><span class="n">x_full</span><span class="p">)</span>
    
    <span class="c1"># 시퀀스 병렬로 다시 변환
</span>    <span class="k">return</span> <span class="nf">reduce_scatter</span><span class="p">(</span><span class="n">x_split</span><span class="p">)</span>  <span class="c1"># 랭크당 [seq_len/num_ranks, batch, hidden]
</span></code></pre></div></div> <p><strong>핵심 수학적 통찰</strong>: 통신 비용이 동일합니다—<code class="language-plaintext highlighter-rouge">all_reduce = reduce_scatter + all_gather</code>—따라서 대역폭 오버헤드가 없습니다.</p> <h2 id="핵심-혁신-2-선택적-활성화-재계산">핵심 혁신 2: 선택적 활성화 재계산</h2> <p><strong>통찰</strong>: 모든 연산이 동등하지 않습니다. 일부는 많은 메모리를 소모하지만 재생성하는 데 최소한의 계산만 필요합니다.</p> <p><strong>대상 연산</strong>: 어텐션에서 Q, K, V 계산 후:</p> <ul> <li>QK^T 행렬 곱셈 → 소프트맥스 → 드롭아웃 → V에 대한 어텐션</li> <li>대형 모델에서 활성화 메모리의 ~70%를 차지</li> <li>하지만 전체 계산의 ~3%만 차지 (대부분 원소별 연산)</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">selective_attention_recomputation</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="c1"># 저장: Q, K, V (메모리 집약적 연산의 입력)
</span>    <span class="n">q_checkpoint</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">k_checkpoint</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> 
    <span class="n">v_checkpoint</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># 재계산: 고메모리, 저계산 연산들
</span>    <span class="k">def</span> <span class="nf">recompute_expensive_memory_ops</span><span class="p">(</span><span class="n">q_in</span><span class="p">,</span> <span class="n">k_in</span><span class="p">,</span> <span class="n">v_in</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q_in</span><span class="p">,</span> <span class="n">k_in</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># QK^T
</span>        <span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>                   <span class="c1"># 소프트맥스
</span>        <span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>           <span class="c1"># 드롭아웃
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">v_in</span><span class="p">)</span>                  <span class="c1"># V에 대한 어텐션
</span>        <span class="k">return</span> <span class="n">output</span>
    
    <span class="c1"># 자동 재계산을 위한 PyTorch 체크포인트 사용
</span>    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">checkpoint</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span>
        <span class="n">recompute_expensive_memory_ops</span><span class="p">,</span> <span class="n">q_checkpoint</span><span class="p">,</span> <span class="n">k_checkpoint</span><span class="p">,</span> <span class="n">v_checkpoint</span>
    <span class="p">)</span>
</code></pre></div></div> <p><strong>메모리 수학</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>원래: 34*s*b*h + 5*a*s²*b  (모든 것 저장)
선택적: 34*s*b*h             (저메모리/고계산 연산만 저장)
감소: 5*a*s²*b 항 제거 (시퀀스 길이 제곱 스케일링 제거!)
</code></pre></div></div> <h2 id="실험-결과-숫자의-진정한-의미">실험 결과: 숫자의 진정한 의미</h2> <h3 id="주요-성능-결과">주요 성능 결과</h3> <p>| 모델 크기 | 메모리 감소 | 훈련 속도 향상 | GPU 사용률 | <strong>실용적 영향</strong> | |———–|————-|—————-|————|—————–| | 22B | 5배 | 29.0% | 41.5% → 43.7% | <strong>겨우 맞던 모델이 이제 편안하게 훈련됨</strong> | | 175B (GPT-3) | 5배 | 31.8% | 51.4% → 52.8% | <strong>10만 달러 훈련 → 7만 6천 달러 (2만 4천 달러 절약)</strong> | | 530B (MT-NLG) | 5배 | 29.7% | 56.0% → 57.0% | <strong>표준 클러스터에서 훈련 가능</strong> | | 1T | 5배 | 32.1% | 56.3% → 57.0% | <strong>1조 매개변수 모델을 실용적으로 만듦</strong> |</p> <h3 id="구성요소-분석-각-기법의-기여도-이해">구성요소 분석: 각 기법의 기여도 이해</h3> <p>| 기법 | 메모리 절약 | 속도 영향 | <strong>핵심 통찰</strong> | |——|————-|———–|—————| | 시퀀스 병렬화만 | ~50% | <strong>+3% 속도 향상</strong> | 메모리 감소 <em>및</em> 성능 향상—레이어 정규화/드롭아웃이 예상보다 비쌌음 | | 선택적 재계산만 | ~50% | <strong>+7% 오버헤드</strong> | 동일한 메모리 절약이지만 다른 성능 프로필—서로 다른 병목 최적화 | | 둘 모두 결합 | <strong>80% (5배)</strong> | <strong>+4% 오버헤드</strong> | 시너지 효과—이익이 선형적으로 결합되는 것보다 좋음 | | 전통적 재계산 | 90% | <strong>+39% 오버헤드</strong> | 현상 유지가 극도로 비싸다는 것을 검증 |</p> <p><strong>놀라운 발견</strong>: 시퀀스 병렬화는 통신을 추가함에도 불구하고 실제로 훈련을 <em>가속화</em>합니다. 이는 레이어 정규화와 드롭아웃이 예상보다 계산적으로 비싸다는 것을 보여줍니다.</p> <h2 id="비판적-평가-장점과-한계">비판적 평가: 장점과 한계</h2> <h3 id="주요-장점">주요 장점</h3> <ol> <li><strong>수학적 엄밀성</strong>: 다양한 병렬화 전략 하에서 메모리 스케일링에 대한 정확한 공식 제공</li> <li><strong>일관된 스케일링</strong>: 22B에서 1T 매개변수까지 30% 향상 유지—근본적 최적화를 시사</li> <li><strong>실용적 구현</strong>: 프로덕션 프레임워크(Megatron-LM, NeMo)에서 사용 가능</li> <li><strong>이론적 검증</strong>: 하드웨어 FLOPs가 예측된 모델 FLOPs와 밀접하게 일치하여 수학적 모델 확인</li> </ol> <h3 id="중요한-한계점">중요한 한계점</h3> <ol> <li><strong>실험적 현실성</strong>: 데이터 병렬화 미사용 (프로덕션 훈련에 비현실적)</li> <li><strong>기준선 완전성</strong>: ZeRO, 매개변수 샤딩, CPU 오프로딩과의 비교 누락</li> <li><strong>하드웨어 특이성</strong>: A100에서만 테스트—일반화 불분명</li> <li><strong>스케일 제약</strong>: 시퀀스 길이가 텐서 병렬 크기로 나누어떨어져야 함</li> </ol> <h3 id="성공을-위한-숨겨진-가정들">성공을 위한 숨겨진 가정들</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 종종 간과되는 중요한 요구사항들:
</span><span class="k">assert</span> <span class="n">sequence_length</span> <span class="o">%</span> <span class="n">tensor_parallel_size</span> <span class="o">==</span> <span class="mi">0</span>  <span class="c1"># 균등하게 나누어떨어져야 함
</span><span class="k">assert</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">memory_fragmentation</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.1</span>     <span class="c1"># 낮은 단편화 필요
</span><span class="k">assert</span> <span class="nf">all_ranks_synchronized</span><span class="p">()</span>                     <span class="c1"># 완벽한 동기화 필요
</span><span class="k">assert</span> <span class="nf">batch_sequences_uniform_length</span><span class="p">()</span>             <span class="c1"># 가변 길이 문제 있음
</span></code></pre></div></div> <h2 id="실용적-구현-가이드">실용적 구현 가이드</h2> <h3 id="가장-잘-작동하는-경우">가장 잘 작동하는 경우</h3> <ul> <li><strong>대형 모델</strong> (&gt;100B 매개변수) 활성화 메모리가 지배적인 경우</li> <li><strong>긴 시퀀스</strong> 5<em>a</em>s²/h 항이 중요한 경우</li> <li><strong>고대역폭 상호연결</strong> (NVLink, InfiniBand) 효율적인 통신을 위해</li> <li><strong>균등한 워크로드</strong> 일관된 시퀀스 길이</li> </ul> <h3 id="주의해야-할-경우">주의해야 할 경우</h3> <ul> <li><strong>가변 시퀀스 길이</strong>: 계산을 낭비할 수 있는 패딩 필요</li> <li><strong>소형 모델</strong>: 오버헤드가 이익을 상회할 수 있음</li> <li><strong>메모리 제약 환경</strong>: 추가 기법(ZeRO, 오프로딩) 필요할 수 있음</li> <li><strong>레거시 하드웨어</strong>: 현대 가속기에 최적화된 통신 패턴</li> </ul> <h3 id="통합-전략">통합 전략</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 권장 구현 방식:
</span><span class="k">def</span> <span class="nf">adaptive_optimization_strategy</span><span class="p">(</span><span class="n">model_size</span><span class="p">,</span> <span class="n">available_memory</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">model_size</span> <span class="o">&gt;</span> <span class="mf">100e9</span><span class="p">:</span>  <span class="c1"># &gt;100B 매개변수
</span>        <span class="n">use_sequence_parallelism</span> <span class="o">=</span> <span class="bp">True</span>
        
        <span class="n">memory_ratio</span> <span class="o">=</span> <span class="nf">estimate_memory_usage</span><span class="p">()</span> <span class="o">/</span> <span class="n">available_memory</span>
        <span class="k">if</span> <span class="n">memory_ratio</span> <span class="o">&gt;</span> <span class="mf">0.9</span><span class="p">:</span>
            <span class="n">use_selective_recomputation</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">elif</span> <span class="n">memory_ratio</span> <span class="o">&gt;</span> <span class="mf">0.7</span><span class="p">:</span>
            <span class="n">use_microbatch_recomputation</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">use_selective_recomputation</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># 작은 모델의 경우, 이익이 복잡성을 정당화하지 못할 수 있음
</span>        <span class="k">return</span> <span class="sh">"</span><span class="s">standard_tensor_parallelism</span><span class="sh">"</span>
</code></pre></div></div> <h2 id="미래-방향과-연구-기회">미래 방향과 연구 기회</h2> <h3 id="즉시-확장-가능한-영역">즉시 확장 가능한 영역</h3> <ol> <li><strong>동적 시퀀스 병렬화</strong>: 가변 길이 시퀀스를 효율적으로 처리</li> <li><strong>하이브리드 메모리 전략</strong>: ZeRO 및 CPU 오프로딩과 결합</li> <li><strong>하드웨어 최적화</strong>: 다양한 가속기 아키텍처에 맞춘 튜닝</li> <li><strong>자동화된 최적화</strong>: ML 가이드 재계산 전략 선택</li> </ol> <h3 id="근본적-질문들">근본적 질문들</h3> <ol> <li><strong>일반화</strong>: 이러한 패턴이 다른 아키텍처(Vision Transformer 등)에 적용되는가?</li> <li><strong>스케일링 한계</strong>: 1조 매개변수 모델을 넘어서면 어떻게 되는가?</li> <li><strong>통신 진화</strong>: 미래의 상호연결이 최적 전략을 어떻게 바꿀 것인가?</li> </ol> <h2 id="큰-그림">큰 그림</h2> <p>이 연구는 “일률적인” 최적화에서 <strong>연산 인식 메모리 관리</strong>로의 <strong>패러다임 전환</strong>을 나타냅니다. 서로 다른 연산이 서로 다른 메모리-계산 트레이드오프를 가진다는 것을 인식함으로써, 훨씬 더 정교한 최적화 전략의 문을 엽니다.</p> <p><strong>연구자들을 위해</strong>: 대규모 훈련에서 메모리 병목을 분석하기 위한 수학적 프레임워크 제공</p> <p><strong>실무자들을 위해</strong>: 훈련 효율성을 즉시 개선할 수 있는 프로덕션 준비된 기법 제공</p> <p><strong>분야 전체를 위해</strong>: 계산 패턴의 신중한 분석이 놀라운 최적화를 가져올 수 있음을 보여줌—대형 모델 훈련 방식에는 여전히 상당한 개선 여지가 있음</p> <p>이 기법들은 이미 이전에는 불가능했던 훈련을 가능하게 하고 있으며, 수학적 프레임워크는 모델이 다조 매개변수 체제로 계속 확장됨에 따라 미래 최적화의 기반을 제공합니다.</p> <hr/>]]></content><author><name></name></author><category term="memory-efficiency"/><category term="sequence-parallelism"/><summary type="html"><![CDATA[Techniques for reducing activation recomputation overhead in large Transformer training via sequence and selective checkpointing.]]></summary></entry><entry><title type="html">Mixture of Experts</title><link href="https://sungyubkim.github.io/blog/moe/" rel="alternate" type="text/html" title="Mixture of Experts"/><published>2025-05-31T00:00:00+09:00</published><updated>2025-05-31T00:00:00+09:00</updated><id>https://sungyubkim.github.io/blog/moe</id><content type="html" xml:base="https://sungyubkim.github.io/blog/moe/"><![CDATA[<h1 id="tldr">TL;DR</h1> <blockquote> <p>이 2017년 기념비적 논문은 <strong>희소 게이팅 전문가 혼합(Sparsely-Gated Mixture-of-Experts, MoE)</strong>을 도입하여 신경망 확장의 근본적 과제를 해결했습니다. 훈련 가능한 게이팅 네트워크가 전문가 서브네트워크를 희소하게 활성화하는 방법으로 1000배 이상의 매개변수 증가를 최소한의 계산 오버헤드로 달성합니다. 정교한 부하 분산과 분산 훈련 솔루션이 결합되었습니다. 언어 모델링(24-39% 향상된 perplexity)과 기계 번역(+1-1.3 BLEU)에서 상당한 개선을 보여주지만, 효율성 주장은 과장되었고 상당한 인프라 복잡성을 수반합니다. 이 연구의 지속적인 영향은 조건부 계산이 대규모로 작동할 수 있음을 증명하여 오늘날의 최대 언어 모델을 구동하는 희소 신경 아키텍처의 현대적 시대를 열었다는 것입니다.</p> </blockquote> <ul> <li>Paper Link: <a href="https://arxiv.org/pdf/1701.06538">https://arxiv.org/pdf/1701.06538</a></li> </ul> <hr/> <h1 id="related-papers">Related Papers</h1> <p><strong>전문가 혼합 아키텍처:</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2101.03961">Switch Transformers</a> - 단순화된 MoE 라우팅으로 조 단위 매개변수 달성</li> <li><a href="/blog/pp/">GPipe</a> - MoE와 결합 가능한 파이프라인 병렬화</li> </ul> <p><strong>분산 훈련:</strong></p> <ul> <li><a href="/blog/tp/">Tensor Parallelism</a> - MoE와 호환되는 텐서 병렬화 기법</li> <li><a href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> - 대규모 MoE 훈련을 위한 메모리 최적화</li> </ul> <p><strong>확장성 및 효율성:</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2211.05102">Efficiently Scaling Transformer Inference</a> - MoE 모델의 효율적 추론</li> <li><a href="/blog/usp/">USP</a> - MoE와 결합 가능한 시퀀스 병렬화</li> </ul> <hr/> <h1 id="takeaways">Takeaways</h1> <h2 id="문제-신경망-확장이-벽에-부딪히다">문제: 신경망 확장이 벽에 부딪히다</h2> <p>신경망은 간단한 원리를 따릅니다: 더 많은 매개변수 = 더 나은 성능. 하지만 전통적인 확장은 이차적 비용 증가로 이어집니다. 모델 크기를 두 배로 늘리고 훈련 데이터를 두 배로 늘리면 계산 비용이 4배 증가합니다. 이는 빠르게 지속 불가능해집니다.</p> <p><strong>조건부 계산</strong>은 해결책을 약속했습니다 - 서로 다른 입력에 대해 네트워크의 다른 부분을 활성화하여 계산량은 일정하게 유지하면서 용량을 대폭 증가시키는 것입니다. 하지만 수십 년의 이론적 연구에도 불구하고, 근본적인 과제들로 인해 아무도 대규모로 작동시키지 못했습니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 핵심 확장 문제
</span><span class="n">전통적_비용</span> <span class="o">=</span> <span class="n">모델_크기</span> <span class="o">*</span> <span class="n">훈련_데이터</span> <span class="o">*</span> <span class="n">매개변수당_계산량</span>
<span class="c1"># 모델_크기와 훈련_데이터가 모두 증가하면 비용이 이차적으로 폭증
</span></code></pre></div></div> <p><strong>주요 장벽들:</strong></p> <ul> <li><strong>GPU 비효율성</strong>: GPU는 분기가 아닌 밀집 계산에 최적화됨</li> <li><strong>배치 크기 감소</strong>: 조건부 활성화가 각 구성 요소의 효과적 배치 크기를 줄임</li> <li><strong>네트워크 대역폭</strong>: 매개변수가 분산될 때 통신 비용이 지배적</li> <li><strong>부하 분산</strong>: 모델이 몇 개의 “선호하는” 구성 요소만 사용하여 용량을 낭비하는 경향</li> </ul> <h2 id="해결책-희소-게이팅-전문가-혼합">해결책: 희소 게이팅 전문가 혼합</h2> <p>저자들의 돌파구는 이 모든 과제를 동시에 해결하는 완전한 시스템을 만든 것이었습니다. 핵심 혁신은 신경망 어디에나 임베드될 수 있는 <strong>MoE 층</strong>입니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SparseMoELayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_experts</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="c1"># 게이팅 네트워크가 어떤 전문가를 사용할지 결정
</span>        <span class="n">self</span><span class="p">.</span><span class="n">gating_network</span> <span class="o">=</span> <span class="nc">GatingNetwork</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_experts</span><span class="p">)</span>
        
        <span class="c1"># 수천 개의 전문가 서브네트워크
</span>        <span class="n">self</span><span class="p">.</span><span class="n">experts</span> <span class="o">=</span> <span class="p">[</span><span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_experts</span><span class="p">)]</span>
        
        <span class="c1"># 입력당 상위 k개 전문가만 활성화
</span>        <span class="n">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># 1. 게이팅 네트워크가 입력당 k개 전문가 선택
</span>        <span class="n">gate_probs</span><span class="p">,</span> <span class="n">selected_experts</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gating_network</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">)</span>
        
        <span class="c1"># 2. 선택된 전문가만 계산 (핵심 효율성 이득)
</span>        <span class="n">expert_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_selected_experts</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">selected_experts</span><span class="p">)</span>
        
        <span class="c1"># 3. 전문가 출력의 가중 조합
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">combine_outputs</span><span class="p">(</span><span class="n">expert_outputs</span><span class="p">,</span> <span class="n">gate_probs</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>

<span class="c1"># 예시: 1000개 전문가와 k=4로 1000배 용량을 얻지만
# 입력당 계산량은 4배만 증가
</span></code></pre></div></div> <h2 id="성공을-가능하게-한-기술적-혁신들">성공을 가능하게 한 기술적 혁신들</h2> <h3 id="1-노이즈-top-k-게이팅-희소하지만-미분-가능">1. 노이즈 Top-K 게이팅: 희소하지만 미분 가능</h3> <p>게이팅 네트워크는 시스템의 핵심입니다. 훈련 가능성을 유지하면서 어떤 전문가를 사용할지 선택해야 합니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">noisy_top_k_gating</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w_gate</span><span class="p">,</span> <span class="n">w_noise</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="c1"># 기본 게이팅 점수
</span>    <span class="n">gate_logits</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_gate</span>  <span class="c1"># [batch, num_experts]
</span>    
    <span class="k">if</span> <span class="n">training</span><span class="p">:</span>
        <span class="c1"># 부하 분산을 위한 노이즈 추가 (중요한 혁신!)
</span>        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">gate_logits</span><span class="p">)</span> <span class="o">*</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">w_noise</span><span class="p">)</span>
        <span class="n">gate_logits</span> <span class="o">=</span> <span class="n">gate_logits</span> <span class="o">+</span> <span class="n">noise</span>
    
    <span class="c1"># 상위 k개 전문가 선택
</span>    <span class="n">top_k_logits</span><span class="p">,</span> <span class="n">top_k_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">topk</span><span class="p">(</span><span class="n">gate_logits</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># 희소 마스크 생성: 나머지를 음의 무한대로 설정
</span>    <span class="n">sparse_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">full_like</span><span class="p">(</span><span class="n">gate_logits</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>
    <span class="n">sparse_logits</span><span class="p">.</span><span class="nf">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">top_k_indices</span><span class="p">,</span> <span class="n">top_k_logits</span><span class="p">)</span>
    
    <span class="c1"># 소프트맥스가 최종 전문가 가중치 제공
</span>    <span class="n">gate_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">sparse_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">gate_probs</span><span class="p">,</span> <span class="n">top_k_indices</span>

<span class="c1"># 핵심 통찰: 노이즈는 단순한 탐색이 아니라 
# 부하 분산을 위한 이산적 전문가 선택을 미분 가능하게 만듦
</span></code></pre></div></div> <h3 id="2-하이브리드-분산-훈련-배치-크기-문제-해결">2. 하이브리드 분산 훈련: 배치 크기 문제 해결</h3> <p>전통적인 데이터 병렬처리는 각 전문가에게 작은 배치를 줍니다. 해결책은 데이터와 모델 병렬처리를 결합하는 것입니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 대신에: 각 디바이스가 batch_size/num_devices 예제를 받음
# 이렇게 하기: 전문가 계산을 위해 디바이스 간 배치 결합
</span>
<span class="k">def</span> <span class="nf">distributed_moe_forward</span><span class="p">(</span><span class="n">inputs_per_device</span><span class="p">,</span> <span class="n">expert_assignments</span><span class="p">):</span>
    <span class="c1"># 1. 각 디바이스가 독립적으로 게이팅 계산 (데이터 병렬)
</span>    <span class="n">local_gates</span> <span class="o">=</span> <span class="p">[</span><span class="nf">compute_gates</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">inputs_per_device</span><span class="p">]</span>
    
    <span class="c1"># 2. 각 전문가가 필요한 모든 입력 수집 (모델 병렬)
</span>    <span class="n">expert_batches</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">device_id</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">gates</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">inputs_per_device</span><span class="p">,</span> <span class="n">local_gates</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">expert_id</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_experts</span><span class="p">):</span>
            <span class="n">selected_inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">gates</span><span class="p">.</span><span class="n">selected_experts</span> <span class="o">==</span> <span class="n">expert_id</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">expert_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">expert_batches</span><span class="p">:</span>
                <span class="n">expert_batches</span><span class="p">[</span><span class="n">expert_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">expert_batches</span><span class="p">[</span><span class="n">expert_id</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">selected_inputs</span><span class="p">)</span>
    
    <span class="c1"># 3. 각 전문가가 모든 디바이스의 결합된 배치 처리
</span>    <span class="n">expert_outputs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">expert_id</span><span class="p">,</span> <span class="n">batched_inputs</span> <span class="ow">in</span> <span class="n">expert_batches</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">combined_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">batched_inputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">expert_outputs</span><span class="p">[</span><span class="n">expert_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">experts</span><span class="p">[</span><span class="n">expert_id</span><span class="p">](</span><span class="n">combined_batch</span><span class="p">)</span>
    
    <span class="c1"># 4. 결과를 디바이스로 다시 분산
</span>    <span class="k">return</span> <span class="nf">scatter_outputs_to_devices</span><span class="p">(</span><span class="n">expert_outputs</span><span class="p">)</span>

<span class="c1"># 결과: 전문가 배치 크기 = k * 총_배치_크기 / 전문가_수
# d개 디바이스로: k * d * 로컬_배치_크기 / 전문가_수
</span></code></pre></div></div> <h3 id="3-부하-분산-전문가-붕괴-방지">3. 부하 분산: 전문가 붕괴 방지</h3> <p>개입 없이는 모델이 몇 개의 전문가만 사용하도록 학습하여 용량을 낭비합니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_load_balancing_losses</span><span class="p">(</span><span class="n">gate_probs</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># 손실 1: 동등한 중요도 장려 (게이트 가중치 합계)
</span>    <span class="n">importance</span> <span class="o">=</span> <span class="n">gate_probs</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 배치에 대한 합계
</span>    <span class="n">importance_loss</span> <span class="o">=</span> <span class="nf">coefficient_of_variation</span><span class="p">(</span><span class="n">importance</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    
    <span class="c1"># 손실 2: 동등한 부하 장려 (전문가당 예제 수)
</span>    <span class="c1"># 전문가 선택이 이산적이므로 더 복잡함
</span>    <span class="n">load_estimator</span> <span class="o">=</span> <span class="nf">estimate_load_differentiably</span><span class="p">(</span><span class="n">gate_probs</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">load_loss</span> <span class="o">=</span> <span class="nf">coefficient_of_variation</span><span class="p">(</span><span class="n">load_estimator</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    
    <span class="k">return</span> <span class="n">importance_loss</span><span class="p">,</span> <span class="n">load_loss</span>

<span class="k">def</span> <span class="nf">estimate_load_differentiably</span><span class="p">(</span><span class="n">gate_probs</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># 이산적 전문가 할당의 부드러운 추정기
</span>    <span class="c1"># 게이팅의 노이즈를 사용하여 이를 미분 가능하게 만듦
</span>    <span class="n">load_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">gate_probs</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">expert_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_experts</span><span class="p">):</span>
        <span class="c1"># 각 예제에 대해 이 전문가가 선택될 확률
</span>        <span class="c1"># 현재 게이트 값과 노이즈 분포에 기반
</span>        <span class="n">other_experts</span> <span class="o">=</span> <span class="n">gate_probs</span><span class="p">[:,</span> <span class="n">others</span><span class="p">]</span>
        <span class="n">kth_largest_other</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">kthvalue</span><span class="p">(</span><span class="n">other_experts</span><span class="p">,</span> <span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># k번째로 큰 경쟁자를 이길 확률
</span>        <span class="n">prob_selected</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span>
            <span class="p">(</span><span class="n">gate_probs</span><span class="p">[:,</span> <span class="n">expert_idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">kth_largest_other</span><span class="p">)</span> <span class="o">/</span> <span class="n">noise_std</span>
        <span class="p">)</span>
        <span class="n">load_probs</span><span class="p">[:,</span> <span class="n">expert_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">prob_selected</span>
    
    <span class="k">return</span> <span class="n">load_probs</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 전문가당 예상 부하
</span></code></pre></div></div> <h2 id="실험-결과-인상적이지만-복잡함">실험 결과: 인상적이지만 복잡함</h2> <h3 id="주요-성능-결과">주요 성능 결과</h3> <table> <thead> <tr> <th>작업</th> <th>기준 모델</th> <th>MoE 모델</th> <th>개선</th> <th>숨겨진 비용</th> </tr> </thead> <tbody> <tr> <td><strong>1B 단어 LM</strong></td> <td>45.0 perplexity (9.4M 매개변수)</td> <td><strong>34.1 perplexity</strong> (4.3B 매개변수)</td> <td><strong>24% 향상</strong></td> <td>430배 더 많은 매개변수</td> </tr> <tr> <td><strong>100B 단어 LM</strong></td> <td>47.0 perplexity (8.4M 매개변수)</td> <td><strong>28.9 perplexity</strong> (68.8B 매개변수)</td> <td><strong>39% 향상</strong></td> <td>8200배 더 많은 매개변수</td> </tr> <tr> <td><strong>WMT’14 En→Fr</strong></td> <td>39.22 BLEU (278M 매개변수)</td> <td><strong>40.56 BLEU</strong> (8.7B 매개변수)</td> <td><strong>+1.34 BLEU</strong></td> <td>31배 더 많은 매개변수</td> </tr> <tr> <td><strong>WMT’14 En→De</strong></td> <td>24.91 BLEU (278M 매개변수)</td> <td><strong>26.03 BLEU</strong> (8.7B 매개변수)</td> <td><strong>+1.12 BLEU</strong></td> <td>31배 더 많은 매개변수</td> </tr> </tbody> </table> <p><strong>핵심 통찰</strong>: 개선은 실제적이고 의미 있지만, 대규모 매개변수 증가와 함께 옵니다. “동일한 계산, 더 나은 결과”가 아니라 “대폭 늘어난 매개변수, 더 나은 결과”입니다.</p> <h3 id="부하-분산-절제-연구">부하 분산 절제 연구</h3> <table> <thead> <tr> <th>구성</th> <th>테스트 Perplexity</th> <th>전문가 균형</th> <th>부하 균형</th> <th>해석</th> </tr> </thead> <tbody> <tr> <td><strong>부하 손실 없음</strong></td> <td>39.8</td> <td>매우 나쁨 (CV=3.04)</td> <td>매우 나쁨 (17.8배 불균형)</td> <td><strong>완전한 실패</strong> - 모델이 ~3개 전문가 사용</td> </tr> <tr> <td><strong>중요도 손실만</strong></td> <td>35.6</td> <td>좋음 (CV=0.06)</td> <td>나쁨 (1.47배 불균형)</td> <td>게이트 가중치는 균형 맞지만 실제 부하는 안 맞음</td> </tr> <tr> <td><strong>부하 손실만</strong></td> <td>35.7</td> <td>보통 (CV=0.22)</td> <td>좋음 (1.15배 불균형)</td> <td>분산 효율성에 더 좋음</td> </tr> <tr> <td><strong>두 손실 모두</strong></td> <td>35.6</td> <td>좋음 (CV=0.06)</td> <td>좋음 (1.14배 불균형)</td> <td><strong>전체적으로 최고</strong> - 두 측면 모두 균형</td> </tr> </tbody> </table> <p><strong>중요한 발견</strong>: 두 손실 함수 모두 필수적입니다. 이것들 없이는 모델이 완전히 실패합니다 (39.8 vs 35.6 perplexity).</p> <h3 id="계산-효율성-현실-점검">계산 효율성 현실 점검</h3> <table> <thead> <tr> <th>모델 유형</th> <th>주장된 이득</th> <th>측정된 TFLOPS/GPU</th> <th>현실</th> </tr> </thead> <tbody> <tr> <td><strong>밀집 기준선</strong></td> <td>-</td> <td>1.07-1.29</td> <td>깔끔한 기준선</td> </tr> <tr> <td><strong>희소 MoE (낮은 계산)</strong></td> <td>“동일한 계산”</td> <td><strong>0.74-0.90</strong></td> <td><strong>실제로는 덜 효율적!</strong></td> </tr> <tr> <td><strong>희소 MoE (높은 계산)</strong></td> <td>“더 나은 효율성”</td> <td>1.56</td> <td>더 나음, 하지만 더 큰 행렬이 도움</td> </tr> </tbody> </table> <p><strong>현실 점검</strong>: 효율성 주장은 의문스럽습니다. 낮은 계산 MoE 모델들은 실제로 밀집 기준선보다 덜 효율적이며, 아마도 분기 오버헤드와 통신 비용 때문일 것입니다.</p> <h2 id="비판적-평가-주의사항이-있는-돌파구">비판적 평가: 주의사항이 있는 돌파구</h2> <h3 id="진정한-돌파구들">진정한 돌파구들</h3> <ol> <li><strong>조건부 계산이 대규모로 작동함을 증명</strong> - 대규모 희소 네트워크의 첫 번째 성공적 실증</li> <li><strong>실용적 과제를 위한 엔지니어링 솔루션</strong> - 부하 분산, 분산 훈련, GPU 효율성</li> <li><strong>여러 작업에서 일관된 개선</strong> - 언어 모델링, 기계 번역, 다국어 설정</li> <li><strong>전문가 특수화가 자연스럽게 나타남</strong> - 모델이 구문적, 의미적 특수화를 학습</li> </ol> <h3 id="상당한-한계들">상당한 한계들</h3> <ol> <li><strong>불공정한 매개변수 비교</strong> - 430배-8200배 매개변수 증가가 “효율성” 주장을 오해의 소지가 있게 만듦</li> <li><strong>인프라 복잡성이 숨겨짐</strong> - 메모리 요구사항, 분산 설정 비용이 충분히 고려되지 않음</li> <li><strong>하드웨어 비교 문제</strong> - 다른 GPU 세대들이 훈련 시간 비교를 무효화함</li> <li><strong>통계적 엄밀성 부족</strong> - 오차 막대, 유의성 검정, 또는 여러 무작위 시드 없음</li> </ol> <h3 id="결과가-실제로-보여주는-것">결과가 실제로 보여주는 것</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 그들이 주장하는 것:
</span><span class="sh">"</span><span class="s">최소한의 계산 오버헤드로 1000배 모델 용량</span><span class="sh">"</span>

<span class="c1"># 그들이 실제로 실증하는 것:
</span><span class="sh">"</span><span class="s">1000배 매개변수가 의미 있는 개선을 줄 수 있지만, 다음과 함께:
 - 상당한 메모리/저장 오버헤드
 - 복잡한 분산 훈련 요구사항  
 - 의문스러운 계산 효율성 이득
 - 신중한 부하 분산의 필요성</span><span class="sh">"</span>
</code></pre></div></div> <h2 id="실용적-시사점과-교훈">실용적 시사점과 교훈</h2> <h3 id="연구자들을-위해">연구자들을 위해</h3> <ol> <li><strong>희소 모델이 작동할 수 있음</strong> - 하지만 정교한 엔지니어링 솔루션이 필요</li> <li><strong>부하 분산이 중요함</strong> - 개입 없이는 모델이 몇 개 전문가 사용으로 붕괴</li> <li><strong>평가에 주의가 필요함</strong> - 매개변수 증가가 공정한 비교를 어렵게 만듦</li> <li><strong>인프라가 중요함</strong> - 분산 훈련 복잡성이 상당함</li> </ol> <h3 id="실무자들을-위해">실무자들을 위해</h3> <ol> <li><strong>제약 조건 고려</strong>: <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">메모리_예산_크고</span> <span class="ow">and</span> <span class="n">분산_훈련_가능</span><span class="p">:</span>
    <span class="n">sparse_moe_고려</span><span class="p">()</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">밀집_모델이_더_나을_수도</span><span class="p">()</span>
</code></pre></div> </div> </li> <li><strong>배치 크기 요구사항</strong>: <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">최소_배치_크기</span> <span class="o">=</span> <span class="n">전문가_수</span> <span class="o">*</span> <span class="n">k</span> <span class="o">/</span> <span class="n">원하는_전문가당_예제수</span>
<span class="c1"># 효율성을 위해 큰 배치 필요 - 일반적으로 512+ 예제
</span></code></pre></div> </div> </li> <li><strong>고려해야 할 숨겨진 비용들</strong>: <ul> <li>메모리: 대형 희소 모델의 경우 10-100배 증가</li> <li>저장: 모델 파일이 거대해짐</li> <li>통신: 전문가 라우팅이 상당한 대역폭 필요</li> <li>복잡성: 부하 분산, 분산 훈련 설정</li> </ul> </li> </ol> <h2 id="장기적-영향과-미래-방향">장기적 영향과 미래 방향</h2> <p>이 논문은 희소 신경망의 현대적 시대를 열었습니다. 그 영향은 다음에서 볼 수 있습니다:</p> <ul> <li><strong>Switch Transformer</strong> (Google, 2021) - 단순화된 MoE 설계</li> <li><strong>GLaM</strong> (Google, 2021) - 64개 전문가 언어 모델</li> <li><strong>PaLM</strong> (Google, 2022) - 밀집 모델에 적용된 확장 통찰</li> <li><strong>GPT-4</strong> (OpenAI, 2023) - 아마도 MoE 기법 사용 (미확인)</li> </ul> <h3 id="분야를-위한-핵심-교훈">분야를 위한 핵심 교훈</h3> <ol> <li><strong>조건부 계산이 실행 가능함</strong> - 하지만 신중한 엔지니어링 필요</li> <li><strong>매개변수 효율성 vs 계산 효율성</strong> - 이들은 다른 최적화 목표</li> <li><strong>확장 법칙이 중요함</strong> - 더 큰 데이터셋이 훨씬 더 큰 모델을 효과적으로 활용할 수 있음</li> <li><strong>인프라 공동 설계</strong> - 알고리즘과 시스템 엔지니어링이 함께 발전해야 함</li> </ol> <h3 id="열린-질문들과-미래-연구">열린 질문들과 미래 연구</h3> <ul> <li><strong>더 나은 부하 분산 알고리즘</strong> - 현재 방법들은 여전히 차선책</li> <li><strong>희소 어텐션 메커니즘</strong> - 어텐션에 유사한 원리 적용</li> <li><strong>자동화된 전문가 특수화</strong> - 언제 어떻게 특수화할지 학습</li> <li><strong>희소 모델의 효율적 추론</strong> - 현재 방법들은 훈련에 최적화됨</li> </ul> <h2 id="결론">결론</h2> <p>이 논문은 조건부 계산이 대규모로 작동할 수 있음을 증명한 진정한 돌파구를 나타냅니다. 효율성 주장이 과장되고 실험적 비교에 상당한 한계가 있지만, 핵심 기여 - 희소 MoE가 의미 있는 개선을 달성할 수 있음을 실증 - 는 전체 연구 방향을 출범시켰습니다.</p> <p>이 연구의 지속적인 가치는 특정 효율성 수치(의문스러운)에 있지 않고 조건부 계산을 실용적으로 만드는 엔지니어링 청사진에 있습니다. 2017년 이후의 모든 주요 희소 신경망은 여기서 놓인 기초 위에 구축됩니다: 노이즈 게이팅, 부하 분산 손실, 하이브리드 분산 훈련.</p> <p>오늘날 대규모 신경망을 다루는 누구에게나 이러한 기법들을 이해하는 것은 필수적입니다 - 반드시 MoE를 사용해야 하기 때문이 아니라, 희소성, 부하 분산, 확장의 원리들이 현대 딥러닝의 기본이 되었기 때문입니다.</p> <hr/>]]></content><author><name></name></author><category term="mixture-of-experts"/><summary type="html"><![CDATA[Sparsely-Gated Mixture-of-Experts (MoE) enables 1000x parameter scaling with minimal compute overhead.]]></summary></entry><entry><title type="html">Ring Self-Attention</title><link href="https://sungyubkim.github.io/blog/ring-self-attention/" rel="alternate" type="text/html" title="Ring Self-Attention"/><published>2025-05-30T00:00:00+09:00</published><updated>2025-05-30T00:00:00+09:00</updated><id>https://sungyubkim.github.io/blog/ring-self-attention</id><content type="html" xml:base="https://sungyubkim.github.io/blog/ring-self-attention/"><![CDATA[<h1 id="tldr">TL;DR</h1> <blockquote> <p>이 논문은 <strong>시퀀스 병렬화(sequence parallelism)</strong>를 소개합니다 - 전체 시퀀스를 하나의 GPU에 저장해야 하는 기존 방식 대신, 시퀀스를 여러 GPU에 분할하여 더 긴 시퀀스로 Transformer 모델을 훈련하는 새로운 방법입니다. 핵심 혁신은 영리한 링 통신 패턴을 통해 분산 어텐션 계산을 가능하게 하는 <strong>링 셀프 어텐션(Ring Self-Attention, RSA)</strong>입니다.</p> <p><strong>주요 기여:</strong></p> <ul> <li>새로운 병렬화 차원: 시퀀스 길이 (기존 데이터/모델 병렬화와 대비)</li> <li>분산 어텐션 계산을 위한 링 셀프 어텐션 알고리즘</li> <li>텐서 병렬화 대비 3배 긴 시퀀스와 13.7배 큰 배치 크기</li> </ul> </blockquote> <ul> <li>Paper Link: <a href="https://arxiv.org/pdf/2105.13120">https://arxiv.org/pdf/2105.13120</a></li> </ul> <hr/> <h1 id="related-papers">Related Papers</h1> <p><strong>시퀀스 병렬화 발전:</strong></p> <ul> <li><a href="/blog/blockwise_ringattention/">Blockwise RingAttention</a> - 링 기반 시퀀스 병렬화의 현대적 발전</li> <li><a href="https://arxiv.org/pdf/2309.14509">DeepSpeed Ulysses</a> - 시퀀스 병렬화의 실용적 구현</li> <li><a href="/blog/usp/">USP</a> - 통합 시퀀스 병렬화 프레임워크</li> </ul> <p><strong>분산 어텐션:</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2310.03294">DISTFLASHATTN</a> - 분산 FlashAttention 구현</li> <li><a href="https://arxiv.org/pdf/2311.09431">Striped Attention</a> - 효율적인 시퀀스 분배 패턴</li> <li><a href="https://arxiv.org/pdf/2406.18485">LoongTrain</a> - 2D 어텐션 병렬화</li> </ul> <p><strong>병렬화 통합:</strong></p> <ul> <li><a href="/blog/tp/">Tensor Parallelism</a> - 텐서 병렬화와의 결합</li> <li><a href="/blog/pp/">GPipe</a> - 파이프라인 병렬화와의 통합</li> <li><a href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> - 메모리 효율적인 병렬 훈련</li> </ul> <p><strong>긴 컨텍스트 처리:</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2411.01783">Context Parallelism for Scalable Million-Token Inference</a> - 추론 시 시퀀스 병렬화</li> </ul> <hr/> <ul> <li>기존 병렬화 전략과 호환 가능하여 “4차원 병렬화” 구현</li> </ul> <p><strong>비판적 현실 점검:</strong> 실험 검증에 상당한 약점이 있습니다 - 불공정한 기준선, 제한된 범위, 과장된 개선 효과. 이 방법은 일반적인 돌파구라기보다는 특정 영역(많은 GPU에서 중간-긴 시퀀스)을 위한 것입니다.</p> <h1 id="takeaways">Takeaways</h1> <h2 id="문제와-동기">문제와 동기</h2> <h3 id="왜-중요한가">왜 중요한가</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 기존 제약: 전체 시퀀스가 하나의 GPU에 맞아야 함
</span><span class="n">traditional_limit</span> <span class="o">=</span> <span class="n">gpu_memory</span> <span class="o">/</span> <span class="p">(</span><span class="n">sequence_length</span><span class="o">^</span><span class="mi">2</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">)</span>

<span class="c1"># 의료 영상 예시
</span><span class="n">medical_image</span> <span class="o">=</span> <span class="mi">512</span> <span class="o">*</span> <span class="mi">512</span> <span class="o">*</span> <span class="mi">512</span>  <span class="c1"># 3D 의료 스캔
</span><span class="n">tokens_per_image</span> <span class="o">=</span> <span class="mi">134</span><span class="n">_million</span>    <span class="c1"># 각 복셀이 토큰이 됨
</span><span class="n">memory_required</span> <span class="o">=</span> <span class="sh">"</span><span class="s">단일 GPU로는 불가능</span><span class="sh">"</span>

<span class="c1"># 이것이 시퀀스 병렬화가 해결하는 근본적인 문제
</span></code></pre></div></div> <p><strong>핵심 통찰</strong>: 기존 연구가 알고리즘적 개선(스파스 어텐션, 선형 어텐션)에 집중하는 반면, 이 논문은 <strong>시스템 접근법</strong>을 취합니다 - 어텐션을 더 효율적으로 만드는 대신 더 많은 GPU를 사용하여 긴 시퀀스를 처리합니다.</p> <h3 id="현재-병렬화-환경">현재 병렬화 환경</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">parallelism_strategies</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">data_parallelism</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">배치를 GPU에 분할</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">pipeline_parallelism</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">모델 레이어를 GPU에 분할</span><span class="sh">'</span><span class="p">,</span> 
    <span class="sh">'</span><span class="s">tensor_parallelism</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">모델 매개변수를 GPU에 분할</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">sequence_parallelism</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">시퀀스 길이를 GPU에 분할 (새로운 방식)</span><span class="sh">'</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="기술적-혁신-링-셀프-어텐션">기술적 혁신: 링 셀프 어텐션</h2> <h3 id="핵심-알고리즘-설명">핵심 알고리즘 설명</h3> <p><strong>1단계: 어텐션 스코어 QK^T 계산</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ring_attention_scores_example</span><span class="p">():</span>
    <span class="c1"># 설정: 4개 GPU, 시퀀스 길이 8, 각 GPU가 2개 토큰 처리
</span>    <span class="n">devices</span> <span class="o">=</span> <span class="p">{</span>
        <span class="mi">0</span><span class="p">:</span> <span class="sh">"</span><span class="s">토큰 [0,1]</span><span class="sh">"</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">:</span> <span class="sh">"</span><span class="s">토큰 [2,3]</span><span class="sh">"</span><span class="p">,</span> 
        <span class="mi">2</span><span class="p">:</span> <span class="sh">"</span><span class="s">토큰 [4,5]</span><span class="sh">"</span><span class="p">,</span>
        <span class="mi">3</span><span class="p">:</span> <span class="sh">"</span><span class="s">토큰 [6,7]</span><span class="sh">"</span>
    <span class="p">}</span>
    
    <span class="c1"># Device 1의 관점에서 살펴보기
</span>    <span class="n">device_1_queries</span> <span class="o">=</span> <span class="sh">"</span><span class="s">토큰 [2,3]에 대한 Q</span><span class="sh">"</span>
    
    <span class="c1"># Device 1은 모든 키와 어텐션을 계산해야 함
</span>    <span class="n">ring_steps</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">단계 0: 로컬 K[2,3] 사용 -&gt; Q[2,3] @ K[2,3]^T 계산</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">단계 1: Device 0에서 K[0,1] 받음 -&gt; Q[2,3] @ K[0,1]^T</span><span class="sh">"</span><span class="p">,</span> 
        <span class="sh">"</span><span class="s">단계 2: Device 3에서 K[6,7] 받음 -&gt; Q[2,3] @ K[6,7]^T</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">단계 3: Device 2에서 K[4,5] 받음 -&gt; Q[2,3] @ K[4,5]^T</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="c1"># 결과: Device 1은 토큰 [2,3]이 모든 토큰 [0-7]에 어텐션하는 스코어를 가짐
</span>    <span class="n">attention_matrix_shape</span> <span class="o">=</span> <span class="sh">"</span><span class="s">[배치, 2_로컬_토큰, 8_전체_토큰]</span><span class="sh">"</span>
    
    <span class="k">return</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Device 1이 완전한 어텐션 스코어 계산: </span><span class="si">{</span><span class="n">attention_matrix_shape</span><span class="si">}</span><span class="sh">"</span>
</code></pre></div></div> <p><strong>2단계: 값을 사용한 최종 출력 계산</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ring_attention_output_example</span><span class="p">():</span>
    <span class="c1"># Device 1은 어텐션 가중치를 가짐: [배치, 2, 8] 
</span>    <span class="c1"># 이제 모든 값 임베딩과 곱해야 함
</span>    
    <span class="n">ring_steps</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">단계 0: 로컬 V[2,3] 사용 -&gt; weights[:,:,2:4] @ V[2,3]</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">단계 1: V[0,1] 받음 -&gt; weights[:,:,0:2] @ V[0,1]</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">단계 2: V[6,7] 받음 -&gt; weights[:,:,6:8] @ V[6,7]</span><span class="sh">"</span><span class="p">,</span> 
        <span class="sh">"</span><span class="s">단계 3: V[4,5] 받음 -&gt; weights[:,:,4:6] @ V[4,5]</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="n">final_output</span> <span class="o">=</span> <span class="sh">"</span><span class="s">모든 부분 출력의 합 = 토큰 [2,3]에 대한 완전한 어텐션</span><span class="sh">"</span>
    <span class="k">return</span> <span class="n">final_output</span>
</code></pre></div></div> <h3 id="중요한-구현-요구사항">중요한 구현 요구사항</h3> <p><strong>메모리 관리</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">memory_requirements</span><span class="p">():</span>
    <span class="c1"># 중요: 어텐션 행렬은 여전히 전체 시퀀스 차원이 필요
</span>    <span class="n">peak_memory_components</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">attention_scores</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">batch_size × local_seq_len × 전체_seq_len</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">attention_weights</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">batch_size × local_seq_len × 전체_seq_len</span><span class="sh">'</span><span class="p">,</span> 
        <span class="sh">'</span><span class="s">temp_embeddings</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">2 × batch_size × local_seq_len × hidden_size</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">gradients</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">역전파 중 추가 2배</span><span class="sh">'</span>
    <span class="p">}</span>
    
    <span class="c1"># 경고: 매우 긴 시퀀스의 경우 어텐션 행렬이 여전히 OOM 유발 가능
</span>    <span class="n">memory_scaling</span> <span class="o">=</span> <span class="sh">"</span><span class="s">O(sequence_length^2) 대신 O(sequence_length), 하지만 주의사항 있음</span><span class="sh">"</span>
    <span class="k">return</span> <span class="n">memory_scaling</span>
</code></pre></div></div> <p><strong>동기화 요구사항</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">synchronization_constraints</span><span class="p">():</span>
    <span class="n">assumptions</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">모든 장치가 링 통신에 참여해야 함</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">링 통신은 동기식이어야 함</span><span class="sh">"</span><span class="p">,</span> 
        <span class="sh">"</span><span class="s">어떤 장치 실패든 전체 링을 중단시킴</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">시퀀스 길이가 장치 수로 나누어떨어져야 함</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">모든 장치가 동일한 로컬 계산 시간을 가져야 함</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="n">failure_modes</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">장치 실패 -&gt; 전체 훈련 중단</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">네트워크 분할 -&gt; 링 통신 실패</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">로드 불균형 -&gt; 가장 느린 장치가 속도 결정</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">메모리 파편화 -&gt; 일관성 없는 성능</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="k">return</span> <span class="n">assumptions</span><span class="p">,</span> <span class="n">failure_modes</span>
</code></pre></div></div> <h2 id="실험-결과-분석">실험 결과 분석</h2> <h3 id="주요-결과와-비판적-해석">주요 결과와 비판적 해석</h3> <table> <thead> <tr> <th>실험</th> <th>기준선</th> <th>시퀀스 병렬화</th> <th>주장된 개선</th> <th><strong>현실 점검</strong></th> </tr> </thead> <tbody> <tr> <td><strong>최대 배치 크기</strong></td> <td>116 (12 GPUs)</td> <td>1,590 (64 GPUs)</td> <td>13.7×</td> <td><strong>불공정</strong>: 5배 많은 GPU 사용. 실제 알고리즘 개선은 ~2-3×</td> </tr> <tr> <td><strong>최대 시퀀스 길이</strong></td> <td>750</td> <td>2,250</td> <td>3.0×</td> <td><strong>보통</strong>: 합리적 개선이지만 혁명적이지 않음</td> </tr> <tr> <td><strong>처리량</strong></td> <td>18K tokens/sec</td> <td>24K tokens/sec</td> <td>1.3×</td> <td><strong>미미함</strong>: 통신 오버헤드를 겨우 보상</td> </tr> <tr> <td><strong>스파스 어텐션</strong></td> <td>4.2K tokens</td> <td>114K tokens</td> <td>27×</td> <td><strong>오해의 소지</strong>: 분산 vs 단일장치 비교, 알고리즘 비교 아님</td> </tr> </tbody> </table> <h3 id="부족한-절제-연구">부족한 절제 연구</h3> <table> <thead> <tr> <th>구성요소</th> <th>테스트된 것</th> <th><strong>누락된 것</strong></th> </tr> </thead> <tbody> <tr> <td>링 통신</td> <td>기본 기능</td> <td>all-gather, all-reduce 대안과 <strong>비교 없음</strong></td> </tr> <tr> <td>2단계 설계</td> <td>정확성 검증</td> <td>단일 단계 또는 다른 분해의 <strong>분석 없음</strong></td> </tr> <tr> <td>통신 패턴</td> <td>링 토폴로지</td> <td>트리, 버터플라이, 메시 패턴 <strong>평가 없음</strong></td> </tr> <tr> <td>메모리 vs 통신</td> <td>정성적 논의</td> <td><strong>체계적</strong> 트레이드오프 분석 없음</td> </tr> </tbody> </table> <h2 id="비판적-평가">비판적 평가</h2> <h3 id="강점">강점</h3> <ol> <li><strong>새로운 문제 프레이밍</strong>: 시퀀스 길이를 병렬화 차원으로 체계적으로 다룬 첫 연구</li> <li><strong>견고한 기술적 혁신</strong>: 링 셀프 어텐션이 수학적으로 정확하고 우아함</li> <li><strong>구현 가능성</strong>: 순수 PyTorch, 특별한 컴파일러 불필요</li> <li><strong>호환성</strong>: 기존 병렬화 전략과 함께 작동</li> </ol> <h3 id="주요-한계점">주요 한계점</h3> <p><strong>실험 검증 문제</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">experimental_problems</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">baseline_bias</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">시퀀스 스케일링에 텐서 병렬화와 비교</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">missing_baselines</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">그래디언트 체크포인팅, 혼합 정밀도와 비교 없음</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">hardware_dated</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">2016년 P100 GPU에서만 테스트</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">limited_scope</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Wikipedia의 BERT만, 다양한 워크로드 없음</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">statistical_rigor</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">오차 막대, 분산 분석, 유의성 테스트 없음</span><span class="sh">'</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>실제 배포 과제</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">deployment_reality</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">hardware_requirements</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">고대역폭 인터커넥트(InfiniBand) 필수</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">optimal_scale</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">16-32 GPU (이후 통신 오버헤드)</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">sweet_spot</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">중간 시퀀스 (1K-4K 토큰), 짧거나 매우 긴 것 아님</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">fault_tolerance</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">단일 장치 실패가 전체 훈련 중단</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">complexity</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">대안 대비 상당한 구현 복잡성</span><span class="sh">'</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="실용적-함의">실용적 함의</h2> <h3 id="시퀀스-병렬화를-언제-사용할지">시퀀스 병렬화를 언제 사용할지</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">should_use_sequence_parallelism</span><span class="p">(</span><span class="n">use_case</span><span class="p">):</span>
    <span class="n">good_fit</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">4K+ 토큰 시퀀스를 가진 의료 영상</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">긴 문서 처리 (법률, 과학 논문)</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">유전체 시퀀스 분석</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">비전 트랜스포머를 사용한 고해상도 이미지 분석</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="n">poor_fit</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">표준 NLP 작업 (≤512 토큰)</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">일반적인 이미지 크기의 컴퓨터 비전</span><span class="sh">"</span><span class="p">,</span> 
        <span class="sh">"</span><span class="s">자원 제약 환경</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">높은 신뢰성이 필요한 프로덕션 시스템</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="n">considerations</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">minimum_gpus</span><span class="sh">'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>  <span class="c1"># 이하에서는 오버헤드가 지배적
</span>        <span class="sh">'</span><span class="s">network_requirements</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">고대역폭, 저지연 인터커넥트</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">sequence_length</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">최적 지점: 1K-8K 토큰</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">batch_size</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">큰 배치가 이익 증폭</span><span class="sh">'</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">good_fit</span><span class="p">,</span> <span class="n">poor_fit</span><span class="p">,</span> <span class="n">considerations</span>
</code></pre></div></div> <h3 id="구현-가이드">구현 가이드</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SequenceParallelismChecklist</span><span class="p">:</span>
    <span class="n">prerequisites</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">고대역폭 네트워크 (InfiniBand 권장)</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">동일한 하드웨어 (같은 GPU)</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">GPU 수로 나누어떨어지는 시퀀스 길이</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">견고한 분산 훈련 인프라</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="n">optimization_tips</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">링 통신 버퍼 미리 할당</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">가능한 곳에 비동기 통신 사용</span><span class="sh">"</span><span class="p">,</span> 
        <span class="sh">"</span><span class="s">메모리용 그래디언트 체크포인팅 구현</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">장치 간 로드 불균형 모니터링</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">장치 실패에 대한 대안 전략 보유</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="n">performance_tuning</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">batch_size</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">메모리 한계까지 증가</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">sequence_chunks</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">계산 vs 통신 균형</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">ring_ordering</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">네트워크 토폴로지에 최적화</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">memory_management</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">버퍼 풀로 파편화 방지</span><span class="sh">'</span>
    <span class="p">}</span>
</code></pre></div></div> <h2 id="향후-연구-방향">향후 연구 방향</h2> <h3 id="즉각적인-개선-필요사항">즉각적인 개선 필요사항</h3> <ol> <li><strong>엄격한 실험 검증</strong>: 공정한 기준선, 최신 하드웨어, 다양한 워크로드</li> <li><strong>장애 내성</strong>: 장치 실패를 우아하게 처리</li> <li><strong>동적 로드 밸런싱</strong>: 이질적 하드웨어에 적응</li> <li><strong>통신 최적화</strong>: 단순한 링 패턴을 넘어서</li> </ol> <h3 id="장기-연구-질문">장기 연구 질문</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">future_directions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">algorithmic_fusion</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">스파스 어텐션, 선형 어텐션과 결합</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">adaptive_partitioning</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">어텐션 패턴 기반 동적 시퀀스 분할</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">cross_modal_scaling</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">멀티모달 트랜스포머로 확장</span><span class="sh">'</span><span class="p">,</span> 
    <span class="sh">'</span><span class="s">efficiency_optimization</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">통신 오버헤드 감소</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">theoretical_analysis</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">형식적 복잡도 분석과 최적성 경계</span><span class="sh">'</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="최신-기법과의-통합">최신 기법과의 통합</h3> <p>이 논문은 많은 중요한 발전 이전에 나왔습니다:</p> <ul> <li><strong>Zero Redundancy Optimizer (ZeRO)</strong>: 메모리 최적화를 위해 결합 가능</li> <li><strong>그래디언트 체크포인팅</strong>: 실용적 배포에 필수</li> <li><strong>혼합 정밀도 훈련</strong>: 현재 표준 관행</li> <li><strong>효율적 어텐션 변형</strong>: FlashAttention, LinearAttention 등</li> </ul> <h2 id="결론">결론</h2> <p><strong>시퀀스 병렬화는 병렬화 환경에서 특정 틈새를 채우는 견고한 기술적 기여를 나타냅니다.</strong> 링 셀프 어텐션 알고리즘은 우아하고 수학적으로 건전합니다. 그러나 실용적 영향은 논문이 주장하는 것보다 제한적입니다.</p> <p><strong>실무자들을 위해</strong>: 중간에서 긴 시퀀스(1K-8K 토큰), 충분한 하드웨어(고대역폭 네트워킹을 가진 16+ GPU), 그리고 시퀀스 길이가 주요 병목인 워크로드가 있을 때 시퀀스 병렬화를 고려하세요.</p> <p><strong>연구자들을 위해</strong>: 이 연구는 시퀀스 길이를 스케일링의 새로운 차원으로 열어주지만, 그래디언트 체크포인팅이나 더 효율적인 어텐션 메커니즘 같은 간단한 대안과 실질적으로 경쟁하려면 상당한 작업이 남아있습니다.</p> <p>근본적인 통찰 - 시스템 수준 솔루션이 알고리즘적 개선을 보완할 수 있다는 것 - 은 가치 있으며 분산 딥러닝의 향후 연구에 영감을 줄 가능성이 높습니다.</p> <hr/>]]></content><author><name></name></author><category term="sequence-parallelism"/><category term="ring-attention"/><category term="distributed-training"/><summary type="html"><![CDATA[Ring Self-Attention enables sequence parallelism across GPUs using ring communication patterns for distributed attention.]]></summary></entry><entry><title type="html">Pipeline Parallel (GPipe)</title><link href="https://sungyubkim.github.io/blog/pp/" rel="alternate" type="text/html" title="Pipeline Parallel (GPipe)"/><published>2025-05-29T00:00:00+09:00</published><updated>2025-05-29T00:00:00+09:00</updated><id>https://sungyubkim.github.io/blog/pp</id><content type="html" xml:base="https://sungyubkim.github.io/blog/pp/"><![CDATA[<h1 id="tldr">TL;DR</h1> <blockquote> <p>GPipe는 <strong>마이크로 배치 파이프라인 병렬화</strong>를 도입하여 단일 기기 메모리 한계를 넘어서는 신경망 훈련을 가능하게 합니다. 주요 기여점:</p> <ul> <li><strong>새로운 알고리즘</strong>: 미니배치를 마이크로배치로 분할하고 모델 파티션에 걸쳐 파이프라이닝하며 동기식 그라디언트 업데이트 수행</li> <li><strong>메모리 효율성</strong>: 재계산을 통해 최대 메모리 사용량을 O(N×L)에서 O(N/M + L/K×N/M)로 감소</li> <li><strong>아키텍처 독립성</strong>: 순차적 레이어 구조(CNN, Transformer 등)에서 범용적으로 작동</li> <li><strong>실용적 확장성</strong>: 8배 파티셔닝에서 79% 효율성으로 25-298배 더 큰 모델 훈련 가능</li> </ul> <p><strong>핵심</strong>: 대규모 모델 훈련을 민주화한 강력한 시스템 기여이지만, 품질 향상에 대한 실험적 증거는 혼재된 비교로 인해 약함.</p> </blockquote> <ul> <li>Paper Link: <a href="https://arxiv.org/pdf/1811.06965">https://arxiv.org/pdf/1811.06965</a></li> </ul> <hr/> <h1 id="related-papers">Related Papers</h1> <p><strong>대규모 모델 훈련:</strong></p> <ul> <li><a href="/blog/moe/">MoE</a> - 파이프라인과 결합 가능한 전문가 혼합 아키텍처</li> <li><a href="https://arxiv.org/pdf/2101.03961">Switch Transformers</a> - 파이프라인 병렬화와 호환되는 MoE</li> <li><a href="/blog/usp/">USP</a> - 파이프라인과 시퀀스 병렬화 통합</li> </ul> <p><strong>시스템 최적화:</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2211.05102">Efficiently Scaling Transformer Inference</a> - 파이프라인 기반 효율적 추론</li> <li><a href="https://arxiv.org/pdf/2309.14509">DeepSpeed Ulysses</a> - 파이프라인과 결합 가능한 시퀀스 병렬화</li> </ul> <hr/> <h1 id="takeaways">Takeaways</h1> <h3 id="핵심-문제와-동기">핵심 문제와 동기</h3> <p>딥러닝의 발전은 모델 확장과 밀접하게 연관되어 있습니다 - 더 큰 모델이 다양한 도메인에서 일관되게 더 나은 성능을 달성합니다. 하지만 하드웨어 메모리 제약이 근본적인 병목을 만듭니다: 가장 큰 가속기조차 특정 크기까지의 모델만 수용할 수 있습니다.</p> <p><strong>도전 과제</strong>: 기존 모델 병렬화 접근법들은 삼각 딜레마에 직면합니다:</p> <ul> <li><strong>효율성</strong>: 대부분의 접근법이 하드웨어를 심각하게 활용하지 못함</li> <li><strong>유연성</strong>: 해결책들이 종종 아키텍처 특화적임</li> <li><strong>일관성</strong>: 비동기 방법들이 훈련 불안정성을 야기</li> </ul> <p>GPipe는 다양한 아키텍처에서 높은 하드웨어 활용률을 달성하면서 훈련 일관성을 유지하는 새로운 패러다임을 도입하여 이를 해결합니다.</p> <h3 id="기술적-혁신-마이크로배치-파이프라인-병렬화">기술적 혁신: 마이크로배치 파이프라인 병렬화</h3> <h4 id="핵심-개념">핵심 개념</h4> <p>기기들이 순차적으로 대기하는 전통적인 모델 병렬화 대신, GPipe는:</p> <ol> <li>모델을 K개 기기에 걸쳐 <strong>분할</strong></li> <li>각 미니배치를 M개 마이크로배치로 <strong>분할</strong></li> <li>마이크로배치를 파티션들을 통해 <strong>파이프라이닝</strong></li> <li>그라디언트를 <strong>동기적으로 누적</strong></li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GPipe</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">num_partitions</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_microbatches</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">num_partitions</span>
        <span class="n">self</span><span class="p">.</span><span class="n">M</span> <span class="o">=</span> <span class="n">num_microbatches</span>
        <span class="c1"># 기기들 간 계산 균형을 맞추도록 모델 분할
</span>        <span class="n">self</span><span class="p">.</span><span class="n">cells</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_partition_model</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">minibatch</span><span class="p">):</span>
        <span class="c1"># 마이크로배치로 분할
</span>        <span class="n">microbatches</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_split_batch</span><span class="p">(</span><span class="n">minibatch</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">M</span><span class="p">)</span>
        
        <span class="c1"># 순방향 패스 파이프라이닝
</span>        <span class="n">activations</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_pipeline_forward</span><span class="p">(</span><span class="n">microbatches</span><span class="p">)</span>
        
        <span class="c1"># 역방향 패스 파이프라이닝
</span>        <span class="n">gradients</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_pipeline_backward</span><span class="p">(</span><span class="n">activations</span><span class="p">)</span>
        
        <span class="c1"># 동기식 그라디언트 업데이트 (일관성을 위한 핵심)
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">_accumulate_and_update</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
</code></pre></div></div> <h4 id="파이프라인-스케줄링-예시">파이프라인 스케줄링 예시</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>K=4개 기기, M=4개 마이크로배치에 대한 타임라인:

시간: 0  1  2  3  4  5  6
기기0: m0 m1 m2 m3 -- -- --
기기1: -- m0 m1 m2 m3 -- --  
기기2: -- -- m0 m1 m2 m3 --
기기3: -- -- -- m0 m1 m2 m3

버블 오버헤드 = (K-1)/(M+K-1) = 3/7 ≈ 43%
</code></pre></div></div> <p><strong>핵심 통찰</strong>: M ≥ 4×K일 때 버블 오버헤드가 무시할 수 있게 되어 거의 선형적 확장이 가능해집니다.</p> <h4 id="메모리-최적화-재계산">메모리 최적화: 재계산</h4> <p>전통적인 훈련은 역전파를 위해 모든 중간 활성화를 저장합니다. GPipe는 파티션 경계에서만 입력을 저장하고 역방향 패스 동안 순방향 패스를 재계산합니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward_with_checkpointing</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># 입력만 저장하고 중간값들은 버림
</span>    <span class="n">self</span><span class="p">.</span><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># 순방향 패스
</span>    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">backward_with_rematerialization</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
    <span class="c1"># 중간값들을 얻기 위해 순방향 패스 재계산
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">checkpoint</span>
    <span class="n">intermediates</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
        <span class="n">intermediates</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># 재계산된 활성화를 사용한 표준 역방향 패스
</span>    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">_backward_pass</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">intermediates</span><span class="p">)</span>
</code></pre></div></div> <p><strong>메모리 분석</strong>:</p> <ul> <li><strong>GPipe 없이</strong>: 배치 크기 N, 레이어 수 L에 대해 O(N×L) 메모리</li> <li><strong>GPipe 사용</strong>: O(N/M + L/K×N/M) 메모리</li> <li><strong>예시</strong>: N=128, L=100, M=8, K=4 → 97% 메모리 감소</li> </ul> <h3 id="실험적-증거와-비판적-평가">실험적 증거와 비판적 평가</h3> <h4 id="주요-결과-분석">주요 결과 분석</h4> <table> <thead> <tr> <th><strong>능력</strong></th> <th><strong>달성 사항</strong></th> <th><strong>증거 품질</strong></th> <th><strong>실용적 임팩트</strong></th> </tr> </thead> <tbody> <tr> <td><strong>메모리 확장</strong></td> <td>AmoebaNet: 25배 증가<br/>Transformer: 298배 증가</td> <td>✅ <strong>강함</strong>: 명확한 확장성 입증</td> <td>🔥 <strong>높음</strong>: 이전에 불가능했던 모델들 가능</td> </tr> <tr> <td><strong>처리량 효율성</strong></td> <td>8배 확장에서 79% 효율성</td> <td>✅ <strong>좋음</strong>: 버블 오버헤드 이론 검증</td> <td>🔥 <strong>높음</strong>: 거의 선형적 확장 달성</td> </tr> <tr> <td><strong>모델 품질</strong></td> <td>ImageNet 84.4% 정확도<br/>이중언어 번역 모델 능가</td> <td>❌ <strong>약함</strong>: 불공정한 기준선, 혼재된 비교</td> <td>❓ <strong>불명확</strong>: 품질 향상이 분리되지 않음</td> </tr> <tr> <td><strong>아키텍처 유연성</strong></td> <td>CNN + Transformer에서 작동</td> <td>⚠️ <strong>제한적</strong>: 2개 아키텍처 계열만</td> <td>📈 <strong>보통</strong>: 유망하지만 더 넓은 검증 필요</td> </tr> </tbody> </table> <h4 id="절제-연구ablation-study-통찰">절제 연구(Ablation Study) 통찰</h4> <table> <thead> <tr> <th><strong>구성요소</strong></th> <th><strong>영향</strong></th> <th><strong>핵심 발견</strong></th> </tr> </thead> <tbody> <tr> <td><strong>재계산</strong></td> <td>3.9배 메모리 감소</td> <td>단일 기기에서 더 큰 모델 적재에 필수적</td> </tr> <tr> <td><strong>마이크로배치 수(M)</strong></td> <td>M=32: 6.3배 가속 vs M=1: 1배</td> <td>파이프라인 이론 검증 - 더 많은 마이크로배치가 버블 시간 감소</td> </tr> <tr> <td><strong>파티션 수(K)</strong></td> <td>Transformer: 선형 확장<br/>AmoebaNet: 하위선형</td> <td><strong>중요</strong>: 균일한 아키텍처가 이질적인 것보다 더 잘 확장됨</td> </tr> <tr> <td><strong>통신</strong></td> <td>고속 상호연결 없이도 유사한 확장</td> <td><strong>놀라움</strong>: 예상과 달리 통신이 병목이 아님</td> </tr> </tbody> </table> <h4 id="가장-중요한-발견-아키텍처-의존성">가장 중요한 발견: 아키텍처 의존성</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Transformer (균일한 레이어): 8개 파티션에서 6.3배 가속
# AmoebaNet (이질적): 8개 파티션에서 3.48배 가속
</span>
<span class="c1"># 왜? 부하 균형 문제:
</span><span class="n">transformer_layer_costs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>  <span class="c1"># 균형 맞추기 쉬움
</span><span class="n">amoebanet_layer_costs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">250</span><span class="p">]</span>        <span class="c1"># 균형 맞추기 어려움
</span></code></pre></div></div> <p>이는 <strong>GPipe의 효과가 근본적으로 아키텍처 이질성에 의해 제한된다</strong>는 중요한 통찰을 보여줍니다 - 원본 논문에서 강조되지 않은 핵심적인 발견입니다.</p> <h3 id="중요한-가정과-성공-조건">중요한 가정과 성공 조건</h3> <h4 id="필수-조건들">필수 조건들</h4> <ol> <li><strong>단일 레이어 메모리 제약</strong>: 각 레이어가 하나의 기기에 맞아야 함</li> <li><strong>순차적 아키텍처</strong>: 모델이 레이어 시퀀스로 표현 가능해야 함</li> <li><strong>충분한 마이크로배치</strong>: 효율성을 위해 M ≥ 4×K 필요</li> <li><strong>균형 잡힌 파티셔닝</strong>: 레이어들이 대략 비슷한 계산 비용을 가져야 함</li> </ol> <h4 id="숨겨진-가정들">숨겨진 가정들</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 배치 정규화 가정
</span><span class="k">def</span> <span class="nf">batch_norm_caveat</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    BatchNorm은 훈련 중 마이크로배치에 대해 통계를 계산하지만
    평가를 위해서는 전체 미니배치 통계가 필요함.
    작은 마이크로배치는 훈련을 불안정하게 만들 수 있음.
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">micro_batch_size</span> <span class="o">&lt;</span> <span class="mi">8</span><span class="p">:</span>
        <span class="n">warnings</span><span class="p">.</span><span class="nf">warn</span><span class="p">(</span><span class="sh">"</span><span class="s">작은 마이크로배치가 BatchNorm을 불안정하게 만들 수 있음</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># 통신 가정
</span><span class="k">def</span> <span class="nf">communication_assumption</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    활성화 전송 시간 &lt;&lt; 계산 시간이라고 가정.
    매우 큰 활성화나 느린 상호연결에서는 성립하지 않음.
    </span><span class="sh">"""</span>
    <span class="n">transfer_time</span> <span class="o">=</span> <span class="n">activation_size</span> <span class="o">/</span> <span class="n">bandwidth</span>
    <span class="k">if</span> <span class="n">transfer_time</span> <span class="o">&gt;</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">compute_time</span><span class="p">:</span>
        <span class="n">warnings</span><span class="p">.</span><span class="nf">warn</span><span class="p">(</span><span class="sh">"</span><span class="s">통신이 병목이 되고 있음</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="강점과-한계">강점과 한계</h3> <h4 id="진정한-강점들">진정한 강점들</h4> <ul> <li><strong>우아한 엔지니어링</strong>: 효율성-유연성-일관성 삼각 딜레마 해결</li> <li><strong>광범위한 적용성</strong>: 다양한 아키텍처와 하드웨어에서 작동</li> <li><strong>강력한 이론적 기반</strong>: 버블 오버헤드 분석이 명확한 지침 제공</li> <li><strong>실용적 임팩트</strong>: 대규모 모델 혁명 가능하게 함</li> </ul> <h4 id="중요한-한계들">중요한 한계들</h4> <ul> <li><strong>아키텍처 의존성</strong>: 이익이 레이어 균일성에 크게 의존</li> <li><strong>메모리 제약</strong>: 단일 기기 메모리보다 큰 레이어 처리 불가</li> <li><strong>배치 크기 제한</strong>: 효과적으로 분할할 수 있을 만큼 큰 배치 필요</li> <li><strong>구현 복잡성</strong>: 데이터 병렬화보다 상당히 복잡</li> </ul> <h4 id="실험적-약점들">실험적 약점들</h4> <ul> <li><strong>기준선 부재</strong>: 동일한 총 매개변수를 가진 데이터 병렬화와의 비교 없음</li> <li><strong>혼재된 평가</strong>: 품질 향상이 규모 효과와 혼재됨</li> <li><strong>제한된 통계적 엄밀성</strong>: 단일 실행, 신뢰구간 없음</li> <li><strong>선별된 결과</strong>: 전이학습을 위한 선택적 데이터셋 선택</li> </ul> <h3 id="연구자와-실무자를-위한-실용적-함의">연구자와 실무자를 위한 실용적 함의</h3> <h4 id="gpipe를-언제-사용할-것인가">GPipe를 언제 사용할 것인가</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">should_use_gpipe</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">hardware</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">GPipe 채택을 위한 의사결정 프레임워크</span><span class="sh">"""</span>
    
    <span class="c1"># 메모리 확인
</span>    <span class="k">if</span> <span class="n">model</span><span class="p">.</span><span class="n">memory_requirement</span> <span class="o">&lt;=</span> <span class="n">single_device_memory</span><span class="p">:</span>
        <span class="k">return</span> <span class="sh">"</span><span class="s">대신 데이터 병렬화 사용</span><span class="sh">"</span>
    
    <span class="c1"># 아키텍처 확인
</span>    <span class="k">if</span> <span class="n">model</span><span class="p">.</span><span class="nf">has_uniform_layers</span><span class="p">():</span>
        <span class="k">return</span> <span class="sh">"</span><span class="s">GPipe가 잘 확장될 것</span><span class="sh">"</span>
    <span class="k">elif</span> <span class="n">model</span><span class="p">.</span><span class="nf">has_very_heterogeneous_layers</span><span class="p">():</span>
        <span class="k">return</span> <span class="sh">"</span><span class="s">다른 접근법 고려 (Mesh-TensorFlow)</span><span class="sh">"</span>
    
    <span class="c1"># 배치 크기 확인
</span>    <span class="n">min_micro_batch</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">num_devices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">min_micro_batch</span> <span class="o">&lt;</span> <span class="mi">8</span><span class="p">:</span>
        <span class="k">return</span> <span class="sh">"</span><span class="s">배치 크기 증가 또는 더 적은 기기 사용</span><span class="sh">"</span>
    
    <span class="k">return</span> <span class="sh">"</span><span class="s">GPipe가 적합함</span><span class="sh">"</span>
</code></pre></div></div> <h4 id="구현-고려사항들">구현 고려사항들</h4> <ol> <li><strong>단순하게 시작</strong>: 균형 잡힌 파티셔닝으로 시작, 나중에 최적화</li> <li><strong>효율성 모니터링</strong>: 버블 오버헤드와 통신 비용 추적</li> <li><strong>배치 정규화 처리</strong>: 파티션 간 SyncBatchNorm 같은 기법 사용</li> <li><strong>메모리 프로파일링</strong>: 재계산 메모리 급증 감시</li> </ol> <h4 id="대안적-접근법들">대안적 접근법들</h4> <ul> <li><strong>데이터 병렬화</strong>: 모델이 단일 기기에 맞을 때</li> <li><strong>텐서 병렬화</strong> (Mesh-TensorFlow): 매우 큰 개별 레이어용</li> <li><strong>전문가 병렬화</strong>: 전문가 혼합 아키텍처용</li> <li><strong>파이프라인 + 데이터 하이브리드</strong>: 최대 규모용</li> </ul> <h3 id="임팩트와-미래-방향">임팩트와 미래 방향</h3> <h4 id="역사적-중요성">역사적 중요성</h4> <p>GPipe는 다음을 통해 대규모 언어 모델 혁명을 가능하게 하는 데 중요한 역할을 했습니다:</p> <ul> <li>트랜스포머 확장을 실용적으로 가능하게 만듦</li> <li>실험을 위한 유연한 프레임워크 제공</li> <li>모델 병렬화가 효율적이면서 범용적일 수 있음을 입증</li> </ul> <h4 id="기술적-발전">기술적 발전</h4> <p>현대 접근법들이 GPipe의 통찰을 바탕으로 구축됩니다:</p> <ul> <li><strong>PipeDream</strong>: 더 높은 처리량을 위한 비동기 파이프라인</li> <li><strong>FairScale</strong>: 프로덕션 준비 구현</li> <li><strong>DeepSpeed</strong>: 하이브리드 병렬화 전략</li> <li><strong>Megatron</strong>: 트랜스포머 특화 최적화 접근법</li> </ul> <h4 id="열린-연구-질문들">열린 연구 질문들</h4> <ol> <li><strong>최적 파티셔닝</strong>: 이질적 아키텍처를 위한 더 나은 알고리즘</li> <li><strong>동적 스케줄링</strong>: 부하 기반 적응적 마이크로배치 크기 조정</li> <li><strong>메모리 최적화</strong>: 더 정교한 체크포인팅 전략</li> <li><strong>장애 허용성</strong>: 긴 훈련 실행에서 기기 장애 처리</li> </ol> <h3 id="큰-그림">큰 그림</h3> <p>GPipe는 직접적인 알고리즘적 돌파구를 제공하기보다는 새로운 과학적 가능성을 가능하게 한 고전적인 <strong>시스템 기여</strong>를 나타냅니다. 그 지속적인 가치는 다음에 있습니다:</p> <ol> <li><strong>민주화</strong>: 기술 대기업을 넘어 대규모 모델 훈련을 접근 가능하게 만듦</li> <li><strong>유연성</strong>: 아키텍처 특화 해결책이 아닌 일반적 프레임워크 제공</li> <li><strong>기반</strong>: 후속 병렬화 연구를 위한 설계 원칙 확립</li> <li><strong>실용적 임팩트</strong>: NLP와 컴퓨터 비전에서 많은 획기적 모델들을 직접 가능하게 함</li> </ol> <p><strong>핵심 통찰</strong>: 이 논문의 진정한 기여는 더 큰 모델이 더 낫다는 것을 증명하는 것이 아니라, 그러한 가설을 탐험할 수 있는 인프라를 제공하는 것입니다. 품질 향상에 대한 약한 실험적 증거가 그러한 탐험을 가능하게 만든 의미를 감소시키지는 않습니다.</p> <p>오늘날 실무자들에게 GPipe의 핵심 아이디어들은 여전히 관련이 있지만, 프로덕션 시스템들은 종종 더 정교한 하이브리드 접근법을 사용합니다. GPipe를 이해하는 것은 현대 분산 훈련 시스템과 모델 병렬화의 근본적인 트레이드오프에 대한 필수적인 직관을 제공합니다.</p> <hr/>]]></content><author><name></name></author><category term="pipeline-parallelism"/><category term="distributed-training"/><category term="memory-efficiency"/><summary type="html"><![CDATA[Deep dive into GPipe's micro-batch pipeline parallelism for training models beyond single-device memory limits.]]></summary></entry></feed>