---
title: "Pipeline Parallel (GPipe)"
date: 2025-05-29
last_modified_at: 2025-05-29
type: posts
layout: single
author_profile: false
comments: true
permalink: /blog/pp/
tags:
    - parallelism/pipeline-stages
    - training/microbatch-optimization
    - memory/gradient-checkpointing
    - systems/model-partitioning
use_math: true
toc: true
toc_label: "Contents"
toc_icon: "file-text"
toc_sticky: true
---

# TL;DR

> GPipe는 **마이크로 배치 파이프라인 병렬화**를 도입하여 단일 기기 메모리 한계를 넘어서는 신경망 훈련을 가능하게 합니다.
> 주요 기여점:
> - **새로운 알고리즘**: 미니배치를 마이크로배치로 분할하고 모델 파티션에 걸쳐 파이프라이닝하며 동기식 그라디언트 업데이트 수행
> - **메모리 효율성**: 재계산을 통해 최대 메모리 사용량을 O(N×L)에서 O(N/M + L/K×N/M)로 감소
> - **아키텍처 독립성**: 순차적 레이어 구조(CNN, Transformer 등)에서 범용적으로 작동
> - **실용적 확장성**: 8배 파티셔닝에서 79% 효율성으로 25-298배 더 큰 모델 훈련 가능
> 
> **핵심**: 대규모 모델 훈련을 민주화한 강력한 시스템 기여이지만, 품질 향상에 대한 실험적 증거는 혼재된 비교로 인해 약함.

- Paper Link: [https://arxiv.org/pdf/1811.06965](https://arxiv.org/pdf/1811.06965)

---

# Related Papers

**대규모 모델 훈련:**
- [MoE](../moe) - 파이프라인과 결합 가능한 전문가 혼합 아키텍처
- [Switch Transformers](https://arxiv.org/pdf/2101.03961) - 파이프라인 병렬화와 호환되는 MoE
- [USP](../usp) - 파이프라인과 시퀀스 병렬화 통합

**시스템 최적화:**
- [Efficiently Scaling Transformer Inference](https://arxiv.org/pdf/2211.05102) - 파이프라인 기반 효율적 추론
- [DeepSpeed Ulysses](https://arxiv.org/pdf/2309.14509) - 파이프라인과 결합 가능한 시퀀스 병렬화

---

# Takeaways

### 핵심 문제와 동기

딥러닝의 발전은 모델 확장과 밀접하게 연관되어 있습니다 - 더 큰 모델이 다양한 도메인에서 일관되게 더 나은 성능을 달성합니다. 하지만 하드웨어 메모리 제약이 근본적인 병목을 만듭니다: 가장 큰 가속기조차 특정 크기까지의 모델만 수용할 수 있습니다.

**도전 과제**: 기존 모델 병렬화 접근법들은 삼각 딜레마에 직면합니다:
- **효율성**: 대부분의 접근법이 하드웨어를 심각하게 활용하지 못함
- **유연성**: 해결책들이 종종 아키텍처 특화적임
- **일관성**: 비동기 방법들이 훈련 불안정성을 야기

GPipe는 다양한 아키텍처에서 높은 하드웨어 활용률을 달성하면서 훈련 일관성을 유지하는 새로운 패러다임을 도입하여 이를 해결합니다.

### 기술적 혁신: 마이크로배치 파이프라인 병렬화

#### 핵심 개념
기기들이 순차적으로 대기하는 전통적인 모델 병렬화 대신, GPipe는:
1. 모델을 K개 기기에 걸쳐 **분할**
2. 각 미니배치를 M개 마이크로배치로 **분할**
3. 마이크로배치를 파티션들을 통해 **파이프라이닝**
4. 그라디언트를 **동기적으로 누적**

```python
class GPipe:
    def __init__(self, layers, num_partitions=4, num_microbatches=8):
        self.K = num_partitions
        self.M = num_microbatches
        # 기기들 간 계산 균형을 맞추도록 모델 분할
        self.cells = self._partition_model(layers)
    
    def training_step(self, minibatch):
        # 마이크로배치로 분할
        microbatches = self._split_batch(minibatch, self.M)
        
        # 순방향 패스 파이프라이닝
        activations = self._pipeline_forward(microbatches)
        
        # 역방향 패스 파이프라이닝
        gradients = self._pipeline_backward(activations)
        
        # 동기식 그라디언트 업데이트 (일관성을 위한 핵심)
        self._accumulate_and_update(gradients)
```

#### 파이프라인 스케줄링 예시
```
K=4개 기기, M=4개 마이크로배치에 대한 타임라인:

시간: 0  1  2  3  4  5  6
기기0: m0 m1 m2 m3 -- -- --
기기1: -- m0 m1 m2 m3 -- --  
기기2: -- -- m0 m1 m2 m3 --
기기3: -- -- -- m0 m1 m2 m3

버블 오버헤드 = (K-1)/(M+K-1) = 3/7 ≈ 43%
```

**핵심 통찰**: M ≥ 4×K일 때 버블 오버헤드가 무시할 수 있게 되어 거의 선형적 확장이 가능해집니다.

#### 메모리 최적화: 재계산

전통적인 훈련은 역전파를 위해 모든 중간 활성화를 저장합니다. GPipe는 파티션 경계에서만 입력을 저장하고 역방향 패스 동안 순방향 패스를 재계산합니다:

```python
def forward_with_checkpointing(self, x):
    # 입력만 저장하고 중간값들은 버림
    self.checkpoint = x.detach().requires_grad_(True)
    
    # 순방향 패스
    for layer in self.layers:
        x = layer(x)
    return x

def backward_with_rematerialization(self, grad_output):
    # 중간값들을 얻기 위해 순방향 패스 재계산
    x = self.checkpoint
    intermediates = []
    for layer in self.layers:
        intermediates.append(x)
        x = layer(x)
    
    # 재계산된 활성화를 사용한 표준 역방향 패스
    return self._backward_pass(grad_output, intermediates)
```

**메모리 분석**:
- **GPipe 없이**: 배치 크기 N, 레이어 수 L에 대해 O(N×L) 메모리
- **GPipe 사용**: O(N/M + L/K×N/M) 메모리
- **예시**: N=128, L=100, M=8, K=4 → 97% 메모리 감소

### 실험적 증거와 비판적 평가

#### 주요 결과 분석

| **능력** | **달성 사항** | **증거 품질** | **실용적 임팩트** |
|----------|---------------|---------------|-------------------|
| **메모리 확장** | AmoebaNet: 25배 증가<br>Transformer: 298배 증가 | ✅ **강함**: 명확한 확장성 입증 | 🔥 **높음**: 이전에 불가능했던 모델들 가능 |
| **처리량 효율성** | 8배 확장에서 79% 효율성 | ✅ **좋음**: 버블 오버헤드 이론 검증 | 🔥 **높음**: 거의 선형적 확장 달성 |
| **모델 품질** | ImageNet 84.4% 정확도<br>이중언어 번역 모델 능가 | ❌ **약함**: 불공정한 기준선, 혼재된 비교 | ❓ **불명확**: 품질 향상이 분리되지 않음 |
| **아키텍처 유연성** | CNN + Transformer에서 작동 | ⚠️ **제한적**: 2개 아키텍처 계열만 | 📈 **보통**: 유망하지만 더 넓은 검증 필요 |

#### 절제 연구(Ablation Study) 통찰

| **구성요소** | **영향** | **핵심 발견** |
|--------------|----------|---------------|
| **재계산** | 3.9배 메모리 감소 | 단일 기기에서 더 큰 모델 적재에 필수적 |
| **마이크로배치 수(M)** | M=32: 6.3배 가속 vs M=1: 1배 | 파이프라인 이론 검증 - 더 많은 마이크로배치가 버블 시간 감소 |
| **파티션 수(K)** | Transformer: 선형 확장<br>AmoebaNet: 하위선형 | **중요**: 균일한 아키텍처가 이질적인 것보다 더 잘 확장됨 |
| **통신** | 고속 상호연결 없이도 유사한 확장 | **놀라움**: 예상과 달리 통신이 병목이 아님 |

#### 가장 중요한 발견: 아키텍처 의존성

```python
# Transformer (균일한 레이어): 8개 파티션에서 6.3배 가속
# AmoebaNet (이질적): 8개 파티션에서 3.48배 가속

# 왜? 부하 균형 문제:
transformer_layer_costs = [100, 100, 100, 100, 100, 100, 100, 100]  # 균형 맞추기 쉬움
amoebanet_layer_costs = [50, 200, 80, 300, 60, 150, 90, 250]        # 균형 맞추기 어려움
```

이는 **GPipe의 효과가 근본적으로 아키텍처 이질성에 의해 제한된다**는 중요한 통찰을 보여줍니다 - 원본 논문에서 강조되지 않은 핵심적인 발견입니다.

### 중요한 가정과 성공 조건

#### 필수 조건들
1. **단일 레이어 메모리 제약**: 각 레이어가 하나의 기기에 맞아야 함
2. **순차적 아키텍처**: 모델이 레이어 시퀀스로 표현 가능해야 함
3. **충분한 마이크로배치**: 효율성을 위해 M ≥ 4×K 필요
4. **균형 잡힌 파티셔닝**: 레이어들이 대략 비슷한 계산 비용을 가져야 함

#### 숨겨진 가정들
```python
# 배치 정규화 가정
def batch_norm_caveat():
    """
    BatchNorm은 훈련 중 마이크로배치에 대해 통계를 계산하지만
    평가를 위해서는 전체 미니배치 통계가 필요함.
    작은 마이크로배치는 훈련을 불안정하게 만들 수 있음.
    """
    if micro_batch_size < 8:
        warnings.warn("작은 마이크로배치가 BatchNorm을 불안정하게 만들 수 있음")

# 통신 가정
def communication_assumption():
    """
    활성화 전송 시간 << 계산 시간이라고 가정.
    매우 큰 활성화나 느린 상호연결에서는 성립하지 않음.
    """
    transfer_time = activation_size / bandwidth
    if transfer_time > 0.1 * compute_time:
        warnings.warn("통신이 병목이 되고 있음")
```

### 강점과 한계

#### 진정한 강점들
- **우아한 엔지니어링**: 효율성-유연성-일관성 삼각 딜레마 해결
- **광범위한 적용성**: 다양한 아키텍처와 하드웨어에서 작동
- **강력한 이론적 기반**: 버블 오버헤드 분석이 명확한 지침 제공
- **실용적 임팩트**: 대규모 모델 혁명 가능하게 함

#### 중요한 한계들
- **아키텍처 의존성**: 이익이 레이어 균일성에 크게 의존
- **메모리 제약**: 단일 기기 메모리보다 큰 레이어 처리 불가
- **배치 크기 제한**: 효과적으로 분할할 수 있을 만큼 큰 배치 필요
- **구현 복잡성**: 데이터 병렬화보다 상당히 복잡

#### 실험적 약점들
- **기준선 부재**: 동일한 총 매개변수를 가진 데이터 병렬화와의 비교 없음
- **혼재된 평가**: 품질 향상이 규모 효과와 혼재됨
- **제한된 통계적 엄밀성**: 단일 실행, 신뢰구간 없음
- **선별된 결과**: 전이학습을 위한 선택적 데이터셋 선택

### 연구자와 실무자를 위한 실용적 함의

#### GPipe를 언제 사용할 것인가
```python
def should_use_gpipe(model, hardware, batch_size):
    """GPipe 채택을 위한 의사결정 프레임워크"""
    
    # 메모리 확인
    if model.memory_requirement <= single_device_memory:
        return "대신 데이터 병렬화 사용"
    
    # 아키텍처 확인
    if model.has_uniform_layers():
        return "GPipe가 잘 확장될 것"
    elif model.has_very_heterogeneous_layers():
        return "다른 접근법 고려 (Mesh-TensorFlow)"
    
    # 배치 크기 확인
    min_micro_batch = batch_size // (4 * num_devices)
    if min_micro_batch < 8:
        return "배치 크기 증가 또는 더 적은 기기 사용"
    
    return "GPipe가 적합함"
```

#### 구현 고려사항들
1. **단순하게 시작**: 균형 잡힌 파티셔닝으로 시작, 나중에 최적화
2. **효율성 모니터링**: 버블 오버헤드와 통신 비용 추적
3. **배치 정규화 처리**: 파티션 간 SyncBatchNorm 같은 기법 사용
4. **메모리 프로파일링**: 재계산 메모리 급증 감시

#### 대안적 접근법들
- **데이터 병렬화**: 모델이 단일 기기에 맞을 때
- **텐서 병렬화** (Mesh-TensorFlow): 매우 큰 개별 레이어용
- **전문가 병렬화**: 전문가 혼합 아키텍처용
- **파이프라인 + 데이터 하이브리드**: 최대 규모용

### 임팩트와 미래 방향

#### 역사적 중요성
GPipe는 다음을 통해 대규모 언어 모델 혁명을 가능하게 하는 데 중요한 역할을 했습니다:
- 트랜스포머 확장을 실용적으로 가능하게 만듦
- 실험을 위한 유연한 프레임워크 제공
- 모델 병렬화가 효율적이면서 범용적일 수 있음을 입증

#### 기술적 발전
현대 접근법들이 GPipe의 통찰을 바탕으로 구축됩니다:
- **PipeDream**: 더 높은 처리량을 위한 비동기 파이프라인
- **FairScale**: 프로덕션 준비 구현
- **DeepSpeed**: 하이브리드 병렬화 전략
- **Megatron**: 트랜스포머 특화 최적화 접근법

#### 열린 연구 질문들
1. **최적 파티셔닝**: 이질적 아키텍처를 위한 더 나은 알고리즘
2. **동적 스케줄링**: 부하 기반 적응적 마이크로배치 크기 조정
3. **메모리 최적화**: 더 정교한 체크포인팅 전략
4. **장애 허용성**: 긴 훈련 실행에서 기기 장애 처리

### 큰 그림

GPipe는 직접적인 알고리즘적 돌파구를 제공하기보다는 새로운 과학적 가능성을 가능하게 한 고전적인 **시스템 기여**를 나타냅니다. 그 지속적인 가치는 다음에 있습니다:

1. **민주화**: 기술 대기업을 넘어 대규모 모델 훈련을 접근 가능하게 만듦
2. **유연성**: 아키텍처 특화 해결책이 아닌 일반적 프레임워크 제공
3. **기반**: 후속 병렬화 연구를 위한 설계 원칙 확립
4. **실용적 임팩트**: NLP와 컴퓨터 비전에서 많은 획기적 모델들을 직접 가능하게 함

**핵심 통찰**: 이 논문의 진정한 기여는 더 큰 모델이 더 낫다는 것을 증명하는 것이 아니라, 그러한 가설을 탐험할 수 있는 인프라를 제공하는 것입니다. 품질 향상에 대한 약한 실험적 증거가 그러한 탐험을 가능하게 만든 의미를 감소시키지는 않습니다.

오늘날 실무자들에게 GPipe의 핵심 아이디어들은 여전히 관련이 있지만, 프로덕션 시스템들은 종종 더 정교한 하이브리드 접근법을 사용합니다. GPipe를 이해하는 것은 현대 분산 훈련 시스템과 모델 병렬화의 근본적인 트레이드오프에 대한 필수적인 직관을 제공합니다.

---