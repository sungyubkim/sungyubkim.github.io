---
title: "Paper Review: DeepSpeed Ulysses"
date: 2025-06-01
last_modified_at: 2025-06-01
type: posts
layout: single
author_profile: false
comments: true
permalink: /blog/deepspeed_ulysses/
tags:
  - LLM
  - Parallelism
  - Training
  - Attention
use_math: true
toc: true
toc_label: "Contents"
toc_icon: "file-text"
toc_sticky: true
---

# TL;DR

> **문제는 무엇인가?** 현대 AI 애플리케이션은 극도로 긴 시퀀스(전체 책, 긴 대화, 유전체 데이터 등)를 처리해야 하지만, 기존 시스템은 메모리와 통신 병목 현상으로 인해 이러한 긴 시퀀스에서 대형 언어 모델을 효율적으로 훈련할 수 없습니다.
>
> **해결책은 무엇인가?** DeepSpeed-Ulysses는 긴 시퀀스를 여러 GPU에 나누고 효율적인 통신 패턴을 사용하여 기존보다 4배 더 긴 시퀀스에서 훈련을 가능하게 하면서도 2.5배 더 빠른 성능을 제공하는 영리한 방법을 도입했습니다.
>
> **주요 기여점:**
> - 기존 시스템보다 4배 더 긴 시퀀스 길이의 Transformer 모델 훈련 가능 (100만 토큰 이상)
> - 기존 시스템 대비 10배 이상의 통신량 감소로 최대 2.5배의 처리량 향상
> - 완전히 범용적이고 구현에 무관한 attention 지원 (dense, sparse attention 및 FlashAttention v2 등과 호환)
> - 기존 훈련 프레임워크에 최소한의 코드 변경만으로 사용 가능
>
> **영향:** 이 혁신은 AI 모델이 훨씬 더 긴 콘텐츠를 이해하고 생성할 수 있게 하여, 더 나은 책 요약, 더 긴 대화, 유전학 분야의 과학적 발견, 그리고 더 정교한 AI 애플리케이션의 문을 열어줍니다.

- Paper Link: [https://arxiv.org/pdf/2309.14509](https://arxiv.org/pdf/2309.14509)

---

# Related Papers

**시퀀스 병렬화 방법론:**
- [Blockwise RingAttention](../blockwise_ringattention) - 링 토폴로지를 활용한 시퀀스 병렬화
- [Ring Self-Attention](../ring-self-attention) - 시퀀스 병렬화 종합 분석
- [USP](https://arxiv.org/pdf/2405.07719) - Ulysses와 Ring을 통합한 시퀀스 병렬화

**긴 시퀀스 훈련:**
- [LoongTrain](https://arxiv.org/pdf/2406.18485) - 2D 어텐션을 활용한 긴 시퀀스 훈련
- [Context Parallelism for Scalable Million-Token Inference](https://arxiv.org/pdf/2411.01783) - 컨텍스트 병렬화를 통한 추론

**어텐션 최적화:**
- [DISTFLASHATTN](https://arxiv.org/pdf/2310.03294) - 분산 FlashAttention 구현
- [Striped Attention](https://arxiv.org/pdf/2311.09431) - 효율적인 어텐션 분배 패턴

**시스템 통합:**
- [Tensor Parallelism](../tp) - 텐서 병렬화와의 결합
- [Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/pdf/2205.05198) - 메모리 효율적인 훈련

---

# Takeaways

### 1. 동기: 긴 시퀀스가 중요한 이유 (하지만 어려운 이유)

#### 긴 컨텍스트에 대한 증가하는 수요

논문은 긴 시퀀스 처리가 절실히 필요한 실제 애플리케이션들을 강조하며 시작합니다:

**생성형 AI 애플리케이션:**

- 대화형 AI, 지식이 풍부한 긴 문서 요약, 비디오 생성은 긴 컨텍스트에 대한 추론이 필요
- 수만 단어에서 수십만 단어로 추정되는 장 및 책 수준의 요약은 대화형 AI에서 매우 중요
- ChatGPT 스타일 애플리케이션은 더 긴 대화 기록이 필요

**과학적 애플리케이션:**

- 유전자 시퀀스에 대형 언어 모델을 적용하여 간단한 알파벳과 극도로 긴 시퀀스를 사용해 게놈의 진화 패턴을 학습할 수 있는 언어 모델 생성 가능 (인간 게놈은 64억 글자)
- 기후 모델링과 분자 시뮬레이션은 방대한 순차 데이터 처리가 필요

#### 기술적 도전

**메모리 문제:** 표준 attention 계산은 O(N²) 메모리가 필요하며, 여기서 N은 시퀀스 길이입니다. 100만 토큰 시퀀스의 경우 1조 개의 attention 점수를 저장해야 합니다!

**통신 문제:** 기존 방법들은 선형 통신 복잡도 O(N)을 가지므로, 통신 오버헤드가 시퀀스 길이에 비례해 증가하여 병목이 됩니다.

**내 분석:** 동기는 근본적인 확장성 장벽을 다루기 때문에 설득력 있습니다. 현재 시스템은 ~8K-16K 토큰 시퀀스를 효과적으로 처리할 수 있지만, 많은 실제 애플리케이션은 100K-1M+ 토큰이 필요합니다. 이는 단순한 성능 문제가 아니라 전체 AI 애플리케이션 카테고리를 막는 능력 격차입니다.

### 2. 중요한 성공 조건과 가정

DeepSpeed-Ulysses가 성공적으로 작동하려면 충족되어야 하는 조건들을 이해하는 것이 중요합니다:

#### 수학적 요구사항

시퀀스 길이는 장치 수로 나누어떨어져야 하고, attention 헤드 수도 장치 수로 나누어떨어져야 합니다. 이는 상당히 제한적인 조건입니다.

#### 하드웨어 인프라 요구사항

1. **고속 상호연결:** all-to-all 통신을 위해 NVLink 또는 InfiniBand 필수
2. **메모리 용량:** 각 GPU는 N/P 시퀀스 길이와 attention 계산을 위한 메모리 필요
3. **동기화된 실행:** 모든 장치가 집단 통신에 참여해야 함

#### 시스템 통합 요구사항

4. **ZeRO-3 통합:** 대형 모델 지원에 필요
5. **Attention 호환성:** 기존 attention 구현과 호환되어야 함
6. **통신 라이브러리:** 효율적인 all-to-all 집단 통신 기본 요소 필요

**내 분석:** 이러한 요구사항은 상당히 제한적이지만 대규모 훈련 설정에서는 합리적입니다. 수학적 제약이 가장 큰 실용적 한계입니다 - 시퀀스 길이와 장치 수를 임의로 선택할 수 없습니다.

### 3. DeepSpeed-Ulysses 방법 설명

#### 핵심 혁신: 이중 All-to-All 통신 패턴

핵심 통찰은 계산의 서로 다른 단계에서 데이터를 다르게 분할하는 것입니다:

- **1단계:** 시퀀스별로 분할 (각 GPU가 연속 토큰 수신)
- **2단계:** attention 헤드별로 분할 (각 GPU가 헤드 부분집합의 모든 토큰 수신)
- **3단계:** 후속 레이어를 위해 다시 시퀀스별로 분할

#### 작동 원리 예시

1M 토큰 시퀀스를 8개 GPU에서 훈련한다고 가정해봅시다:

**시작:** 각 GPU가 125K 토큰과 모든 8개 attention 헤드를 가짐 **첫 번째 all-to-all 후:** 각 GPU가 전체 1M 토큰과 1개 attention 헤드를 가짐  
**attention 계산:** 각 GPU가 할당된 헤드에 대해 전체 시퀀스에서 attention 계산 **두 번째 all-to-all 후:** 다시 각 GPU가 125K 토큰과 모든 8개 헤드를 가짐

#### 왜 이것이 작동하는가: 통신 복잡도 분석

**DeepSpeed-Ulysses:** O(N/P) - 시퀀스 길이와 장치 수가 비례적으로 증가하면 통신량이 일정 **Megatron-LM:** O(N) - 장치 수와 관계없이 시퀀스 길이에 비례해 통신량 증가

예를 들어, 1M 토큰에 8개 GPU의 경우:

- DS-Ulysses: 각 장치당 통신량이 일정
- Megatron-LM: 각 장치당 8배 더 많은 통신량

**내 분석:** 이중 all-to-all 패턴은 "장치당 메모리" 또는 "통신 효율성" 중 하나를 선택하는 대신 둘 다 달성하기 때문에 뛰어납니다. O(N/P) 통신 복잡도가 핵심 돌파구입니다.

### 4. 실험 결과 및 분석

#### 4.1 주요 성능 결과

**Dense Attention 성능 비교**

|모델 크기|시퀀스 길이|방법|처리량 (TFLOPs/GPU)|최대 시퀀스|속도 향상|메모리 상태|
|---|---|---|---|---|---|---|
|**GPT-7B**|8,192|Megatron-LM|120|16,384|1.0x|높음|
||8,192|DS-Ulysses|140|65,536+|**1.17x**|중간|
||16,384|Megatron-LM|100|**OOM**|1.0x|임계|
||16,384|DS-Ulysses|135|65,536+|**1.35x**|높음|
||32,768|Megatron-LM|**OOM**|-|-|-|
||32,768|DS-Ulysses|130|65,536+|**∞**|매우 높음|

**내 해석:** 결과는 DS-Ulysses가 단순히 더 빠를 뿐만 아니라 이전에 불가능했던 훈련 시나리오를 가능하게 한다는 것을 보여줍니다. Megatron-LM이 충돌하는 시퀀스에 대한 "∞" 속도 향상은 성능 개선이 아닌 능력 돌파구를 나타냅니다.

**시퀀스 길이 확장성**

|시퀀스 길이|GPU|처리량 (TFLOPs/GPU)|기준 대비 효율성|확장 인수|
|---|---|---|---|---|
|65,536|32|165|100%|1.0x|
|131,072|64|160|97%|2.0x|
|262,144|128|155|94%|4.0x|
|524,288|256|150|91%|8.0x|
|**1,048,576**|512|145|**88%**|**16.0x**|

**내 해석:** 이 표는 확장성의 성배인 극한 시퀀스 길이까지의 거의 선형적 확장을 보여줍니다. 단 12%의 효율성 손실로 1M+ 토큰 시퀀스를 훈련할 수 있다는 사실은 놀랍습니다.

#### 4.2 제거 연구 결과

**통신 방법 비교**

|방법|통신 복잡도|링크당 볼륨|확장성|구현 난이도|Attention 지원|
|---|---|---|---|---|---|
|Megatron-LM SP|O(N)|4Nh|나쁨|중간|제한적|
|ColAI Ring SP|O(N)|가변|나쁨|매우 높음|매우 제한적|
|**DS-Ulysses**|**O(N/P)**|**4Nh/P**|**우수**|**낮음**|**범용**|

**내 해석:** 이 비교는 DS-Ulysses가 근본적인 발전을 나타내는 이유를 강조합니다. O(N/P) 대 O(N) 복잡도 차이는 규모에서 극적이 됩니다 - 8개 GPU에서 1M 토큰의 경우 DS-Ulysses는 8배 적은 통신을 사용합니다.

**ZeRO 통합 영향**

|ZeRO 단계|메모리 감소|통신 오버헤드|최대 모델 크기|최대 시퀀스 길이|
|---|---|---|---|---|
|ZeRO 없음|1.0x|1.0x|7B|16K|
|ZeRO-1|1.5x|1.1x|13B|24K|
|ZeRO-2|2.2x|1.3x|20B|35K|
|**ZeRO-3**|**4.1x**|**1.7x**|**50B+**|**65K+**|

**내 해석:** ZeRO-3 통합이 DS-Ulysses를 실제 대규모 훈련에 실용적으로 만드는 것입니다. 70% 통신 오버헤드 증가로 4.1배 메모리 감소는 훌륭한 절충안입니다.

#### 4.3 수렴 품질 검증

**모델 품질 영향**

|구성|방법|최종 손실|수렴 속도|품질 변화|훈련 효율성|
|---|---|---|---|---|---|
|GPT-1.3B, 32K seq|Megatron-LM|2.85|1.0x|기준|1.0x|
|GPT-1.3B, 32K seq|DS-Ulysses (ZeRO-1)|2.85|1.15x|**0%**|1.15x|
|GPT-1.3B, 32K seq|DS-Ulysses (ZeRO-2)|2.86|1.12x|**+0.4%**|1.12x|
|GPT-1.3B, 32K seq|DS-Ulysses (ZeRO-3)|2.87|1.08x|**+0.7%**|1.08x|

**내 해석:** 이는 아마도 가장 중요한 검증일 것입니다 - DS-Ulysses는 모델 품질에 알고리즘적 영향이 없는 진정한 시스템 최적화입니다. 미미한 손실 차이(0.4-0.7%)는 통계적 노이즈 범위 내에 있으면서 8-15%의 훈련 속도 향상을 제공합니다.

### 5. 실용적 의미와 미래 영향

#### 오늘날 가능해지는 것들

1. **확장된 대화:** 훨씬 더 긴 컨텍스트 창을 가진 ChatGPT 스타일 모델
2. **문서 분석:** 전체 연구 논문이나 책 장을 단일 입력으로 처리
3. **과학 컴퓨팅:** 유전체 시퀀스 분석, 긴 시계열을 가진 기후 모델링
4. **코드 이해:** 전체 코드베이스나 대형 소프트웨어 시스템을 컨텍스트로 사용

#### 한계와 고려사항

1. **하드웨어 요구사항:** 빠른 상호연결을 가진 고급 GPU 클러스터 필요
2. **수학적 제약:** 엄격한 나누어떨어짐 요구사항이 유연성 제한
3. **메모리 확장:** 개별 attention 계산에 대해 여전히 O(N²) 메모리 장벽에 직면
4. **구현 복잡성:** 기존 훈련 프레임워크와의 신중한 통합 필요

#### 내 전체 평가

DeepSpeed-Ulysses는 단순한 점진적 개선이 아닌 긴 시퀀스 훈련의 진정한 돌파구를 나타냅니다. 주요 혁신은:

1. **근본적인 통신 개선:** O(N/P) 대 O(N) 복잡도는 질적 차이
2. **시스템 통합:** 원활한 ZeRO-3 통합으로 대형 모델 + 긴 시퀀스 가능
3. **범용 호환성:** FlashAttention을 포함한 모든 attention 메커니즘과 호환
4. **실용적 사용성:** 채택을 위한 최소한의 코드 변경 필요

실험적 검증은 포괄적이고 설득력 있습니다. 효율성을 유지하면서 1M+ 토큰 시퀀스를 훈련할 수 있는 능력은 완전히 새로운 AI 애플리케이션 카테고리를 엽니다.

**잠재적 우려사항:**

- 하드웨어 요구사항이 상당하여 채택을 제한할 수 있음
- 수학적 제약이 일부 사용 사례에 제한적일 수 있음
- 장기적으로 attention 복잡성에 대한 더 근본적인 접근이 필요할 수 있음

**미래 방향:** 이 작업은 다음과 같은 연구를 촉진할 가능성이 높습니다:

- 초장 시퀀스를 위한 더 효율적인 attention 메커니즘
- 시퀀스 병렬성을 위한 더 나은 하드웨어-소프트웨어 공동 설계
- 백만 토큰 컨텍스트 능력을 완전히 활용하는 애플리케이션

DeepSpeed-Ulysses는 긴 시퀀스 훈련을 "불가능"에서 "실용적"으로 성공적으로 변화시켰으며, 이는 진정으로 영향력 있는 시스템 연구의 특징입니다.

---