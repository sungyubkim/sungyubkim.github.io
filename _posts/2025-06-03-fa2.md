---
title: "Flash Attention 2"
date: 2025-06-03
last_modified_at: 2025-06-03
layout: post
permalink: /blog/fa2/
description: "FlashAttention-2 doubles training speed through better work partitioning, reduced non-matmul FLOPs, and improved parallelism."
tags: attention flash-attention memory-efficiency inference hardware-optimization performance-scaling
related_posts: true
disqus_comments: true
giscus_comments: false
toc:
  sidebar: left
---

# TL;DR

> **이 논문은 무엇에 관한 것인가?** FlashAttention-2는 AI 모델(특히 ChatGPT와 같은 대형 언어 모델)을 **2배 더 빠르게** 훈련하고 실행하면서 **메모리 사용량을 획기적으로 줄이는** 돌파구 알고리즘입니다. 마치 거대한 도서관에서 책을 찾는 더 효율적인 방법을 찾는 것과 같습니다 - 모든 책을 개별적으로 확인하는 대신, 작업 메모리에 맞는 스마트한 청크로 검색을 구성하는 방식입니다.
> 
> **핵심 기여점:**
> 
> 1. **성능 향상**: 이전 FlashAttention보다 2배, 표준 방법보다 최대 9배 빠름
> 2. **하드웨어 최적화**: GPU 이론적 최대치의 73% 달성 (이전 25-40%에서 향상)
> 3. **메모리 효율성**: 긴 시퀀스에서 42배 메모리 사용량 감소 가능
> 4. **경제적 영향**: 대형 모델 훈련 비용을 90% 절감 (GPT-3 규모의 경우 460만 달러에서 45만 달러로)
> 5. **세 가지 스마트 최적화**:
>     - 비용이 많이 드는 비행렬 연산 감소
>     - GPU 코어 전반의 향상된 병렬화
>     - 프로세서 간 낭비적인 메모리 통신 제거
> 
> **이것이 중요한 이유:** 이 작업은 계산 비용과 메모리 요구사항을 획기적으로 줄여 더 많은 연구자와 회사가 고급 AI에 접근할 수 있게 만듭니다. 마차에서 스포츠카로 업그레이드하는 것과 같습니다 - 같은 목적지이지만 훨씬 더 효율적입니다.

- [Paper Link](https://arxiv.org/pdf/2307.08691)

---

# Related Papers

**FlashAttention 시리즈:**
- [FlashAttention](/blog/fa/) - 원조 IO-aware 어텐션 알고리즘과 타일링 기법의 기초
- [FlashAttention-3](/blog/fa3/) - 비대칭 어텐션과 FP8 지원으로 한층 발전된 버전
- [From Online Softmax to FlashAttention](https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf) - 온라인 소프트맥스의 이론적 기초와 수치 안정성

**하드웨어 최적화 및 시스템 연구:**
- [Reducing Activation Recomputation in Large Transformer Models](/blog/sp/) - 메모리 효율적인 병렬 훈련 기법
- [USP](/blog/usp/) - 통합 시퀀스 병렬화 프레임워크와의 상호작용
- [Tensor Parallelism](/blog/tp/) - 텐서 병렬화와 FlashAttention의 결합 전략
- [GPipe](/blog/pp/)  - 파이프라인 병렬화와의 통합 방법론

---

# Takeaways

### 1. 동기: 왜 이 연구가 필요했는가?

**핵심 문제: 어텐션은 비싸다**

방대한 도서관에서 모든 책 쌍 사이의 연결을 찾아야 하는 사서라고 상상해보세요. 1,000권의 책이 있다면 1,000,000번의 비교(1,000²)가 필요합니다. 10,000권이라면 100,000,000번의 비교가 필요합니다. 이것이 바로 AI 어텐션 메커니즘에서 일어나는 일입니다 - 계산 비용이 입력 길이의 제곱에 비례하여 증가합니다.

**FlashAttention-2 이전의 실제 영향:**

- **메모리 위기**: 4K 단어 시퀀스의 표준 어텐션에는 67GB 메모리가 필요하지만, 대부분의 GPU는 총 40-80GB만 가지고 있음
- **속도 병목**: 어텐션 연산은 GPU 능력의 25-40%만 활용하고 있었음
- **경제적 장벽**: 대형 모델 훈련에 수백만 달러가 소요되어 AI 연구를 거대 기술 기업으로 제한

**저자들의 핵심 통찰:** 이미 주요 돌파구였던 FlashAttention-1조차도 현대 GPU 아키텍처를 완전히 활용하지 못하고 있어 여전히 비효율적이라는 것을 깨달았습니다. 포뮬러 1 자동차를 가지고 있지만 시내 교통에서만 사용하는 것과 같습니다 - 전체 잠재력을 활용하지 못하고 있었습니다.

### 2. 기술적 분석: 병목 현상 이해하기

**프로파일링으로 드러난 세 가지 핵심 문제:**

1. **비행렬 연산이 16배 더 비싸다**
    
    - 현대 GPU는 행렬 곱셈에 최적화된 특수 "텐서 코어"를 가지고 있음
    - A100 GPU: 행렬 연산 312 TFLOPs/s vs 기타 연산 19.5 TFLOPs/s
    - FlashAttention-1은 비싼 비행렬 연산을 너무 많이 수행하고 있었음
2. **긴 시퀀스에 대한 GPU 활용도 부족**
    
    - GPU는 108개의 스트리밍 멀티프로세서가 모두 바쁠 때 최고 성능 발휘
    - 긴 시퀀스는 종종 작은 배치 크기를 의미하여 많은 프로세서가 유휴 상태
    - 108차선 고속도로가 있지만 16차선만 사용하는 것과 같음
3. **낭비적인 메모리 통신**
    
    - GPU 프로세서들이 느린 공유 메모리를 통해 끊임없이 서로 통신
    - 작업자들이 정보를 공유하기 위해 공장 바닥을 계속 걸어다니는 것과 같음

### 3. 핵심 가정과 성공 조건

**FlashAttention-2가 작동하기 위한 중요한 요구사항:**

**하드웨어 가정:**

- **현대 GPU 아키텍처**: 텐서 코어가 있는 Ampere/Hopper GPU(A100, H100) 필요
- **충분한 고속 메모리**: 적절한 SRAM 필요 (A100에서 스트리밍 멀티프로세서당 192KB)
- **높은 메모리 대역폭**: 최적 성능을 위해 >1TB/s 메모리 대역폭 필요

**알고리즘 가정:**

- **블록 친화적 시퀀스**: 시퀀스 길이가 블록 크기(64, 128, 256)로 나누어떨어질 때 최고 성능
- **수치 정밀도**: fp16/bf16 정밀도 필요 - fp32로는 효율적으로 작동하지 않음
- **메모리 계층 인식**: 어텐션 블록을 고속 SRAM에 맞출 수 있어야 함

**모델 제약:**

- **헤드 차원 제한**: 최대 256 헤드 차원 지원 (FlashAttention-1의 128에서 확장)
- **시퀀스 길이**: 이론적 제한 없음, 하지만 사용 가능한 메모리에 따른 실용적 제한

**내 분석:** 이러한 가정들은 FlashAttention-2를 매우 강력하게 만들지만 동시에 어느 정도 제한적이기도 합니다. 현대 하드웨어에 고도로 최적화되어 있지만 구형 GPU나 다른 아키텍처에서는 이점을 제공하지 않을 것입니다. 이는 시스템 연구에서 흔한 트레이드오프입니다 - 특정 하드웨어에 특화함으로써 성능을 얻는 것입니다.

### 4. FlashAttention-2 방법: 실제 작동 원리

**개요: 세 가지 최적화 전략**

FlashAttention-2를 식당 주방 최적화로 생각해보세요:

1. **준비 작업 줄이기** (비행렬 연산 감소)
2. **더 나은 직원 배치** (향상된 병렬화)
3. **주방 간 소통 제거** (최적화된 작업 분할)

#### 최적화 1: 비행렬 연산 감소

**문제:**

```python
# FlashAttention-1: 비싼 재스케일링 연산들
def online_softmax_v1(scores, prev_max, prev_sum):
    """많은 비싼 연산이 있는 원래 버전"""
    new_max = torch.maximum(prev_max, torch.max(scores, dim=-1, keepdim=True)[0])
    
    # ❌ 여러 개의 비싼 재스케일링 연산들
    exp_prev = torch.exp(prev_max - new_max)  # 비싸다!
    prev_sum_rescaled = exp_prev * prev_sum   # 비싸다!
    exp_curr = torch.exp(scores - new_max)    # 비싸다!
    curr_sum = torch.sum(exp_curr, dim=-1, keepdim=True)  # 비싸다!
    
    # ❌ 더 많은 비싼 연산들
    new_sum = prev_sum_rescaled + curr_sum
    return exp_curr, new_max, new_sum
```

**해결책:**

```python
# FlashAttention-2: 연산 감소로 최적화된 버전
def online_softmax_v2(scores, prev_max, prev_sum):
    """전략적 연산 감소로 최적화된 버전"""
    curr_max = torch.max(scores, dim=-1, keepdim=True)[0]
    new_max = torch.maximum(prev_max, curr_max)
    
    # ✅ 스마트한 조건부 로직으로 연산 감소
    if torch.equal(new_max, curr_max):
        # 현재 최댓값이 전역 최댓값 - 이 경우에 최적화
        exp_scores = torch.exp(scores - new_max)
        exp_prev_factor = torch.exp(prev_max - new_max)
        new_sum = exp_prev_factor * prev_sum + torch.sum(exp_scores, dim=-1, keepdim=True)
    else:
        # 이전 최댓값이 전역 최댓값 - 다른 최적화
        exp_scores = torch.exp(scores - new_max)
        new_sum = prev_sum + torch.sum(exp_scores, dim=-1, keepdim=True)
    
    return exp_scores, new_max, new_sum
```

**실제 예시 영향:** 길이 2048 시퀀스의 경우, 이 최적화만으로도 비행렬 FLOPs를 전체 연산의 ~15%에서 ~8%로 줄여 ~17% 속도 향상을 제공합니다.

#### 최적화 2: 시퀀스 길이 병렬화

**문제:**

```python
# FlashAttention-1: 제한된 병렬화
def flashattention1_parallelization(batch_size, num_heads, seq_len):
    """원래 병렬화 전략"""
    # 배치와 헤드에만 병렬화
    num_thread_blocks = batch_size * num_heads
    
    # 예시: batch=2, heads=8, seq_len=4096
    # 결과: 108개의 사용 가능한 프로세서에 대해 16개의 스레드 블록만
    # GPU 활용도: 16/108 = 15% 😞
    return num_thread_blocks
```

**해결책:**

```python
# FlashAttention-2: 향상된 병렬화
def flashattention2_parallelization(batch_size, num_heads, seq_len, block_size=128):
    """시퀀스 차원이 포함된 향상된 병렬화"""
    # 배치, 헤드, 그리고 시퀀스 블록에 병렬화
    seq_blocks = (seq_len + block_size - 1) // block_size
    num_thread_blocks = batch_size * num_heads * seq_blocks
    
    # 같은 예시: batch=2, heads=8, seq_len=4096, block_size=128
    # seq_blocks = 4096/128 = 32
    # 결과: 2 * 8 * 32 = 512 스레드 블록
    # GPU 활용도: 512/108 = 474% (프로세서당 여러 블록) 🚀
    return num_thread_blocks

def parallel_attention_computation(Q, K, V):
    """시퀀스 병렬화가 실제로 작동하는 방식"""
    batch_size, seq_len, num_heads, head_dim = Q.shape
    block_size = 128
    
    # 각 스레드 블록이 다른 시퀀스 세그먼트를 처리
    results = []
    for seq_block_idx in range(0, seq_len, block_size):
        start_idx = seq_block_idx
        end_idx = min(seq_block_idx + block_size, seq_len)
        
        # 이 블록은 Q[start_idx:end_idx]를 모든 K, V에 대해 처리
        Q_block = Q[:, start_idx:end_idx, :, :]
        
        # 이 Q 블록에 대해 전체 K, V에 대한 어텐션 계산
        block_result = compute_attention_block(Q_block, K, V, start_idx)
        results.append(block_result)
    
    return torch.cat(results, dim=1)
```

**구체적인 예시:**

- **시나리오**: batch_size=2, 8개 어텐션 헤드, sequence length=4096로 모델 훈련
- **FlashAttention-1**: 16개 스레드 블록 → 15% GPU 활용도
- **FlashAttention-2**: 512개 스레드 블록 → 거의 100% GPU 활용도
- **결과**: 더 나은 하드웨어 활용으로 ~29% 속도 향상

#### 최적화 3: 워프 통신 제거

**문제 - "Sliced-K" 접근법:**

```python
# FlashAttention-1: 비효율적인 워프 통신
def flashattention1_warp_work(Q_block, K_block, V_block, num_warps=4):
    """원래 접근법: K와 V를 워프에 분산"""
    head_dim = K_block.size(-1)
    warp_size = head_dim // num_warps  # 각 워프가 차원의 1/4 담당
    
    warp_results = []
    for warp_id in range(num_warps):
        start_dim = warp_id * warp_size
        end_dim = (warp_id + 1) * warp_size
        
        # 각 워프가 K, V의 슬라이스로 계산
        K_warp = K_block[:, :, :, start_dim:end_dim]  # 워프가 슬라이스 담당
        V_warp = V_block[:, :, :, start_dim:end_dim]  # 워프가 슬라이스 담당
        
        # 부분 결과 계산
        scores_warp = torch.matmul(Q_block, K_warp.transpose(-2, -1))  # 모든 Q × K 슬라이스
        attn_warp = torch.softmax(scores_warp, dim=-1)
        output_warp = torch.matmul(attn_warp, V_warp)
        
        warp_results.append(output_warp)
    
    # ❌ 비용이 많이 듦: 모든 워프가 통신하고 동기화해야 함
    synchronize_warps()  # 비싼 배리어
    final_result = reduce_across_warps(warp_results)  # 비싼 리덕션
    
    return final_result
```

**해결책 - "Sliced-Q" 접근법:**

```python
# FlashAttention-2: 효율적인 독립적 워프 작업
def flashattention2_warp_work(Q_block, K_block, V_block, num_warps=4):
    """새로운 접근법: Q를 워프에 분산"""
    seq_len = Q_block.size(-2)
    warp_size = seq_len // num_warps  # 각 워프가 시퀀스의 1/4 담당
    
    warp_results = []
    for warp_id in range(num_warps):
        start_seq = warp_id * warp_size
        end_seq = (warp_id + 1) * warp_size
        
        # 각 워프가 Q의 슬라이스로 계산
        Q_warp = Q_block[:, start_seq:end_seq, :, :]  # 워프가 Q 슬라이스 담당
        # K와 V는 공유됨 (읽기 전용) - 통신 불필요!
        
        # 독립적인 계산 - 동기화 불필요
        scores_warp = torch.matmul(Q_warp, K_block.transpose(-2, -1))  # Q 슬라이스 × 모든 K
        attn_warp = torch.softmax(scores_warp, dim=-1)
        output_warp = torch.matmul(attn_warp, V_block)  # 어텐션 × 모든 V
        
        warp_results.append(output_warp)
    
    # ✅ 동기화 없음: 독립적인 결과들을 단순히 연결
    final_result = torch.cat(warp_results, dim=-2)  # 간단한 연결
    
    return final_result
```

**왜 이것이 작동하는가 - 식당 비유:**

- **FlashAttention-1 (Sliced-K)**: 4명의 요리사가 각각 다른 재료를 담당, 끊임없이 조율해야 함
    
    - 요리사 1은 야채, 요리사 2는 고기 등을 담당
    - 끊임없이 소통해야 함: "야채 준비 끝났나?" "고기 얼마나 준비됐나?"
    - 많은 대기와 조율 오버헤드
- **FlashAttention-2 (Sliced-Q)**: 4명의 요리사가 각각 다른 테이블을 담당, 모든 재료 공유
    
    - 요리사 1은 테이블 1-10, 요리사 2는 테이블 11-20 등을 담당
    - 모든 재료(K, V)가 모든 사람에게 사용 가능
    - 조율 불필요 - 각 요리사가 독립적으로 작업

**성능 영향:** 이 최적화는 공유 메모리 동기화 오버헤드를 제거하여 ~28% 속도 향상을 제공합니다.

### 5. 실험 결과: 성능 향상 증명

#### 주요 성능 결과

|**지표**|**PyTorch 표준**|**FlashAttention-1**|**FlashAttention-2**|**개선 정도**|
|---|---|---|---|---|
|**속도 (TFLOPs/s)**|30|120|**230**|**PyTorch 대비 7.7배, FA-1 대비 1.9배**|
|**이론적 최댓값 %**|10%|38%|**73%**|**GPU 활용도 7.3배 향상**|
|**4K 시퀀스 메모리**|67 GB|3.2 GB|**3.2 GB**|**21배 메모리 감소**|
|**훈련 비용 (GPT-3)**|460만 달러|230만 달러|**45만 8천 달러**|**90% 비용 절감**|

**이 결과들에 대한 내 분석:**

**정말 인상적인 부분들:**

1. **GPU 활용도 점프**: 38%에서 73%로 이론적 최댓값에 도달하는 것은 놀랍습니다. 대부분의 최적화는 10-20% 개선을 제공하지만, 하드웨어 효율성을 거의 두 배로 높이는 것은 뛰어납니다.
    
2. **메모리 효율성 복합 효과**: 21배 메모리 감소는 단순히 RAM을 적게 사용하는 것이 아닙니다 - 완전히 새로운 애플리케이션을 가능하게 합니다. 이제 단일 GPU에서 이전에는 불가능했던 모델을 훈련할 수 있습니다.
    
3. **경제적 변혁**: 90% 비용 절감은 단순한 최적화가 아닙니다 - AI 연구를 민주화하는 패러다임 전환입니다.
    

#### 절제 연구: 각 최적화가 기여하는 바 이해하기

|**구성**|**TFLOPs/s**|**개선**|**핵심 이점**|
|---|---|---|---|
|**베이스라인 (FlashAttention-1)**|120|-|시작점|
|**+ 비행렬 연산 감소**|140|+17%|더 나은 텐서 코어 사용|
|**+ 시퀀스 병렬화**|180|+29%|더 높은 GPU 점유율|
|**+ 워프 최적화**|**230**|+28%|동기화 제거|
|**모든 것 결합**|**230**|+92%|**곱셈 효과**|

**내 심층 분석:**

**각 최적화가 중요한 이유:**

1. **비행렬 연산 감소 (+17%)**:
    
    - 작아 보이지만 실제로는 기초적임
    - 현대 GPU는 행렬 곱셈을 중심으로 설계됨 - 나머지는 부차적
    - 비싼 연산을 ~50% 줄임으로써 GPU의 진정한 잠재력을 해방
    - 비포장도로에서 고속도로로 바꾸는 것과 같음 - 기본 경로가 이제 최적화됨
2. **시퀀스 병렬화 (+29%)**:
    
    - 리소스 활용에 관한 것 - 고전적인 시스템 최적화
    - 이전: GPU 코어의 85%가 긴 시퀀스에 대해 유휴 상태
    - 이후: 거의 100% 활용
    - 통찰: 어텐션이 본질적으로 순차적으로 보여도 일부분을 병렬화할 수 없다는 뜻은 아님
3. **워프 최적화 (+28%)**:
    
    - 가장 미묘하지만 아마도 가장 중요한 최적화
    - 깊은 하드웨어 아키텍처를 이해하는 것
    - 통찰: 공유 메모리 통신은 숨겨진 병목
    - 동기화를 제거함으로써 근본적인 제약을 없앰

**곱셈 효과:** 17% + 29% + 28% = 74%이지만 실제 개선은 92%입니다. 이런 초선형 개선이 일어나는 이유:

- 각 최적화가 다른 것들이 더 잘 작동하게 만듦
- 하나의 병목을 제거하면 종종 다른 병목들도 드러나고 제거됨
- 하드웨어 최적화는 단순히 더하는 것이 아니라 복합됨

#### 순방향 vs 역방향 패스 분석

|**패스**|**방법**|**TFLOPs/s**|**이론적 %**|**내 분석**|
|---|---|---|---|---|
|**순방향**|PyTorch|60|19%|메모리 제한적, 비효율적|
|**순방향**|FlashAttention-2|**187**|**60%**|**3.1배 개선**|
|**역방향**|PyTorch|45|14%|더욱 도전적|
|**역방향**|FlashAttention-2|**165**|**53%**|**3.7배 개선**|

**역방향 패스 개선이 더 극적인 이유:**

역방향 패스가 더 복잡한 이유:

1. **더 많은 행렬 연산**: 순방향 패스의 2개 vs 5개 행렬 곱셈
2. **더 많은 메모리 압박**: 중간값을 저장/재계산해야 함
3. **복잡한 의존성**: 기울기 계산에 더 복잡한 데이터 의존성

FlashAttention-2의 최적화가 역방향 패스에서 더 도움이 되는 이유:

- **메모리 효율성**: 메모리가 부족할 때 타일링과 재계산 전략이 빛남
- **병렬화**: 더 많은 계산은 더 많은 병렬 실행 기회를 의미
- **통신 감소**: 복잡한 의존성으로 인해 통신 제거가 더 가치 있음

### 6. 실제 영향과 중요성

#### 훈련 시간 혁명

**GPT-3 175B 훈련 비교:**

|**방법**|**시간**|**비용**|**접근성**|
|---|---|---|---|
|**표준**|~200일|460만 달러|거대 기술 기업만|
|**FlashAttention-1**|~100일|230만 달러|대기업|
|**FlashAttention-2**|**10일**|**45만 8천 달러**|**많은 연구소**|

**더 넓은 함의에 대한 내 평가:**

**연구 민주화:**

- 이전: Google, OpenAI, Microsoft만이 대형 모델 훈련 가능
- 이후: 중견 기업과 자금이 충분한 연구소들이 참여 가능
- 참여자 풀을 확장하여 AI 연구를 10배 가속화

**개발 사이클 가속화:**

- 200일 훈련 사이클 vs 10일 사이클로 빠른 반복 가능
- 연구자들이 같은 시간에 20배 더 많은 실험 시도 가능
- 복합 효과: 더 빠른 실험 → 더 나은 통찰 → 더 나은 모델

**환경 영향:**

- 90% 비용 절감은 종종 ~90% 에너지 절감으로 이어짐
- FlashAttention-2로 GPT-3 훈련은 ~10배 적은 전력 사용
- 대규모 AI 연구를 더 지속 가능하게 만듦

### 7. 비판적 평가와 제한사항

**FlashAttention-2를 성공적으로 만드는 요소들:**

1. **하드웨어-소프트웨어 공동 설계**: GPU 아키텍처에 대한 깊은 이해
2. **체계적 최적화**: 하나의 큰 변화보다는 세 가지 상호보완적 개선
3. **실용적 영향**: 많은 연구자들이 직면하는 실제 문제 해결
4. **엄격한 평가**: 여러 차원에 걸친 포괄적 벤치마크

**제한사항과 제약사항:**

1. **하드웨어 의존성**: 현대 GPU(A100, H100)에서만 작동
    
    - **내 견해**: 이는 장점이자 단점 - 고성능을 위해서는 특화가 필요
2. **복잡성**: 표준 어텐션보다 훨씬 복잡
    
    - **내 견해**: 복잡성은 성능 향상으로 정당화되지만 유지보수 우려 제기
3. **정밀도 요구사항**: fp16/bf16 필요, fp32로는 효율적으로 작동하지 않음
    
    - **내 견해**: 수치 정밀도를 제한하지만 텐서 코어 활용을 위해 필요
4. **블록 크기 민감성**: 성능이 좋은 블록 크기 선택에 의존
    
    - **내 견해**: 일부 튜닝 전문성이 필요하지만 기본값이 잘 작동

### 8. 왜 이 논문이 AI 진보에 중요한가

**기술적 우수성:** FlashAttention-2는 최고의 시스템 연구를 대표합니다 - 깊은 하드웨어 이해와 알고리즘 혁신을 결합하여 실용적 문제를 해결합니다.

**더 넓은 영향:** 이 작업은 인프라 개선이 알고리즘 돌파구만큼 중요할 수 있음을 보여줍니다. 때로는 더 나은 AI로 가는 길이 새로운 모델 아키텍처가 아니라 기존 아키텍처를 훨씬 더 효율적으로 실행하게 만드는 것입니다.

**연구 철학:** 이 논문은 다음의 가치를 보여줍니다:

- 최적화하기 전에 병목 현상을 프로파일링하고 이해하기
- 임시방편적 개선보다는 체계적 최적화
- 여러 차원에 걸친 엄격한 벤치마킹
- 계산적 영향과 경제적 영향 모두 고려

**개인적 성찰:** FlashAttention-2에서 가장 인상적인 것은 이론적 돌파구가 아닌 엔지니어링 우수성을 통해 불가능한 것을 가능하게 만든다는 점입니다. AI에서 인프라 작업이 알고리즘 혁신만큼 변혁적일 수 있다는 것을 상기시켜줍니다. 대형 모델 훈련의 90% 비용 절감은 단순한 숫자가 아닙니다 - 전체 분야의 진보를 가속화할 AI 연구의 민주화입니다.

이 논문은 또한 하드웨어를 깊이 이해하는 것의 중요성을 보여줍니다. 저자들은 단순히 코드를 프로파일링한 것이 아니라 하드웨어 제약에 맞게 알고리즘을 재설계할 만큼 GPU 아키텍처를 충분히 이해했습니다. AI 워크로드가 증가하고 에너지 효율성이 중요해지면서 이는 점점 더 중요해지고 있습니다.

내 의견으로는, FlashAttention-2는 오늘날 우리가 보고 있는 긴 컨텍스트 AI 혁명의 핵심 촉진자 중 하나로 기억될 것입니다. 100K+ 컨텍스트 윈도우를 가진 모델들이 실용적이 된 것은 주로 기본 계산을 실현 가능하게 만든 이와 같은 작업 때문입니다.

---