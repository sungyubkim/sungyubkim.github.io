---
title: "Paper Review: Mixture of Experts"
date: 2025-05-31
last_modified_at: 2025-05-31
type: posts
layout: single
author_profile: false
comments: true
permalink: /blog/moe/
tags:
  - LLM
  - MoE
  - Parallelism
  - Architecture
use_math: true
---

# TL;DR

> 이 2017년 기념비적 논문은 **희소 게이팅 전문가 혼합(Sparsely-Gated Mixture-of-Experts, MoE)**을 도입하여 신경망 확장의 근본적 과제를 해결했습니다. 훈련 가능한 게이팅 네트워크가 전문가 서브네트워크를 희소하게 활성화하는 방법으로 1000배 이상의 매개변수 증가를 최소한의 계산 오버헤드로 달성합니다. 정교한 부하 분산과 분산 훈련 솔루션이 결합되었습니다. 언어 모델링(24-39% 향상된 perplexity)과 기계 번역(+1-1.3 BLEU)에서 상당한 개선을 보여주지만, 효율성 주장은 과장되었고 상당한 인프라 복잡성을 수반합니다. 이 연구의 지속적인 영향은 조건부 계산이 대규모로 작동할 수 있음을 증명하여 오늘날의 최대 언어 모델을 구동하는 희소 신경 아키텍처의 현대적 시대를 열었다는 것입니다.

- Paper Link: [https://arxiv.org/pdf/1701.06538](https://arxiv.org/pdf/1701.06538)

---

# Related Papers

**전문가 혼합 아키텍처:**
- [Switch Transformers](https://arxiv.org/pdf/2101.03961) - 단순화된 MoE 라우팅으로 조 단위 매개변수 달성
- [GPipe](../pp) - MoE와 결합 가능한 파이프라인 병렬화

**분산 훈련:**
- [Tensor Parallelism](../tp) - MoE와 호환되는 텐서 병렬화 기법
- [Reducing Activation Recomputation in Large Transformer Models](https://arxiv.org/pdf/2205.05198) - 대규모 MoE 훈련을 위한 메모리 최적화

**확장성 및 효율성:**
- [Efficiently Scaling Transformer Inference](https://arxiv.org/pdf/2211.05102) - MoE 모델의 효율적 추론
- [USP](https://arxiv.org/pdf/2405.07719) - MoE와 결합 가능한 시퀀스 병렬화

---

# Takeaways

## 문제: 신경망 확장이 벽에 부딪히다

신경망은 간단한 원리를 따릅니다: 더 많은 매개변수 = 더 나은 성능. 하지만 전통적인 확장은 이차적 비용 증가로 이어집니다. 모델 크기를 두 배로 늘리고 훈련 데이터를 두 배로 늘리면 계산 비용이 4배 증가합니다. 이는 빠르게 지속 불가능해집니다.

**조건부 계산**은 해결책을 약속했습니다 - 서로 다른 입력에 대해 네트워크의 다른 부분을 활성화하여 계산량은 일정하게 유지하면서 용량을 대폭 증가시키는 것입니다. 하지만 수십 년의 이론적 연구에도 불구하고, 근본적인 과제들로 인해 아무도 대규모로 작동시키지 못했습니다:

```python
# 핵심 확장 문제
전통적_비용 = 모델_크기 * 훈련_데이터 * 매개변수당_계산량
# 모델_크기와 훈련_데이터가 모두 증가하면 비용이 이차적으로 폭증
```

**주요 장벽들:**
- **GPU 비효율성**: GPU는 분기가 아닌 밀집 계산에 최적화됨
- **배치 크기 감소**: 조건부 활성화가 각 구성 요소의 효과적 배치 크기를 줄임
- **네트워크 대역폭**: 매개변수가 분산될 때 통신 비용이 지배적
- **부하 분산**: 모델이 몇 개의 "선호하는" 구성 요소만 사용하여 용량을 낭비하는 경향

## 해결책: 희소 게이팅 전문가 혼합

저자들의 돌파구는 이 모든 과제를 동시에 해결하는 완전한 시스템을 만든 것이었습니다. 핵심 혁신은 신경망 어디에나 임베드될 수 있는 **MoE 층**입니다:

```python
class SparseMoELayer:
    def __init__(self, d_model, num_experts=1000, k=4):
        # 게이팅 네트워크가 어떤 전문가를 사용할지 결정
        self.gating_network = GatingNetwork(d_model, num_experts)
        
        # 수천 개의 전문가 서브네트워크
        self.experts = [FeedForward(d_model) for _ in range(num_experts)]
        
        # 입력당 상위 k개 전문가만 활성화
        self.k = k
        
    def forward(self, x):
        # 1. 게이팅 네트워크가 입력당 k개 전문가 선택
        gate_probs, selected_experts = self.gating_network(x, k=self.k)
        
        # 2. 선택된 전문가만 계산 (핵심 효율성 이득)
        expert_outputs = self.compute_selected_experts(x, selected_experts)
        
        # 3. 전문가 출력의 가중 조합
        output = self.combine_outputs(expert_outputs, gate_probs)
        
        return output

# 예시: 1000개 전문가와 k=4로 1000배 용량을 얻지만
# 입력당 계산량은 4배만 증가
```

## 성공을 가능하게 한 기술적 혁신들

### 1. 노이즈 Top-K 게이팅: 희소하지만 미분 가능

게이팅 네트워크는 시스템의 핵심입니다. 훈련 가능성을 유지하면서 어떤 전문가를 사용할지 선택해야 합니다:

```python
def noisy_top_k_gating(x, w_gate, w_noise, k, training=True):
    # 기본 게이팅 점수
    gate_logits = x @ w_gate  # [batch, num_experts]
    
    if training:
        # 부하 분산을 위한 노이즈 추가 (중요한 혁신!)
        noise = torch.randn_like(gate_logits) * softplus(x @ w_noise)
        gate_logits = gate_logits + noise
    
    # 상위 k개 전문가 선택
    top_k_logits, top_k_indices = torch.topk(gate_logits, k)
    
    # 희소 마스크 생성: 나머지를 음의 무한대로 설정
    sparse_logits = torch.full_like(gate_logits, float('-inf'))
    sparse_logits.scatter_(1, top_k_indices, top_k_logits)
    
    # 소프트맥스가 최종 전문가 가중치 제공
    gate_probs = torch.softmax(sparse_logits, dim=-1)
    
    return gate_probs, top_k_indices

# 핵심 통찰: 노이즈는 단순한 탐색이 아니라 
# 부하 분산을 위한 이산적 전문가 선택을 미분 가능하게 만듦
```

### 2. 하이브리드 분산 훈련: 배치 크기 문제 해결

전통적인 데이터 병렬처리는 각 전문가에게 작은 배치를 줍니다. 해결책은 데이터와 모델 병렬처리를 결합하는 것입니다:

```python
# 대신에: 각 디바이스가 batch_size/num_devices 예제를 받음
# 이렇게 하기: 전문가 계산을 위해 디바이스 간 배치 결합

def distributed_moe_forward(inputs_per_device, expert_assignments):
    # 1. 각 디바이스가 독립적으로 게이팅 계산 (데이터 병렬)
    local_gates = [compute_gates(inp) for inp in inputs_per_device]
    
    # 2. 각 전문가가 필요한 모든 입력 수집 (모델 병렬)
    expert_batches = {}
    for device_id, (inputs, gates) in enumerate(zip(inputs_per_device, local_gates)):
        for expert_id in range(num_experts):
            selected_inputs = inputs[gates.selected_experts == expert_id]
            if expert_id not in expert_batches:
                expert_batches[expert_id] = []
            expert_batches[expert_id].append(selected_inputs)
    
    # 3. 각 전문가가 모든 디바이스의 결합된 배치 처리
    expert_outputs = {}
    for expert_id, batched_inputs in expert_batches.items():
        combined_batch = torch.cat(batched_inputs, dim=0)
        expert_outputs[expert_id] = experts[expert_id](combined_batch)
    
    # 4. 결과를 디바이스로 다시 분산
    return scatter_outputs_to_devices(expert_outputs)

# 결과: 전문가 배치 크기 = k * 총_배치_크기 / 전문가_수
# d개 디바이스로: k * d * 로컬_배치_크기 / 전문가_수
```

### 3. 부하 분산: 전문가 붕괴 방지

개입 없이는 모델이 몇 개의 전문가만 사용하도록 학습하여 용량을 낭비합니다:

```python
def compute_load_balancing_losses(gate_probs, x):
    # 손실 1: 동등한 중요도 장려 (게이트 가중치 합계)
    importance = gate_probs.sum(dim=0)  # 배치에 대한 합계
    importance_loss = coefficient_of_variation(importance) ** 2
    
    # 손실 2: 동등한 부하 장려 (전문가당 예제 수)
    # 전문가 선택이 이산적이므로 더 복잡함
    load_estimator = estimate_load_differentiably(gate_probs, x)
    load_loss = coefficient_of_variation(load_estimator) ** 2
    
    return importance_loss, load_loss

def estimate_load_differentiably(gate_probs, x):
    # 이산적 전문가 할당의 부드러운 추정기
    # 게이팅의 노이즈를 사용하여 이를 미분 가능하게 만듦
    load_probs = torch.zeros_like(gate_probs)
    
    for expert_idx in range(num_experts):
        # 각 예제에 대해 이 전문가가 선택될 확률
        # 현재 게이트 값과 노이즈 분포에 기반
        other_experts = gate_probs[:, others]
        kth_largest_other = torch.kthvalue(other_experts, k-1)[0]
        
        # k번째로 큰 경쟁자를 이길 확률
        prob_selected = torch.sigmoid(
            (gate_probs[:, expert_idx] - kth_largest_other) / noise_std
        )
        load_probs[:, expert_idx] = prob_selected
    
    return load_probs.sum(dim=0)  # 전문가당 예상 부하
```

## 실험 결과: 인상적이지만 복잡함

### 주요 성능 결과

| 작업 | 기준 모델 | MoE 모델 | 개선 | 숨겨진 비용 |
|------|-----------|----------|------|-------------|
| **1B 단어 LM** | 45.0 perplexity (9.4M 매개변수) | **34.1 perplexity** (4.3B 매개변수) | **24% 향상** | 430배 더 많은 매개변수 |
| **100B 단어 LM** | 47.0 perplexity (8.4M 매개변수) | **28.9 perplexity** (68.8B 매개변수) | **39% 향상** | 8200배 더 많은 매개변수 |
| **WMT'14 En→Fr** | 39.22 BLEU (278M 매개변수) | **40.56 BLEU** (8.7B 매개변수) | **+1.34 BLEU** | 31배 더 많은 매개변수 |
| **WMT'14 En→De** | 24.91 BLEU (278M 매개변수) | **26.03 BLEU** (8.7B 매개변수) | **+1.12 BLEU** | 31배 더 많은 매개변수 |

**핵심 통찰**: 개선은 실제적이고 의미 있지만, 대규모 매개변수 증가와 함께 옵니다. "동일한 계산, 더 나은 결과"가 아니라 "대폭 늘어난 매개변수, 더 나은 결과"입니다.

### 부하 분산 절제 연구

| 구성 | 테스트 Perplexity | 전문가 균형 | 부하 균형 | 해석 |
|------|------------------|-------------|-----------|------|
| **부하 손실 없음** | 39.8 | 매우 나쁨 (CV=3.04) | 매우 나쁨 (17.8배 불균형) | **완전한 실패** - 모델이 ~3개 전문가 사용 |
| **중요도 손실만** | 35.6 | 좋음 (CV=0.06) | 나쁨 (1.47배 불균형) | 게이트 가중치는 균형 맞지만 실제 부하는 안 맞음 |
| **부하 손실만** | 35.7 | 보통 (CV=0.22) | 좋음 (1.15배 불균형) | 분산 효율성에 더 좋음 |
| **두 손실 모두** | 35.6 | 좋음 (CV=0.06) | 좋음 (1.14배 불균형) | **전체적으로 최고** - 두 측면 모두 균형 |

**중요한 발견**: 두 손실 함수 모두 필수적입니다. 이것들 없이는 모델이 완전히 실패합니다 (39.8 vs 35.6 perplexity).

### 계산 효율성 현실 점검

| 모델 유형 | 주장된 이득 | 측정된 TFLOPS/GPU | 현실 |
|-----------|-------------|-------------------|------|
| **밀집 기준선** | - | 1.07-1.29 | 깔끔한 기준선 |
| **희소 MoE (낮은 계산)** | "동일한 계산" | **0.74-0.90** | **실제로는 덜 효율적!** |
| **희소 MoE (높은 계산)** | "더 나은 효율성" | 1.56 | 더 나음, 하지만 더 큰 행렬이 도움 |

**현실 점검**: 효율성 주장은 의문스럽습니다. 낮은 계산 MoE 모델들은 실제로 밀집 기준선보다 덜 효율적이며, 아마도 분기 오버헤드와 통신 비용 때문일 것입니다.

## 비판적 평가: 주의사항이 있는 돌파구

### 진정한 돌파구들
1. **조건부 계산이 대규모로 작동함을 증명** - 대규모 희소 네트워크의 첫 번째 성공적 실증
2. **실용적 과제를 위한 엔지니어링 솔루션** - 부하 분산, 분산 훈련, GPU 효율성
3. **여러 작업에서 일관된 개선** - 언어 모델링, 기계 번역, 다국어 설정
4. **전문가 특수화가 자연스럽게 나타남** - 모델이 구문적, 의미적 특수화를 학습

### 상당한 한계들
1. **불공정한 매개변수 비교** - 430배-8200배 매개변수 증가가 "효율성" 주장을 오해의 소지가 있게 만듦
2. **인프라 복잡성이 숨겨짐** - 메모리 요구사항, 분산 설정 비용이 충분히 고려되지 않음
3. **하드웨어 비교 문제** - 다른 GPU 세대들이 훈련 시간 비교를 무효화함
4. **통계적 엄밀성 부족** - 오차 막대, 유의성 검정, 또는 여러 무작위 시드 없음

### 결과가 실제로 보여주는 것
```python
# 그들이 주장하는 것:
"최소한의 계산 오버헤드로 1000배 모델 용량"

# 그들이 실제로 실증하는 것:
"1000배 매개변수가 의미 있는 개선을 줄 수 있지만, 다음과 함께:
 - 상당한 메모리/저장 오버헤드
 - 복잡한 분산 훈련 요구사항  
 - 의문스러운 계산 효율성 이득
 - 신중한 부하 분산의 필요성"
```

## 실용적 시사점과 교훈

### 연구자들을 위해
1. **희소 모델이 작동할 수 있음** - 하지만 정교한 엔지니어링 솔루션이 필요
2. **부하 분산이 중요함** - 개입 없이는 모델이 몇 개 전문가 사용으로 붕괴
3. **평가에 주의가 필요함** - 매개변수 증가가 공정한 비교를 어렵게 만듦
4. **인프라가 중요함** - 분산 훈련 복잡성이 상당함

### 실무자들을 위해
1. **제약 조건 고려**:
   ```python
   if 메모리_예산_크고 and 분산_훈련_가능:
       sparse_moe_고려()
   else:
       밀집_모델이_더_나을_수도()
   ```

2. **배치 크기 요구사항**:
   ```python
   최소_배치_크기 = 전문가_수 * k / 원하는_전문가당_예제수
   # 효율성을 위해 큰 배치 필요 - 일반적으로 512+ 예제
   ```

3. **고려해야 할 숨겨진 비용들**:
   - 메모리: 대형 희소 모델의 경우 10-100배 증가
   - 저장: 모델 파일이 거대해짐
   - 통신: 전문가 라우팅이 상당한 대역폭 필요
   - 복잡성: 부하 분산, 분산 훈련 설정

## 장기적 영향과 미래 방향

이 논문은 희소 신경망의 현대적 시대를 열었습니다. 그 영향은 다음에서 볼 수 있습니다:
- **Switch Transformer** (Google, 2021) - 단순화된 MoE 설계
- **GLaM** (Google, 2021) - 64개 전문가 언어 모델  
- **PaLM** (Google, 2022) - 밀집 모델에 적용된 확장 통찰
- **GPT-4** (OpenAI, 2023) - 아마도 MoE 기법 사용 (미확인)

### 분야를 위한 핵심 교훈
1. **조건부 계산이 실행 가능함** - 하지만 신중한 엔지니어링 필요
2. **매개변수 효율성 vs 계산 효율성** - 이들은 다른 최적화 목표
3. **확장 법칙이 중요함** - 더 큰 데이터셋이 훨씬 더 큰 모델을 효과적으로 활용할 수 있음
4. **인프라 공동 설계** - 알고리즘과 시스템 엔지니어링이 함께 발전해야 함

### 열린 질문들과 미래 연구
- **더 나은 부하 분산 알고리즘** - 현재 방법들은 여전히 차선책
- **희소 어텐션 메커니즘** - 어텐션에 유사한 원리 적용
- **자동화된 전문가 특수화** - 언제 어떻게 특수화할지 학습
- **희소 모델의 효율적 추론** - 현재 방법들은 훈련에 최적화됨

## 결론

이 논문은 조건부 계산이 대규모로 작동할 수 있음을 증명한 진정한 돌파구를 나타냅니다. 효율성 주장이 과장되고 실험적 비교에 상당한 한계가 있지만, 핵심 기여 - 희소 MoE가 의미 있는 개선을 달성할 수 있음을 실증 - 는 전체 연구 방향을 출범시켰습니다.

이 연구의 지속적인 가치는 특정 효율성 수치(의문스러운)에 있지 않고 조건부 계산을 실용적으로 만드는 엔지니어링 청사진에 있습니다. 2017년 이후의 모든 주요 희소 신경망은 여기서 놓인 기초 위에 구축됩니다: 노이즈 게이팅, 부하 분산 손실, 하이브리드 분산 훈련.

오늘날 대규모 신경망을 다루는 누구에게나 이러한 기법들을 이해하는 것은 필수적입니다 - 반드시 MoE를 사용해야 하기 때문이 아니라, 희소성, 부하 분산, 확장의 원리들이 현대 딥러닝의 기본이 되었기 때문입니다.

---