<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Reducing Activation Recomputation in Large Transformer Models | Sung-Yub Kim </title> <meta name="author" content="Sung-Yub Kim"> <meta name="description" content="Techniques for reducing activation recomputation overhead in large Transformer training via sequence and selective checkpointing."> <meta name="keywords" content="machine-learning, deep-learning, llm, foundation-models, research"> <meta property="og:site_name" content="Sung-Yub Kim"> <meta property="og:type" content="article"> <meta property="og:title" content="Sung-Yub Kim | Reducing Activation Recomputation in Large Transformer Models"> <meta property="og:url" content="https://sungyubkim.github.io/blog/sp/"> <meta property="og:description" content="Techniques for reducing activation recomputation overhead in large Transformer training via sequence and selective checkpointing."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Reducing Activation Recomputation in Large Transformer Models"> <meta name="twitter:description" content="Techniques for reducing activation recomputation overhead in large Transformer training via sequence and selective checkpointing."> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Sung-Yub Kim"
        },
        "url": "https://sungyubkim.github.io/blog/sp/",
        "@type": "BlogPosting",
        "description": "Techniques for reducing activation recomputation overhead in large Transformer training via sequence and selective checkpointing.",
        "headline": "Reducing Activation Recomputation in Large Transformer Models",
        
        "sameAs": ["https://github.com/sungyubkim","https://scholar.google.com/citations?user=m2rhgrkAAAAJ","https://www.linkedin.com/in/sung-yub-kim-0a82a1264","https://twitter.com/SungyubK"],
        
        "name": "Sung-Yub Kim",
        "@context": "https://schema.org"
    }
  </script> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sungyubkim.github.io/blog/sp/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Sung-Yub</span> Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Reducing Activation Recomputation in Large Transformer Models</h1> <p class="post-meta"> Created on June 01, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/memory-efficiency"> <i class="fa-solid fa-hashtag fa-sm"></i> memory-efficiency</a>   <a href="/blog/tag/sequence-parallelism"> <i class="fa-solid fa-hashtag fa-sm"></i> sequence-parallelism</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="tldr">TL;DR</h1> <blockquote> <p><strong>문제</strong>: 대규모 트랜스포머 모델(100B+ 매개변수) 훈련 시 중대한 병목현상 발생—그래디언트 계산을 위한 활성화(activation) 저장이 엄청난 메모리를 소모하여, 비싼 “활성화 재계산(activation recomputation)”을 강요하고 이는 30-40%의 훈련 시간 오버헤드를 추가함.</p> <p><strong>해결책</strong>: 함께 5배 메모리 감소와 30% 속도 향상을 달성하는 두 가지 상호보완적 기법:</p> <ol> <li> <strong>시퀀스 병렬화(Sequence Parallelism)</strong>: 텐서 병렬화가 처리할 수 없는 연산에서 시퀀스 차원을 따라 활성화를 분할</li> <li> <strong>선택적 활성화 재계산(Selective Activation Recomputation)</strong>: 특정한 고메모리, 저계산 어텐션 연산만 재계산</li> </ol> <p><strong>영향</strong>: 2240개 A100에서 54% GPU 사용률로 1조 매개변수 모델 훈련 가능—이전에는 불가능했던 규모를 실용적으로 만듦.</p> </blockquote> <ul> <li>Paper Link: <a href="https://arxiv.org/pdf/2205.05198" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2205.05198</a> </li> </ul> <hr> <h1 id="related-papers">Related Papers</h1> <p><strong>메모리 최적화 기법:</strong></p> <ul> <li> <a href="/blog/tp/">Tensor Parallelism</a> - 텐서 병렬화와 메모리 효율성 결합</li> <li> <a href="/blog/pp/">GPipe</a> - 파이프라인 병렬화에서의 메모리 최적화</li> <li> <a href="/blog/ring-self-attention/">Ring Self-Attention</a> - 시퀀스 병렬화와 메모리 관리</li> </ul> <p><strong>대규모 모델 훈련:</strong></p> <ul> <li> <a href="https://arxiv.org/pdf/2101.03961" rel="external nofollow noopener" target="_blank">Switch Transformers</a> - 대규모 MoE 모델의 메모리 효율성</li> <li> <a href="/blog/moe/">MoE</a> - 전문가 혼합 모델의 메모리 요구사항</li> <li> <a href="https://arxiv.org/pdf/2406.18485" rel="external nofollow noopener" target="_blank">LoongTrain</a> - 긴 시퀀스 훈련에서의 메모리 최적화</li> </ul> <p><strong>분산 훈련 시스템:</strong></p> <ul> <li> <a href="/blog/deepspeed_ulysses/">DeepSpeed Ulysses</a> - 시퀀스 병렬화와 메모리 효율성</li> <li> <a href="/blog/usp/">USP</a> - 통합 병렬화 프레임워크에서의 메모리 관리</li> <li> <a href="/blog/blockwise_ringattention/">Blockwise RingAttention</a> - 어텐션 계산의 메모리 효율성</li> </ul> <hr> <h1 id="takeaways">Takeaways</h1> <h2 id="근본적-문제-트랜스포머-훈련의-메모리-벽">근본적 문제: 트랜스포머 훈련의 메모리 벽</h2> <p>메모리에 맞지 않을 정도로 큰 모델을 훈련한다고 상상해보세요. 전통적인 해결책은 <strong>활성화 재계산</strong>(그래디언트 체크포인팅)입니다—역전파를 위한 중간 계산을 저장하는 대신, 버리고 나중에 다시 계산하는 것입니다. 이는 메모리를 절약하지만 본질적으로 순전파를 두 번 실행해야 하므로 엄청난 계산 오버헤드를 추가합니다.</p> <p><strong>문제의 규모:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 530B 매개변수 모델의 경우
</span><span class="n">activation_memory</span> <span class="o">=</span> <span class="mi">34</span> <span class="o">*</span> <span class="n">seq_len</span> <span class="o">*</span> <span class="n">batch</span> <span class="o">*</span> <span class="n">hidden</span> <span class="o">+</span> <span class="mi">5</span> <span class="o">*</span> <span class="n">heads</span> <span class="o">*</span> <span class="n">seq_len</span><span class="err">²</span>
<span class="c1"># ≈ 레이어당 160GB (최적화 없이)
# vs. 80GB A100 GPU 메모리 용량
</span></code></pre></div></div> <p>이는 고통스러운 트레이드오프를 강요합니다: 비싼 재계산을 사용하거나 모델을 전혀 훈련할 수 없거나.</p> <h2 id="핵심-혁신-1-시퀀스-병렬화">핵심 혁신 1: 시퀀스 병렬화</h2> <p><strong>통찰</strong>: 텐서 병렬화는 계산 집약적인 연산(행렬 곱셈)에는 훌륭하지만 간단한 연산(레이어 정규화, 드롭아웃)은 장치 간에 복제되어 메모리를 낭비합니다.</p> <p><strong>해결책</strong>: 이러한 “간단한” 연산들은 시퀀스 차원을 따라 독립적이므로, 대신 그 차원에서 분할할 수 있습니다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 전통적인 텐서 병렬화
</span><span class="k">def</span> <span class="nf">tensor_parallel_layer</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>  <span class="c1"># x: [seq_len, batch, hidden]
</span>    <span class="c1"># 레이어 정규화가 각 장치에서 전체 데이터로 실행됨 (낭비적!)
</span>    <span class="n">x_norm</span> <span class="o">=</span> <span class="nf">layer_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># 각 랭크에서 [seq_len, batch, hidden]
</span>    
    <span class="c1"># 행렬 곱셈은 효율적으로 분할됨
</span>    <span class="n">x_split</span> <span class="o">=</span> <span class="nf">matmul_tensor_parallel</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span>  <span class="c1"># [seq_len, batch, hidden/num_ranks]
</span>    <span class="k">return</span> <span class="n">x_split</span>

<span class="c1"># 시퀀스 병렬화 적용
</span><span class="k">def</span> <span class="nf">sequence_tensor_parallel_layer</span><span class="p">(</span><span class="n">x_seq</span><span class="p">):</span>  <span class="c1"># x_seq: [seq_len/num_ranks, batch, hidden]
</span>    <span class="c1"># 시퀀스 분할 데이터에서 레이어 정규화 (메모리 효율적!)
</span>    <span class="n">x_norm</span> <span class="o">=</span> <span class="nf">layer_norm</span><span class="p">(</span><span class="n">x_seq</span><span class="p">)</span>  <span class="c1"># 랭크당 [seq_len/num_ranks, batch, hidden]
</span>    
    <span class="c1"># 필요시 텐서 병렬로 변환
</span>    <span class="n">x_full</span> <span class="o">=</span> <span class="nf">all_gather</span><span class="p">(</span><span class="n">x_norm</span><span class="p">)</span>  <span class="c1"># 각 랭크에서 [seq_len, batch, hidden]
</span>    <span class="n">x_split</span> <span class="o">=</span> <span class="nf">matmul_tensor_parallel</span><span class="p">(</span><span class="n">x_full</span><span class="p">)</span>
    
    <span class="c1"># 시퀀스 병렬로 다시 변환
</span>    <span class="k">return</span> <span class="nf">reduce_scatter</span><span class="p">(</span><span class="n">x_split</span><span class="p">)</span>  <span class="c1"># 랭크당 [seq_len/num_ranks, batch, hidden]
</span></code></pre></div></div> <p><strong>핵심 수학적 통찰</strong>: 통신 비용이 동일합니다—<code class="language-plaintext highlighter-rouge">all_reduce = reduce_scatter + all_gather</code>—따라서 대역폭 오버헤드가 없습니다.</p> <h2 id="핵심-혁신-2-선택적-활성화-재계산">핵심 혁신 2: 선택적 활성화 재계산</h2> <p><strong>통찰</strong>: 모든 연산이 동등하지 않습니다. 일부는 많은 메모리를 소모하지만 재생성하는 데 최소한의 계산만 필요합니다.</p> <p><strong>대상 연산</strong>: 어텐션에서 Q, K, V 계산 후:</p> <ul> <li>QK^T 행렬 곱셈 → 소프트맥스 → 드롭아웃 → V에 대한 어텐션</li> <li>대형 모델에서 활성화 메모리의 ~70%를 차지</li> <li>하지만 전체 계산의 ~3%만 차지 (대부분 원소별 연산)</li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">selective_attention_recomputation</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span><span class="p">):</span>
    <span class="c1"># 저장: Q, K, V (메모리 집약적 연산의 입력)
</span>    <span class="n">q_checkpoint</span> <span class="o">=</span> <span class="n">q</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    <span class="n">k_checkpoint</span> <span class="o">=</span> <span class="n">k</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span> 
    <span class="n">v_checkpoint</span> <span class="o">=</span> <span class="n">v</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># 재계산: 고메모리, 저계산 연산들
</span>    <span class="k">def</span> <span class="nf">recompute_expensive_memory_ops</span><span class="p">(</span><span class="n">q_in</span><span class="p">,</span> <span class="n">k_in</span><span class="p">,</span> <span class="n">v_in</span><span class="p">):</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">q_in</span><span class="p">,</span> <span class="n">k_in</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>  <span class="c1"># QK^T
</span>        <span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>                   <span class="c1"># 소프트맥스
</span>        <span class="n">weights</span> <span class="o">=</span> <span class="n">F</span><span class="p">.</span><span class="nf">dropout</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>           <span class="c1"># 드롭아웃
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">weights</span><span class="p">,</span> <span class="n">v_in</span><span class="p">)</span>                  <span class="c1"># V에 대한 어텐션
</span>        <span class="k">return</span> <span class="n">output</span>
    
    <span class="c1"># 자동 재계산을 위한 PyTorch 체크포인트 사용
</span>    <span class="k">return</span> <span class="n">torch</span><span class="p">.</span><span class="n">utils</span><span class="p">.</span><span class="n">checkpoint</span><span class="p">.</span><span class="nf">checkpoint</span><span class="p">(</span>
        <span class="n">recompute_expensive_memory_ops</span><span class="p">,</span> <span class="n">q_checkpoint</span><span class="p">,</span> <span class="n">k_checkpoint</span><span class="p">,</span> <span class="n">v_checkpoint</span>
    <span class="p">)</span>
</code></pre></div></div> <p><strong>메모리 수학</strong>:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>원래: 34*s*b*h + 5*a*s²*b  (모든 것 저장)
선택적: 34*s*b*h             (저메모리/고계산 연산만 저장)
감소: 5*a*s²*b 항 제거 (시퀀스 길이 제곱 스케일링 제거!)
</code></pre></div></div> <h2 id="실험-결과-숫자의-진정한-의미">실험 결과: 숫자의 진정한 의미</h2> <h3 id="주요-성능-결과">주요 성능 결과</h3> <p>| 모델 크기 | 메모리 감소 | 훈련 속도 향상 | GPU 사용률 | <strong>실용적 영향</strong> | |———–|————-|—————-|————|—————–| | 22B | 5배 | 29.0% | 41.5% → 43.7% | <strong>겨우 맞던 모델이 이제 편안하게 훈련됨</strong> | | 175B (GPT-3) | 5배 | 31.8% | 51.4% → 52.8% | <strong>10만 달러 훈련 → 7만 6천 달러 (2만 4천 달러 절약)</strong> | | 530B (MT-NLG) | 5배 | 29.7% | 56.0% → 57.0% | <strong>표준 클러스터에서 훈련 가능</strong> | | 1T | 5배 | 32.1% | 56.3% → 57.0% | <strong>1조 매개변수 모델을 실용적으로 만듦</strong> |</p> <h3 id="구성요소-분석-각-기법의-기여도-이해">구성요소 분석: 각 기법의 기여도 이해</h3> <p>| 기법 | 메모리 절약 | 속도 영향 | <strong>핵심 통찰</strong> | |——|————-|———–|—————| | 시퀀스 병렬화만 | ~50% | <strong>+3% 속도 향상</strong> | 메모리 감소 <em>및</em> 성능 향상—레이어 정규화/드롭아웃이 예상보다 비쌌음 | | 선택적 재계산만 | ~50% | <strong>+7% 오버헤드</strong> | 동일한 메모리 절약이지만 다른 성능 프로필—서로 다른 병목 최적화 | | 둘 모두 결합 | <strong>80% (5배)</strong> | <strong>+4% 오버헤드</strong> | 시너지 효과—이익이 선형적으로 결합되는 것보다 좋음 | | 전통적 재계산 | 90% | <strong>+39% 오버헤드</strong> | 현상 유지가 극도로 비싸다는 것을 검증 |</p> <p><strong>놀라운 발견</strong>: 시퀀스 병렬화는 통신을 추가함에도 불구하고 실제로 훈련을 <em>가속화</em>합니다. 이는 레이어 정규화와 드롭아웃이 예상보다 계산적으로 비싸다는 것을 보여줍니다.</p> <h2 id="비판적-평가-장점과-한계">비판적 평가: 장점과 한계</h2> <h3 id="주요-장점">주요 장점</h3> <ol> <li> <strong>수학적 엄밀성</strong>: 다양한 병렬화 전략 하에서 메모리 스케일링에 대한 정확한 공식 제공</li> <li> <strong>일관된 스케일링</strong>: 22B에서 1T 매개변수까지 30% 향상 유지—근본적 최적화를 시사</li> <li> <strong>실용적 구현</strong>: 프로덕션 프레임워크(Megatron-LM, NeMo)에서 사용 가능</li> <li> <strong>이론적 검증</strong>: 하드웨어 FLOPs가 예측된 모델 FLOPs와 밀접하게 일치하여 수학적 모델 확인</li> </ol> <h3 id="중요한-한계점">중요한 한계점</h3> <ol> <li> <strong>실험적 현실성</strong>: 데이터 병렬화 미사용 (프로덕션 훈련에 비현실적)</li> <li> <strong>기준선 완전성</strong>: ZeRO, 매개변수 샤딩, CPU 오프로딩과의 비교 누락</li> <li> <strong>하드웨어 특이성</strong>: A100에서만 테스트—일반화 불분명</li> <li> <strong>스케일 제약</strong>: 시퀀스 길이가 텐서 병렬 크기로 나누어떨어져야 함</li> </ol> <h3 id="성공을-위한-숨겨진-가정들">성공을 위한 숨겨진 가정들</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 종종 간과되는 중요한 요구사항들:
</span><span class="k">assert</span> <span class="n">sequence_length</span> <span class="o">%</span> <span class="n">tensor_parallel_size</span> <span class="o">==</span> <span class="mi">0</span>  <span class="c1"># 균등하게 나누어떨어져야 함
</span><span class="k">assert</span> <span class="n">torch</span><span class="p">.</span><span class="n">cuda</span><span class="p">.</span><span class="nf">memory_fragmentation</span><span class="p">()</span> <span class="o">&lt;</span> <span class="mf">0.1</span>     <span class="c1"># 낮은 단편화 필요
</span><span class="k">assert</span> <span class="nf">all_ranks_synchronized</span><span class="p">()</span>                     <span class="c1"># 완벽한 동기화 필요
</span><span class="k">assert</span> <span class="nf">batch_sequences_uniform_length</span><span class="p">()</span>             <span class="c1"># 가변 길이 문제 있음
</span></code></pre></div></div> <h2 id="실용적-구현-가이드">실용적 구현 가이드</h2> <h3 id="가장-잘-작동하는-경우">가장 잘 작동하는 경우</h3> <ul> <li> <strong>대형 모델</strong> (&gt;100B 매개변수) 활성화 메모리가 지배적인 경우</li> <li> <strong>긴 시퀀스</strong> 5<em>a</em>s²/h 항이 중요한 경우</li> <li> <strong>고대역폭 상호연결</strong> (NVLink, InfiniBand) 효율적인 통신을 위해</li> <li> <strong>균등한 워크로드</strong> 일관된 시퀀스 길이</li> </ul> <h3 id="주의해야-할-경우">주의해야 할 경우</h3> <ul> <li> <strong>가변 시퀀스 길이</strong>: 계산을 낭비할 수 있는 패딩 필요</li> <li> <strong>소형 모델</strong>: 오버헤드가 이익을 상회할 수 있음</li> <li> <strong>메모리 제약 환경</strong>: 추가 기법(ZeRO, 오프로딩) 필요할 수 있음</li> <li> <strong>레거시 하드웨어</strong>: 현대 가속기에 최적화된 통신 패턴</li> </ul> <h3 id="통합-전략">통합 전략</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 권장 구현 방식:
</span><span class="k">def</span> <span class="nf">adaptive_optimization_strategy</span><span class="p">(</span><span class="n">model_size</span><span class="p">,</span> <span class="n">available_memory</span><span class="p">,</span> <span class="n">sequence_length</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">model_size</span> <span class="o">&gt;</span> <span class="mf">100e9</span><span class="p">:</span>  <span class="c1"># &gt;100B 매개변수
</span>        <span class="n">use_sequence_parallelism</span> <span class="o">=</span> <span class="bp">True</span>
        
        <span class="n">memory_ratio</span> <span class="o">=</span> <span class="nf">estimate_memory_usage</span><span class="p">()</span> <span class="o">/</span> <span class="n">available_memory</span>
        <span class="k">if</span> <span class="n">memory_ratio</span> <span class="o">&gt;</span> <span class="mf">0.9</span><span class="p">:</span>
            <span class="n">use_selective_recomputation</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">elif</span> <span class="n">memory_ratio</span> <span class="o">&gt;</span> <span class="mf">0.7</span><span class="p">:</span>
            <span class="n">use_microbatch_recomputation</span> <span class="o">=</span> <span class="bp">True</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">use_selective_recomputation</span> <span class="o">=</span> <span class="bp">False</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># 작은 모델의 경우, 이익이 복잡성을 정당화하지 못할 수 있음
</span>        <span class="k">return</span> <span class="sh">"</span><span class="s">standard_tensor_parallelism</span><span class="sh">"</span>
</code></pre></div></div> <h2 id="미래-방향과-연구-기회">미래 방향과 연구 기회</h2> <h3 id="즉시-확장-가능한-영역">즉시 확장 가능한 영역</h3> <ol> <li> <strong>동적 시퀀스 병렬화</strong>: 가변 길이 시퀀스를 효율적으로 처리</li> <li> <strong>하이브리드 메모리 전략</strong>: ZeRO 및 CPU 오프로딩과 결합</li> <li> <strong>하드웨어 최적화</strong>: 다양한 가속기 아키텍처에 맞춘 튜닝</li> <li> <strong>자동화된 최적화</strong>: ML 가이드 재계산 전략 선택</li> </ol> <h3 id="근본적-질문들">근본적 질문들</h3> <ol> <li> <strong>일반화</strong>: 이러한 패턴이 다른 아키텍처(Vision Transformer 등)에 적용되는가?</li> <li> <strong>스케일링 한계</strong>: 1조 매개변수 모델을 넘어서면 어떻게 되는가?</li> <li> <strong>통신 진화</strong>: 미래의 상호연결이 최적 전략을 어떻게 바꿀 것인가?</li> </ol> <h2 id="큰-그림">큰 그림</h2> <p>이 연구는 “일률적인” 최적화에서 <strong>연산 인식 메모리 관리</strong>로의 <strong>패러다임 전환</strong>을 나타냅니다. 서로 다른 연산이 서로 다른 메모리-계산 트레이드오프를 가진다는 것을 인식함으로써, 훨씬 더 정교한 최적화 전략의 문을 엽니다.</p> <p><strong>연구자들을 위해</strong>: 대규모 훈련에서 메모리 병목을 분석하기 위한 수학적 프레임워크 제공</p> <p><strong>실무자들을 위해</strong>: 훈련 효율성을 즉시 개선할 수 있는 프로덕션 준비된 기법 제공</p> <p><strong>분야 전체를 위해</strong>: 계산 패턴의 신중한 분석이 놀라운 최적화를 가져올 수 있음을 보여줌—대형 모델 훈련 방식에는 여전히 상당한 개선 여지가 있음</p> <p>이 기법들은 이미 이전에는 불가능했던 훈련을 가능하게 하고 있으며, 수학적 프레임워크는 모델이 다조 매개변수 체제로 계속 확장됨에 따라 미래 최적화의 기반을 제공합니다.</p> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa3/">Flash Attention 3</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa2/">Flash Attention 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa/">Flash Attention</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/usp/">Unified Sequence Parallelism</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/deepspeed_ulysses/">DeepSpeed Ulysses</a> </li> <div id="disqus_thread" style="max-width: 930px; margin: 0 auto;"> <script type="text/javascript">
    var disqus_shortname  = 'sungyubkim';
    var disqus_identifier = '/blog/sp';
    var disqus_title      = "Reducing Activation Recomputation in Large Transformer Models";
    (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Sung-Yub Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 29, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>