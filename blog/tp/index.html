<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Tensor Parallel | Sung-Yub Kim </title> <meta name="author" content="Sung-Yub Kim"> <meta name="description" content="Megatron-LM's tensor model parallelism for training large Transformer models across multiple GPUs."> <meta name="keywords" content="machine-learning, deep-learning, llm, foundation-models, research"> <meta property="og:site_name" content="Sung-Yub Kim"> <meta property="og:type" content="article"> <meta property="og:title" content="Sung-Yub Kim | Tensor Parallel"> <meta property="og:url" content="https://sungyubkim.github.io/blog/tp/"> <meta property="og:description" content="Megatron-LM's tensor model parallelism for training large Transformer models across multiple GPUs."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Tensor Parallel"> <meta name="twitter:description" content="Megatron-LM's tensor model parallelism for training large Transformer models across multiple GPUs."> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Sung-Yub Kim"
        },
        "url": "https://sungyubkim.github.io/blog/tp/",
        "@type": "BlogPosting",
        "description": "Megatron-LM's tensor model parallelism for training large Transformer models across multiple GPUs.",
        "headline": "Tensor Parallel",
        
        "sameAs": ["https://github.com/sungyubkim","https://scholar.google.com/citations?user=m2rhgrkAAAAJ","https://www.linkedin.com/in/sung-yub-kim-0a82a1264","https://twitter.com/SungyubK"],
        
        "name": "Sung-Yub Kim",
        "@context": "https://schema.org"
    }
  </script> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sungyubkim.github.io/blog/tp/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Sung-Yub</span> Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Tensor Parallel</h1> <p class="post-meta"> Created on May 28, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/tensor-parallelism"> <i class="fa-solid fa-hashtag fa-sm"></i> tensor-parallelism</a>   <a href="/blog/tag/distributed-training"> <i class="fa-solid fa-hashtag fa-sm"></i> distributed-training</a>   <a href="/blog/tag/memory-efficiency"> <i class="fa-solid fa-hashtag fa-sm"></i> memory-efficiency</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="tldr">TL;DR</h1> <blockquote> <p>메가트론-LM은 여러 GPU에 걸쳐 개별 모델 레이어를 분할하여 거대한 트랜스포머 언어 모델을 훈련시키는 우아하고 효율적인 접근 방식을 소개합니다. 이 “텐서 모델 병렬화” 기술을 사용하여 연구자들은 83억 개의 파라미터를 가진 모델(GPT-2보다 5.6배 큰)을 높은 계산 효율성(512 GPU에서 76% 스케일링 효율성)으로 성공적으로 훈련시켰습니다. 이 접근 방식은 기존 PyTorch 코드에 최소한의 변경만 필요하며, 다른 병렬화 기술과 결합될 수 있고, 언어 모델링 성능을 향상시킵니다. 연구자들은 또한 더 큰 모델로 확장할 때 레이어 정규화 배치가 중요해진다는 것을 발견했습니다.</p> </blockquote> <ul> <li>Paper Link: <a href="https://arxiv.org/pdf/1909.08053" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/1909.08053</a> </li> </ul> <hr> <h1 id="related-papers">Related Papers</h1> <p><strong>모델 병렬화 기법:</strong></p> <ul> <li> <a href="/blog/pp/">GPipe</a> - 파이프라인 병렬화를 통한 대규모 모델 훈련</li> <li> <a href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> - 메모리 효율적인 병렬 훈련 기법</li> <li> <a href="/blog/ring-self-attention/">Ring Self-Attention</a> - 시퀀스 차원 병렬화</li> </ul> <p><strong>분산 훈련 시스템:</strong></p> <ul> <li> <a href="https://arxiv.org/pdf/2309.14509" rel="external nofollow noopener" target="_blank">DeepSpeed Ulysses</a> - 시퀀스 병렬화와 텐서 병렬화 결합</li> <li> <a href="/blog/usp/">USP</a> - 통합 시퀀스 병렬화 프레임워크</li> <li> <a href="https://arxiv.org/pdf/2406.18485" rel="external nofollow noopener" target="_blank">LoongTrain</a> - 하이브리드 병렬화 접근법</li> </ul> <hr> <h1 id="takeaways">Takeaways</h1> <h3 id="1-동기-및-배경">1. 동기 및 배경</h3> <h3 id="문제-대형-모델에-대한-메모리-제한">문제: 대형 모델에 대한 메모리 제한</h3> <p>이 연구의 주요 동기는 간단했습니다: 연구자들은 더 큰 트랜스포머 모델이 자연어 작업에서 성능을 꾸준히 향상시키지만, GPU 메모리 제한에 부딪히고 있다는 것을 관찰했습니다.</p> <p>메가트론-LM 이전에, BERT(3억 4천만 파라미터)와 GPT-2(15억 파라미터)와 같은 언어 모델은 모델 크기를 확장하면 더 나은 결과를 얻을 수 있음을 보여주었습니다. 그러나 추가 확장은 개별 GPU의 메모리 용량에 의해 제한되었습니다. 32GB 메모리를 가진 고성능 NVIDIA V100을 사용하더라도, 연구자들은 훈련 중에 약 12억 파라미터의 모델만 맞출 수 있었습니다(메모리 최적화 기술을 사용해도).</p> <p>근본적인 병목 현상은 분명했습니다: 더 큰 모델을 훈련시키기 위해서는 데이터뿐만 아니라 모델 자체를 여러 GPU에 분할하는 방법이 필요했습니다.</p> <h3 id="기존-접근-방식과-그-한계">기존 접근 방식과 그 한계</h3> <p>이 연구 이전에는 분산 훈련에 두 가지 주요 접근 방식이 있었습니다:</p> <ol> <li> <strong>데이터 병렬화</strong>: 각 GPU가 모델의 완전한 복사본을 가지고, 배치를 GPU에 걸쳐 분할합니다. 이것은 대형 모델의 메모리 제한에 도움이 되지 않습니다.</li> <li> <strong>파이프라인 병렬화</strong>(예: GPipe): 서로 다른 레이어를 서로 다른 GPU에 할당하여 모델을 분할합니다. 효과적이지만, 이 접근 방식은 효율성을 떨어뜨리는 파이프라인 버블(GPU 유휴 시간)을 도입합니다.</li> </ol> <p>부족했던 것은 레이어 간이 아닌 각 레이어 내에서 세밀한 병렬화를 가능하게 하는, 트랜스포머 모델의 독특한 계산 구조를 특별히 다루는 효율적인 접근 방식이었습니다.</p> <h3 id="2-핵심-가정-및-성공-조건">2. 핵심 가정 및 성공 조건</h3> <p>메가트론-LM의 접근 방식이 효과적으로 작동하기 위해서는 몇 가지 핵심 가정과 조건이 필요했습니다:</p> <ol> <li> <strong>고대역폭 GPU 인터커넥트</strong>: 이 접근 방식은 GPU 간의 빠른 통신을 가정합니다. 연구자들은 NVLink(GPU 간 300+ GB/s 대역폭)를 갖춘 NVIDIA의 DGX SuperPOD를 사용했는데, 이는 표준 PCIe 연결(~32 GB/s)보다 훨씬 빠릅니다.</li> <li> <strong>은닉 차원의 균등 분할 가능성</strong>: 모델의 은닉 차원은 텐서 병렬화에 사용되는 GPU 수로 균등하게 나누어져야 합니다. 이는 가중치 행렬이 나머지 없이 분할될 수 있도록 합니다.</li> <li> <strong>병렬화의 이점을 얻기에 충분히 큰 행렬</strong>: 이 접근 방식은 행렬 연산을 분할하는 계산상의 이점이 통신 오버헤드보다 크다고 가정합니다. 이는 충분히 큰 모델에서만 유효합니다.</li> <li> <strong>NCCL 통신 라이브러리</strong>: 구현은 GPU 간의 효율적인 집합 통신 작업을 위해 NVIDIA의 NCCL 라이브러리에 의존합니다.</li> <li> <strong>혼합 정밀도 훈련</strong>: 메모리 효율성과 계산 처리량을 최대화하기 위해, 혼합 정밀도 훈련(FP16/FP32)이 실질적으로 필요합니다.</li> <li> <strong>대형 모델의 안정적인 훈련</strong>: 모델이 커질수록 수치적 안정성이 더 어려워집니다. 연구자들은 표준 트랜스포머 아키텍처에 대한 수정, 특히 레이어 정규화 배치가 필요하다는 것을 발견했습니다.</li> </ol> <h3 id="3-메가트론-lm-방법-설명">3. 메가트론-LM 방법 설명</h3> <p>메가트론-LM의 핵심 통찰은 트랜스포머 모델 중 최소한의 통신 오버헤드로 GPU에 걸쳐 효율적으로 병렬화할 수 있는 부분을 식별하는 것입니다.</p> <h3 id="31-핵심-개념-행과-열-분할을-통한-텐서-병렬화">3.1 핵심 개념: 행과 열 분할을 통한 텐서 병렬화</h3> <p>주요 혁신은 트랜스포머 레이어 내의 큰 가중치 행렬을 여러 GPU에 걸쳐 분할하는 것입니다. 두 가지 주요 유형의 행렬 분할이 있습니다:</p> <h3 id="열-병렬-선형-레이어">열-병렬 선형 레이어:</h3> <p>각 GPU는 동일한 입력을 받지만 출력 뉴런의 일부만 담당합니다. 이는 가중치 행렬을 열 방향으로 분할하는 것과 같습니다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">ColumnParallelLinear</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">gather_output</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">ColumnParallelLinear</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1"># 모델 병렬 그룹에서 월드 사이즈와 랭크 가져오기
</span>        <span class="n">world_size</span> <span class="o">=</span> <span class="nf">get_model_parallel_world_size</span><span class="p">()</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="nf">get_model_parallel_rank</span><span class="p">()</span>

        <span class="c1"># GPU에 걸쳐 출력 크기 분할
</span>        <span class="n">self</span><span class="p">.</span><span class="n">output_size_per_partition</span> <span class="o">=</span> <span class="n">output_size</span> <span class="o">//</span> <span class="n">world_size</span>

        <span class="c1"># 로컬 선형 레이어 생성 (출력 뉴런의 일부만)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">local_linear</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span>
            <span class="n">input_size</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">output_size_per_partition</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>

        <span class="c1"># GPU에 걸쳐 출력을 수집할지 여부를 나타내는 플래그
</span>        <span class="n">self</span><span class="p">.</span><span class="n">gather_output</span> <span class="o">=</span> <span class="n">gather_output</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_</span><span class="p">):</span>
        <span class="c1"># 로컬 순방향 계산
</span>        <span class="n">local_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">local_linear</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span>

        <span class="c1"># 필요한 경우, 모든 GPU에서 출력 수집
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">gather_output</span><span class="p">:</span>
            <span class="c1"># 모든 출력을 수집하는 all-gather 연산
</span>            <span class="n">output</span> <span class="o">=</span> <span class="nf">all_gather_from_model_parallel</span><span class="p">(</span><span class="n">local_output</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">output</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">local_output</span>

</code></pre></div></div> <h3 id="행-병렬-선형-레이어">행-병렬 선형 레이어:</h3> <p>각 GPU는 입력의 일부만 받아 전체 출력의 부분적인 결과를 계산합니다. 이는 가중치 행렬을 행 방향으로 분할하는 것과 같습니다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">RowParallelLinear</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">True</span><span class="p">,</span> <span class="n">input_is_parallel</span><span class="o">=</span><span class="bp">False</span><span class="p">):</span>
        <span class="nf">super</span><span class="p">(</span><span class="n">RowParallelLinear</span><span class="p">,</span> <span class="n">self</span><span class="p">).</span><span class="nf">__init__</span><span class="p">()</span>
        <span class="c1"># 모델 병렬 그룹에서 월드 사이즈와 랭크 가져오기
</span>        <span class="n">world_size</span> <span class="o">=</span> <span class="nf">get_model_parallel_world_size</span><span class="p">()</span>
        <span class="n">rank</span> <span class="o">=</span> <span class="nf">get_model_parallel_rank</span><span class="p">()</span>

        <span class="c1"># GPU에 걸쳐 입력 크기 분할
</span>        <span class="n">self</span><span class="p">.</span><span class="n">input_size_per_partition</span> <span class="o">=</span> <span class="n">input_size</span> <span class="o">//</span> <span class="n">world_size</span>

        <span class="c1"># 로컬 선형 레이어 생성 (입력 뉴런의 일부만)
</span>        <span class="n">self</span><span class="p">.</span><span class="n">local_linear</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Linear</span><span class="p">(</span>
            <span class="n">self</span><span class="p">.</span><span class="n">input_size_per_partition</span><span class="p">,</span> <span class="n">output_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="bp">False</span><span class="p">)</span>

        <span class="c1"># 입력이 이미 분할되어 있는지 나타내는 플래그
</span>        <span class="n">self</span><span class="p">.</span><span class="n">input_is_parallel</span> <span class="o">=</span> <span class="n">input_is_parallel</span>

        <span class="c1"># 바이어스는 all-reduce 후에 적용되므로 한 개만 필요
</span>        <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">nn</span><span class="p">.</span><span class="nc">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="p">.</span><span class="nf">empty</span><span class="p">(</span><span class="n">output_size</span><span class="p">))</span> <span class="k">if</span> <span class="n">bias</span> <span class="k">else</span> <span class="bp">None</span>

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">input_</span><span class="p">):</span>
        <span class="c1"># 입력이 아직 분할되지 않았다면, 분산시킴
</span>        <span class="k">if</span> <span class="ow">not</span> <span class="n">self</span><span class="p">.</span><span class="n">input_is_parallel</span><span class="p">:</span>
            <span class="n">input_parallel</span> <span class="o">=</span> <span class="nf">scatter_to_model_parallel</span><span class="p">(</span><span class="n">input_</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">input_parallel</span> <span class="o">=</span> <span class="n">input_</span>

        <span class="c1"># 로컬 순방향 계산
</span>        <span class="n">local_output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">local_linear</span><span class="p">(</span><span class="n">input_parallel</span><span class="p">)</span>

        <span class="c1"># 완전한 출력을 얻기 위한 all-reduce
</span>        <span class="n">output</span> <span class="o">=</span> <span class="nf">all_reduce_from_model_parallel</span><span class="p">(</span><span class="n">local_output</span><span class="p">)</span>

        <span class="c1"># all-reduce 후 바이어스 적용
</span>        <span class="k">if</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span> <span class="ow">is</span> <span class="ow">not</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">output</span> <span class="o">=</span> <span class="n">output</span> <span class="o">+</span> <span class="n">self</span><span class="p">.</span><span class="n">bias</span>

        <span class="k">return</span> <span class="n">output</span>

</code></pre></div></div> <h3 id="32-실제-예시">3.2 실제 예시</h3> <p>1024 은닉 차원과 2-way 텐서 병렬화를 사용한 트랜스포머 레이어의 구체적인 예를 들어보겠습니다:</p> <p><strong>셀프-어텐션 블록 예시:</strong></p> <p>병렬화 없이, 셀프-어텐션 계산은 다음과 같을 것입니다:</p> <ol> <li>QKV 투영: 입력 [batch_size, seq_len, 1024] → 크기 [1024, 1024]의 3개 행렬 → 출력 [batch_size, seq_len, 3072]</li> <li>어텐션 계산 및 출력 투영: [batch_size, seq_len, 3072] → [batch_size, seq_len, 1024]</li> </ol> <p>2-way 텐서 병렬화를 사용하면:</p> <p><strong>GPU 0:</strong></p> <ul> <li>QKV 투영: 입력 [batch_size, seq_len, 1024] → 크기 [1024, 512]의 3개 행렬 → 출력 [batch_size, seq_len, 1536]</li> <li>헤드의 절반에 대한 어텐션 계산</li> <li>출력 투영: 입력 [batch_size, seq_len, 512] → 행렬 [512, 1024] → 부분 출력 [batch_size, seq_len, 1024]</li> </ul> <p><strong>GPU 1:</strong></p> <ul> <li>QKV 투영: 입력 [batch_size, seq_len, 1024] → 크기 [1024, 512]의 3개 행렬 → 출력 [batch_size, seq_len, 1536]</li> <li>나머지 헤드의 어텐션 계산</li> <li>출력 투영: 입력 [batch_size, seq_len, 512] → 행렬 [512, 1024] → 부분 출력 [batch_size, seq_len, 1024]</li> </ul> <p><strong>통신:</strong></p> <ul> <li>출력 투영 결과에 대한 all-reduce로 완전한 결과 획득</li> </ul> <p>설계상, 각 GPU는 가중치에 대해 절반의 메모리를 사용하고 절반의 연산을 계산하지만, 셀프-어텐션 블록 끝에서 단 한 번의 통신만 필요합니다.</p> <h3 id="4-실험-결과-및-분석">4. 실험 결과 및 분석</h3> <h3 id="41-주요-성능-결과">4.1 주요 성능 결과</h3> <table> <thead> <tr> <th>모델 크기</th> <th>GPU</th> <th>텐서 병렬화</th> <th>배치 크기</th> <th>처리량</th> <th>스케일링 효율성</th> <th>지속 성능</th> </tr> </thead> <tbody> <tr> <td>1.2B</td> <td>1</td> <td>1</td> <td>8</td> <td>39 TFLOPS</td> <td>100% (기준선)</td> <td>피크의 30%</td> </tr> <tr> <td>2.5B</td> <td>2</td> <td>2</td> <td>8</td> <td>73 TFLOPS</td> <td>94%</td> <td>-</td> </tr> <tr> <td>4.2B</td> <td>4</td> <td>4</td> <td>8</td> <td>138 TFLOPS</td> <td>88%</td> <td>-</td> </tr> <tr> <td>8.3B</td> <td>8</td> <td>8</td> <td>8</td> <td>254 TFLOPS</td> <td>81%</td> <td>-</td> </tr> <tr> <td>8.3B</td> <td>512</td> <td>8</td> <td>512</td> <td>15.1 PFLOPS</td> <td>76%</td> <td>-</td> </tr> </tbody> </table> <p>이 결과는 여러 가지 이유로 인상적입니다. 첫째, 단일 GPU에서 39 TFLOPS의 기준 성능은 NVIDIA V100 GPU의 이론적 피크의 30%로, 복잡한 딥러닝 애플리케이션에서는 꽤 좋은 수치입니다. 둘째, 텐서 병렬화가 증가해도 스케일링 효율성이 높게 유지되어 8-way 병렬화에서 81% 효율성을 보입니다.</p> <p>가장 인상적인 결과는 512 GPU를 사용하여 달성한 15.1 PFLOPS로, 8-way 텐서 병렬화와 64-way 데이터 병렬화를 결합했습니다. 이는 단일 GPU 기준선 대비 76%의 스케일링 효율성을 나타내는데, 이 규모의 분산 시스템에서는 놀라운 수치입니다.</p> <h3 id="42-언어-모델링-성능">4.2 언어 모델링 성능</h3> <table> <thead> <tr> <th>모델</th> <th>파라미터</th> <th>언어 모델링 퍼플렉시티</th> <th>LAMBADA 정확도</th> </tr> </thead> <tbody> <tr> <td>GPT-2</td> <td>1.5B</td> <td>~35.7</td> <td>45.9%</td> </tr> <tr> <td>메가트론-LM</td> <td>1.5B</td> <td>~35.0</td> <td>50.6%</td> </tr> <tr> <td>메가트론-LM</td> <td>8.3B</td> <td>~29.8</td> <td>63.2%</td> </tr> </tbody> </table> <p>이 결과는 더 큰 모델이 더 나은 언어 모델링 능력을 제공한다는 가설을 검증합니다. 83억 파라미터 모델은 15억 모델보다 퍼플렉시티(낮을수록 좋음)와 LAMBADA 정확도(높을수록 좋음) 모두에서 크게 성능이 향상되었습니다.</p> <p>특히 흥미로운 점은 크기가 동일한 15억 메가트론-LM 모델도 GPT-2보다 약간 더 나은 성능을 보였다는 것인데, 이는 그들의 훈련 접근 방식이나 아키텍처 수정이 규모와 상관없이 유익했을 수 있음을 시사합니다.</p> <h3 id="43-애블레이션-연구-레이어-정규화-배치">4.3 애블레이션 연구: 레이어 정규화 배치</h3> <table> <thead> <tr> <th>레이어 정규화 위치</th> <th>1B 모델 퍼플렉시티</th> <th>4B 모델 퍼플렉시티</th> </tr> </thead> <tbody> <tr> <td>Pre-LN (원래)</td> <td>3.77</td> <td>발산</td> </tr> <tr> <td>Post-LN (수정됨)</td> <td>3.69</td> <td>3.31</td> </tr> </tbody> </table> <p>이 애블레이션 연구는 중요한 점을 밝혀냈습니다: 트랜스포머 모델이 커질수록, 작은 모델에서 잘 작동했던 아키텍처 선택이 더 이상 실행 가능하지 않을 수 있습니다. 구체적으로, 표준 레이어 정규화 배치(Pre-LN, 어텐션/FFN 블록 이전에 레이어 정규화 적용)는 더 큰 모델에서 훈련 발산을 일으켰습니다.</p> <p>레이어 정규화를 어텐션/FFN 블록 이후로 이동시킴으로써(Post-LN), 그들은 안정적인 훈련과 더 나은 성능을 달성했습니다. 이 발견은 꽤 중요했으며 이후 대형 모델 설계에 영향을 미쳤습니다.</p> <h3 id="5-영향-및-유산">5. 영향 및 유산</h3> <p>메가트론-LM의 영향은 상당했습니다:</p> <ol> <li> <strong>더 큰 모델 가능</strong>: 이 접근 방식은 이전에 가능했던 것보다 훨씬 더 큰 모델을 훈련시킬 수 있게 하여, 수천억 개의 파라미터를 가진 GPT-3, Gopher, PaLM 등의 후속 모델을 위한 기반을 마련했습니다.</li> <li> <strong>아키텍처 혁신</strong>: 레이어 정규화 배치 및 기타 훈련 안정화 기술에 대한 그들의 발견은 후속 모델 설계에 영향을 미쳤습니다.</li> <li> <strong>대규모 훈련의 민주화</strong>: PyTorch 코드에 최소한의 변경으로 접근 방식을 구현함으로써, 특수한 컴파일러나 하드웨어 없이도 연구자들이 대규모 훈련에 접근할 수 있게 했습니다.</li> <li> <strong>다른 접근 방식과의 보완성</strong>: 아마도 가장 중요한 것은, 그들의 텐서 병렬화 접근 방식이 파이프라인 병렬화 및 데이터 병렬화와 보완적임이 증명되어, 가장 큰 모델 훈련에서 표준이 된 3D 병렬화를 가능하게 했다는 점입니다.</li> </ol> <p>메가트론-LM 연구는 언어 모델 확장의 중요한 순간을 나타내며, 적절한 병렬화 전략을 통해 수십억 개의 파라미터를 가진 모델을 훈련시키는 것이 가능할 뿐만 아니라 높은 계산 효율성으로 수행될 수 있음을 보여주었습니다. 이는 최근 언어 모델의 발전을 정의한 모델 규모의 폭발적인 증가를 위한 기반을 직접적으로 마련했습니다.</p> <p>결론적으로, 메가트론-LM의 모델 병렬화 접근 방식은 오늘날의 대형 언어 모델을 향한 중요한 디딤돌이었습니다. 회고적으로 보면 비교적 간단한 기술처럼 보일 수 있지만, 기존 코드에 최소한의 변경만으로 중요한 스케일링 병목 현상을 우아하게 해결했습니다. 이 효과성과 접근성의 조합은 대규모 언어 모델링 분야의 발전을 가속화하는 데 특히 영향력이 컸습니다.</p> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa3/">Flash Attention 3</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa2/">Flash Attention 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa/">Flash Attention</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/usp/">Unified Sequence Parallelism</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> </li> <div id="disqus_thread" style="max-width: 930px; margin: 0 auto;"> <script type="text/javascript">
    var disqus_shortname  = 'sungyubkim';
    var disqus_identifier = '/blog/tp';
    var disqus_title      = "Tensor Parallel";
    (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Sung-Yub Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 29, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>