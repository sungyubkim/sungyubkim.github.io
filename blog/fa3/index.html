<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Flash Attention 3 | Sung-Yub Kim </title> <meta name="author" content="Sung-Yub Kim"> <meta name="description" content="FlashAttention-3 achieves 1.5-2x speedup on H100 GPUs via warp-specialization, WGMMA pipelining, and FP8 support."> <meta name="keywords" content="machine-learning, deep-learning, llm, foundation-models, research"> <meta property="og:site_name" content="Sung-Yub Kim"> <meta property="og:type" content="article"> <meta property="og:title" content="Sung-Yub Kim | Flash Attention 3"> <meta property="og:url" content="https://sungyubkim.github.io/blog/fa3/"> <meta property="og:description" content="FlashAttention-3 achieves 1.5-2x speedup on H100 GPUs via warp-specialization, WGMMA pipelining, and FP8 support."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Flash Attention 3"> <meta name="twitter:description" content="FlashAttention-3 achieves 1.5-2x speedup on H100 GPUs via warp-specialization, WGMMA pipelining, and FP8 support."> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Sung-Yub Kim"
        },
        "url": "https://sungyubkim.github.io/blog/fa3/",
        "@type": "BlogPosting",
        "description": "FlashAttention-3 achieves 1.5-2x speedup on H100 GPUs via warp-specialization, WGMMA pipelining, and FP8 support.",
        "headline": "Flash Attention 3",
        
        "sameAs": ["https://github.com/sungyubkim","https://scholar.google.com/citations?user=m2rhgrkAAAAJ","https://www.linkedin.com/in/sung-yub-kim-0a82a1264","https://twitter.com/SungyubK"],
        
        "name": "Sung-Yub Kim",
        "@context": "https://schema.org"
    }
  </script> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sungyubkim.github.io/blog/fa3/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Sung-Yub</span> Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Flash Attention 3</h1> <p class="post-meta"> Created on June 04, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> attention</a>   <a href="/blog/tag/memory-efficiency"> <i class="fa-solid fa-hashtag fa-sm"></i> memory-efficiency</a>   <a href="/blog/tag/hardware-optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> hardware-optimization</a>   <a href="/blog/tag/inference"> <i class="fa-solid fa-hashtag fa-sm"></i> inference</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="tldr">TL;DR</h1> <blockquote> <p>FlashAttention-3는 AI 모델(ChatGPT 같은)의 어텐션 메커니즘을 <strong>1.5-2.0배 빠르게</strong> 만들면서 <strong>절반 정밀도(FP8)</strong>를 사용해도 정확도 손실 없이 작동하는 획기적인 최적화 기술입니다.</p> <p><strong>문제점</strong>: FlashAttention-2가 H100 GPU에서 단 35%의 활용률만을 달성 - 즉, 비싼 AI 하드웨어가 가장 중요한 연산 중에 대부분 놀고 있었습니다.</p> <p><strong>해결책</strong>: 함께 작동하는 세 가지 핵심 혁신:</p> <ol> <li> <strong>비동기 처리</strong>: 데이터 로딩과 연산을 위한 별도의 작업자를 두는 것과 같음</li> <li> <strong>중첩 연산</strong>: 기다리는 대신 여러 작업을 동시에 계산</li> <li> <strong>스마트 저정밀도</strong>: 정확도 손실 없이 16비트 대신 8비트 숫자 사용</li> </ol> <p><strong>주요 결과</strong>:</p> <ul> <li> <strong>속도</strong>: 최대 740 TFLOPs/s (GPU 활용률 75% vs 이전 35%)</li> <li> <strong>효율성</strong>: FP8 정밀도로 거의 1.2 PFLOPs/s</li> <li> <strong>정확도</strong>: 표준 FP8 방법보다 2.6배 더 정확</li> <li> <strong>임팩트</strong>: 장문맥 AI 애플리케이션(전체 책, 코드베이스 분석)을 실용적으로 만듦</li> </ul> <p>어텐션이 현대 AI의 연산 병목이기 때문에 이를 빠르게 만드는 것은 더 강력한 AI 애플리케이션을 더 낮은 비용으로 가능하게 합니다.</p> </blockquote> <ul> <li>Paper Link: <a href="https://arxiv.org/pdf/2407.08608" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2407.08608</a> </li> </ul> <hr> <h1 id="related-papers">Related Papers</h1> <p><strong>FlashAttention 시리즈 발전:</strong></p> <ul> <li> <a href="/blog/fa/">FlashAttention</a> - 원조 IO-aware 어텐션 알고리즘과 타일링 기법의 기초</li> <li> <a href="/blog/fa2/">FlashAttention-2</a> - 2배 빠른 속도와 향상된 병렬화로 발전된 버전</li> <li> <a href="https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf" rel="external nofollow noopener" target="_blank">From Online Softmax to FlashAttention</a> - 온라인 소프트맥스의 이론적 기초와 수치 안정성</li> </ul> <p><strong>효율적인 어텐션 메커니즘:</strong></p> <ul> <li> <a href="https://arxiv.org/pdf/2112.05682" rel="external nofollow noopener" target="_blank">Memory-efficient Attention</a> - 메모리 최적화 어텐션 기법들의 비교 분석</li> <li> <a href="https://arxiv.org/pdf/1911.02150" rel="external nofollow noopener" target="_blank">MQA</a> - 멀티쿼리 어텐션으로 키/값 공유를 통한 추론 가속</li> <li> <a href="https://arxiv.org/pdf/2305.13245" rel="external nofollow noopener" target="_blank">GQA</a> - 그룹드 쿼리 어텐션의 품질-속도 균형점 탐색</li> </ul> <p><strong>위치 인코딩과 긴 시퀀스 처리:</strong></p> <ul> <li> <a href="https://arxiv.org/pdf/2104.09864" rel="external nofollow noopener" target="_blank">RoFormer</a> - 회전 위치 임베딩과의 최적화된 통합</li> <li> <a href="https://arxiv.org/pdf/2309.00071" rel="external nofollow noopener" target="_blank">YaRN</a> - RoPE 기반 컨텍스트 길이 확장 기법</li> <li> <a href="https://arxiv.org/pdf/2310.05209" rel="external nofollow noopener" target="_blank">Scaling Laws of RoPE-based Extrapolation</a> - 위치 인코딩 확장과 성능 스케일링</li> </ul> <p><strong>하드웨어 최적화 및 시스템 연구:</strong></p> <ul> <li> <a href="/blog/tp/">Tensor Parallelism</a> - 텐서 병렬화와 FlashAttention의 결합 전략</li> <li> <a href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> - 메모리 효율적인 병렬 훈련 기법</li> <li> <a href="/blog/pp/">GPipe</a> - 파이프라인 병렬화와의 통합 방법론</li> </ul> <hr> <h1 id="takeaways">Takeaways</h1> <h2 id="1-동기-왜-flashattention-3가-필요했는가">1. 동기: 왜 FlashAttention-3가 필요했는가</h2> <h3 id="성능-격차-문제">성능 격차 문제</h3> <p><strong>실험적 동기</strong>: 저자들은 기존 어텐션 구현의 심각한 비효율성을 발견했습니다. FlashAttention-2가 새로운 GPU에서 최적화된 행렬 곱셈(GEMM) 커널 대비 낮은 활용률을 달성하는데, Hopper H100 GPU에서 35% 대 80-90%였습니다.</p> <p>이는 포뮬러 1 자동차를 가지고 있지만 엔진 출력의 35%만 사용하는 것과 같습니다 - 프리미엄 하드웨어 비용을 지불하지만 평범한 성능을 얻고 있었습니다. 저자들은 기존 어텐션 알고리즘이 새로운 GPU 기능을 활용하도록 설계되지 않았음을 깨달았습니다.</p> <p><strong>하드웨어 진화 격차</strong>: 현대 GPU(NVIDIA H100 같은)는 새로운 기능들을 도입했습니다:</p> <ul> <li> <strong>비동기 텐서 코어</strong>: 데이터를 로딩하면서 동시에 연산 가능</li> <li> <strong>FP8 정밀도</strong>: FP16보다 2배 빠르지만 신중한 처리 필요</li> <li> <strong>전용 메모리 유닛</strong>: 특정 연산을 위해 설계된 하드웨어</li> </ul> <p>하지만 FlashAttention-2는 구형 하드웨어를 위해 설계되어 이러한 기능들을 효과적으로 사용할 수 없었습니다.</p> <h3 id="장문맥-도전">장문맥 도전</h3> <p><strong>실제 임팩트</strong>: 어텐션을 더 긴 맥락으로 확장하면 새로운 능력(여러 긴 문서와 대용량 코드베이스의 파일들에 대한 모델링과 추론), 새로운 모달리티(고해상도 이미지, 오디오, 비디오), 새로운 애플리케이션(긴 기록과의 사용자 상호작용, 긴 지평선의 에이전트 워크플로)이 열릴 것입니다.</p> <p>현재 AI 모델들은 다음과 같은 작업에서 어려움을 겪습니다:</p> <ul> <li>전체 책이나 연구 논문 분석</li> <li>완전한 영화 스크립트나 긴 대화 이해</li> <li>고해상도 이미지나 긴 오디오 파일 처리</li> <li>장기간 상호작용에서 맥락 유지</li> </ul> <p>병목은? 어텐션 연산이 시퀀스 길이에 대해 제곱으로 증가하여 긴 맥락을 금지적으로 비싸게 만듭니다.</p> <h2 id="2-핵심-방법-세-가지-상승효과-혁신">2. 핵심 방법: 세 가지 상승효과 혁신</h2> <h3 id="혁신-1-생산자-소비자-비동기성">혁신 1: 생산자-소비자 비동기성</h3> <p><strong>통찰</strong>: 전통적인 어텐션은 데이터가 로드될 때까지 기다린 후 연산합니다. 현대 GPU는 동시에 로드하고 연산할 수 있지만, 작업을 다르게 구성해야 합니다.</p> <p><strong>Python 의사코드</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">import</span> <span class="n">asyncio</span>
<span class="kn">import</span> <span class="n">torch</span>

<span class="k">class</span> <span class="nc">AsyncAttention</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">data_loader_workers</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1"># 생산자 워프
</span>        <span class="n">self</span><span class="p">.</span><span class="n">compute_workers</span> <span class="o">=</span> <span class="p">[]</span>      <span class="c1"># 소비자 워프
</span>        <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span> <span class="o">=</span> <span class="nc">CircularBuffer</span><span class="p">(</span><span class="n">stages</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    
    <span class="k">async</span> <span class="k">def</span> <span class="nf">producer_worker</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">K_blocks</span><span class="p">,</span> <span class="n">V_blocks</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">공유 버퍼에 데이터를 지속적으로 로드</span><span class="sh">"""</span>
        <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="p">(</span><span class="n">K_i</span><span class="p">,</span> <span class="n">V_i</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">K_blocks</span><span class="p">,</span> <span class="n">V_blocks</span><span class="p">)):</span>
            <span class="c1"># 버퍼 슬롯이 비어있을 때까지 대기
</span>            <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">wait_for_slot</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
            
            <span class="c1"># 비동기적으로 데이터 로드 (블록하지 않음)
</span>            <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">gpu_load_async</span><span class="p">(</span><span class="n">K_i</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">slot</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">))</span>
            <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">gpu_load_async</span><span class="p">(</span><span class="n">V_i</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">slot</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">))</span>
            
            <span class="c1"># 데이터 준비 완료 신호
</span>            <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">mark_ready</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
    
    <span class="k">async</span> <span class="k">def</span> <span class="nf">consumer_worker</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Q_block</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">로드된 데이터에 대해 지속적으로 연산</span><span class="sh">"""</span>
        <span class="n">result</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">Q_block</span><span class="p">)</span>
        
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="nf">len</span><span class="p">(</span><span class="n">K_blocks</span><span class="p">)):</span>
            <span class="c1"># 데이터가 로드될 때까지 대기
</span>            <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">wait_for_data</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
            
            <span class="c1"># 다음 데이터가 백그라운드에서 로드되는 동안 연산
</span>            <span class="n">K_i</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">get_K</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
            <span class="n">V_i</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">get_V</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
            
            <span class="c1"># 어텐션 연산
</span>            <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">Q_block</span><span class="p">,</span> <span class="n">K_i</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span>
            <span class="n">probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
            <span class="n">result</span> <span class="o">+=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">matmul</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">V_i</span><span class="p">)</span>
            
            <span class="c1"># 슬롯이 재사용을 위해 비었다고 신호
</span>            <span class="n">self</span><span class="p">.</span><span class="n">shared_buffer</span><span class="p">.</span><span class="nf">mark_consumed</span><span class="p">(</span><span class="n">i</span> <span class="o">%</span> <span class="mi">3</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">result</span>
</code></pre></div></div> <p><strong>작동 원리 - 레스토랑 비유</strong>:</p> <ul> <li> <strong>전통적 접근</strong>: 셰프가 웨이터가 재료를 가져올 때까지 기다린 후 요리하고, 다시 기다림</li> <li> <strong>FlashAttention-3</strong>: 웨이터가 셰프가 사용 가능한 재료로 요리하는 동안 지속적으로 재료를 가져옴</li> <li> <strong>결과</strong>: 주방이 놀지 않고 최대 용량으로 운영됨</li> </ul> <p><strong>내 분석</strong>: 이는 현대 CPU의 파이프라이닝 작동 방식과 일치하지만 GPU 어텐션 연산에 적용한 점이 우아합니다. 핵심 통찰은 GPU 메모리 대역폭과 연산 능력을 순차적이 아닌 동시에 활용할 수 있다는 것입니다.</p> <h3 id="혁신-2-gemm-소프트맥스-중첩">혁신 2: GEMM-소프트맥스 중첩</h3> <p><strong>문제</strong>: H100 SXM5 GPU는 989 TFLOPS의 FP16 행렬곱을 가지지만 소프트맥스에 필요한 지수함수 같은 특수 함수는 단 3.9 TFLOPS입니다. 소프트맥스 연산(비싼 지수 함수 포함)이 빠른 행렬 곱셈을 막고 있었습니다.</p> <p><strong>Python 의사코드</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">PipelinedAttention</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">stage_current</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="n">self</span><span class="p">.</span><span class="n">stage_next</span> <span class="o">=</span> <span class="mi">1</span>
    
    <span class="k">async</span> <span class="k">def</span> <span class="nf">two_stage_pipeline</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">Q</span><span class="p">,</span> <span class="n">K_blocks</span><span class="p">,</span> <span class="n">V_blocks</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">QK^T, 소프트맥스, PV 연산을 중첩</span><span class="sh">"""</span>
        <span class="n">O</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">Q</span><span class="p">)</span>
        
        <span class="c1"># 단계 0: 첫 번째 연산 초기화
</span>        <span class="n">S_current</span> <span class="o">=</span> <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">async_matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K_blocks</span><span class="p">[</span><span class="mi">0</span><span class="p">].</span><span class="n">T</span><span class="p">)</span>
        <span class="k">await</span> <span class="n">self</span><span class="p">.</span><span class="nf">wait_completion</span><span class="p">()</span>
        
        <span class="n">m</span><span class="p">,</span> <span class="n">P_current</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_softmax_stats</span><span class="p">(</span><span class="n">S_current</span><span class="p">)</span>
        
        <span class="c1"># 메인 파이프라인 루프
</span>        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nf">len</span><span class="p">(</span><span class="n">K_blocks</span><span class="p">)</span> <span class="o">-</span> <span class="mi">1</span><span class="p">):</span>
            <span class="c1"># 파이프라인 단계 1: 다음 QK^T 시작 (기다리지 않음)
</span>            <span class="n">S_next_future</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">async_matmul</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K_blocks</span><span class="p">[</span><span class="n">i</span><span class="p">].</span><span class="n">T</span><span class="p">)</span>
            
            <span class="c1"># 파이프라인 단계 2: 현재 PV 시작 (기다리지 않음)  
</span>            <span class="n">O_update_future</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">async_matmul</span><span class="p">(</span><span class="n">P_current</span><span class="p">,</span> <span class="n">V_blocks</span><span class="p">[</span><span class="n">i</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
            
            <span class="c1"># 파이프라인 단계 3: QK^T 대기, 소프트맥스 연산
</span>            <span class="n">S_next</span> <span class="o">=</span> <span class="k">await</span> <span class="n">S_next_future</span>
            <span class="n">m</span><span class="p">,</span> <span class="n">P_next</span><span class="p">,</span> <span class="n">l</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_softmax_stats</span><span class="p">(</span><span class="n">S_next</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
            
            <span class="c1"># 파이프라인 단계 4: PV 대기, 출력 업데이트
</span>            <span class="n">O_update</span> <span class="o">=</span> <span class="k">await</span> <span class="n">O_update_future</span>
            <span class="n">O</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">rescale_and_add</span><span class="p">(</span><span class="n">O</span><span class="p">,</span> <span class="n">O_update</span><span class="p">,</span> <span class="n">m</span><span class="p">,</span> <span class="n">l</span><span class="p">)</span>
            
            <span class="c1"># 다음 반복을 위해 교체
</span>            <span class="n">P_current</span> <span class="o">=</span> <span class="n">P_next</span>
        
        <span class="k">return</span> <span class="n">O</span>
    
    <span class="k">def</span> <span class="nf">compute_softmax_stats</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">S</span><span class="p">,</span> <span class="n">m_old</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">l_old</span><span class="o">=</span><span class="bp">None</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">실행 통계를 사용한 수치적으로 안정적인 소프트맥스</span><span class="sh">"""</span>
        <span class="k">if</span> <span class="n">m_old</span> <span class="ow">is</span> <span class="bp">None</span><span class="p">:</span>
            <span class="n">m_old</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="n">S</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>
            <span class="n">l_old</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">S</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
        
        <span class="c1"># 행 최댓값 업데이트
</span>        <span class="n">m_new</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">maximum</span><span class="p">(</span><span class="n">m_old</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">S</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">])</span>
        
        <span class="c1"># 확률 계산 및 행 합계 업데이트
</span>        <span class="n">P</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">S</span> <span class="o">-</span> <span class="n">m_new</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">))</span>
        <span class="n">l_new</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">exp</span><span class="p">(</span><span class="n">m_old</span> <span class="o">-</span> <span class="n">m_new</span><span class="p">)</span> <span class="o">*</span> <span class="n">l_old</span> <span class="o">+</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">P</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">m_new</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">l_new</span>
</code></pre></div></div> <p><strong>조립 라인 비유</strong>:</p> <ul> <li> <strong>전통적</strong>: 자동차 조립이 각 스테이션에서 정지 - 엔진 설치, 그 다음 페인팅, 그 다음 시트 설치</li> <li> <strong>FlashAttention-3</strong>: 여러 자동차가 동시에 다른 스테이션에서 - A차가 페인팅되는 동안 B차는 엔진, C차는 시트</li> <li> <strong>결과</strong>: 같은 자원으로 3배 높은 처리량</li> </ul> <p><strong>숫자를 사용한 실제 예시</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 전통적 순차 타이밍
</span><span class="n">QK_time</span> <span class="o">=</span> <span class="mi">10</span><span class="n">ms</span>    <span class="c1"># 행렬 곱셈
</span><span class="n">Softmax_time</span> <span class="o">=</span> <span class="mi">5</span><span class="n">ms</span> <span class="c1"># 지수 연산  
</span><span class="n">PV_time</span> <span class="o">=</span> <span class="mi">10</span><span class="n">ms</span>    <span class="c1"># 행렬 곱셈
</span><span class="n">Total</span> <span class="o">=</span> <span class="mi">25</span><span class="n">ms</span> <span class="n">per</span> <span class="n">block</span>

<span class="c1"># 파이프라인 타이밍
# 초기 설정 후 세 연산 모두 동시 실행
# 병목은 가장 느린 연산 (10ms)
</span><span class="n">Pipeline_time</span> <span class="o">=</span> <span class="mi">10</span><span class="n">ms</span> <span class="n">per</span> <span class="n">block</span>
<span class="n">Speedup</span> <span class="o">=</span> <span class="mi">25</span><span class="o">/</span><span class="mi">10</span> <span class="o">=</span> <span class="mf">2.5</span><span class="n">x</span> <span class="n">이론적</span>
</code></pre></div></div> <p><strong>내 통찰</strong>: 이는 CPU 명령어 파이프라이닝과 유사하지만 고수준 수학적 연산에 적용된 것입니다. 천재적인 점은 다른 연산들이 다른 하드웨어 유닛을 사용하므로 동시에 실행될 수 있다는 것을 인식한 것입니다.</p> <h3 id="혁신-3-정확도-보존을-가진-fp8">혁신 3: 정확도 보존을 가진 FP8</h3> <p><strong>도전</strong>: FP8은 2배 속도를 제공하지만 일반적으로 수치 정확도를 파괴합니다. 대형 모델들은 일반적으로 대부분의 다른 값들보다 크기가 훨씬 큰 이상치 값들을 가지고 있어 양자화를 어렵게 만듭니다.</p> <p><strong>두 부분 해결책</strong>:</p> <h4 id="파트-a-블록-양자화">파트 A: 블록 양자화</h4> <p><strong>Python 의사코드</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">block_quantization</span><span class="p">(</span><span class="n">tensor</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">64</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">전체 텐서 대신 블록 단위로 양자화</span><span class="sh">"""</span>
    <span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="n">shape</span>
    
    <span class="c1"># 문제: 텐서별 양자화
</span>    <span class="c1"># tensor_max = tensor.abs().max()  # 이상치에 의해 지배됨!
</span>    <span class="c1"># scale = tensor_max / 127
</span>    <span class="c1"># 결과: 대부분의 값이 0 또는 ±1이 됨
</span>    
    <span class="c1"># 해결책: 블록별 양자화
</span>    <span class="n">blocks</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">block_size</span><span class="p">,</span> <span class="n">hidden</span><span class="p">)</span>
    
    <span class="c1"># 각 블록이 자체 스케일을 가짐
</span>    <span class="n">block_maxes</span> <span class="o">=</span> <span class="n">blocks</span><span class="p">.</span><span class="nf">abs</span><span class="p">().</span><span class="nf">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">].</span><span class="nf">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="bp">True</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">scales</span> <span class="o">=</span> <span class="n">block_maxes</span> <span class="o">/</span> <span class="mf">127.0</span>
    
    <span class="c1"># 각 블록을 별도로 양자화
</span>    <span class="n">quantized_blocks</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">blocks</span> <span class="o">/</span> <span class="n">scales</span><span class="p">).</span><span class="nf">clamp</span><span class="p">(</span><span class="o">-</span><span class="mi">127</span><span class="p">,</span> <span class="mi">127</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">quantized_blocks</span><span class="p">.</span><span class="nf">view</span><span class="p">(</span><span class="n">tensor</span><span class="p">.</span><span class="n">shape</span><span class="p">),</span> <span class="n">scales</span>

<span class="c1"># 이상치가 있는 예시
</span><span class="n">tensor</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1024</span><span class="p">,</span> <span class="mi">128</span><span class="p">)</span>
<span class="n">tensor</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="p">:</span><span class="mi">10</span><span class="p">,</span> <span class="p">:</span><span class="mi">10</span><span class="p">]</span> <span class="o">=</span> <span class="mf">50.0</span>  <span class="c1"># 50배 더 큰 1% 이상치
</span>
<span class="c1"># 텐서별 양자화 (나쁨)
</span><span class="n">global_scale</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">.</span><span class="nf">abs</span><span class="p">().</span><span class="nf">max</span><span class="p">()</span> <span class="o">/</span> <span class="mi">127</span>  <span class="c1"># = 50/127 = 0.39
</span><span class="n">quantized_global</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">round</span><span class="p">(</span><span class="n">tensor</span> <span class="o">/</span> <span class="n">global_scale</span><span class="p">)</span>
<span class="c1"># 결과: 일반 값들이 0, ±1, ±2가 됨 (끔찍한 정밀도)
</span>
<span class="c1"># 블록 양자화 (좋음) 
</span><span class="n">quantized_blocks</span><span class="p">,</span> <span class="n">scales</span> <span class="o">=</span> <span class="nf">block_quantization</span><span class="p">(</span><span class="n">tensor</span><span class="p">)</span>
<span class="c1"># 결과: 일반 블록은 전체 ±127 범위 사용, 이상치 블록은 별도 스케일
</span></code></pre></div></div> <h4 id="파트-b-비일관성-처리">파트 B: 비일관성 처리</h4> <p><strong>Python 의사코드</strong>:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">incoherent_processing</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">랜덤 직교 변환을 사용하여 이상치 분산</span><span class="sh">"""</span>
    <span class="n">d</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    
    <span class="c1"># 랜덤 직교 행렬 M = D1 * Hadamard * D2 생성
</span>    <span class="c1"># 핵심 통찰: M이 직교이므로 MM^T = I
</span>    <span class="c1"># 따라서 (Q*M) @ (K*M)^T = Q @ M @ M^T @ K^T = Q @ K^T
</span>    <span class="c1"># 어텐션 출력은 변하지 않지만 이상치가 분산됨!
</span>    
    <span class="n">D1</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">d</span><span class="p">,))</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>  <span class="c1"># 랜덤 ±1
</span>    <span class="n">D2</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="p">(</span><span class="n">d</span><span class="p">,))</span> <span class="o">*</span> <span class="mi">2</span> <span class="o">-</span> <span class="mi">1</span>
    
    <span class="k">def</span> <span class="nf">fast_hadamard_transform</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
        <span class="sh">"""</span><span class="s">O(d^2) 대신 O(d log d) 행렬 곱셈</span><span class="sh">"""</span>
        <span class="k">return</span> <span class="nf">hadamard_recursive</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1"># Walsh-Hadamard 변환
</span>    
    <span class="c1"># Q와 K 모두 변환
</span>    <span class="n">Q_transformed</span> <span class="o">=</span> <span class="nf">fast_hadamard_transform</span><span class="p">(</span><span class="n">Q</span> <span class="o">*</span> <span class="n">D1</span><span class="p">)</span> <span class="o">*</span> <span class="n">D2</span>
    <span class="n">K_transformed</span> <span class="o">=</span> <span class="nf">fast_hadamard_transform</span><span class="p">(</span><span class="n">K</span> <span class="o">*</span> <span class="n">D1</span><span class="p">)</span> <span class="o">*</span> <span class="n">D2</span>
    
    <span class="k">return</span> <span class="n">Q_transformed</span><span class="p">,</span> <span class="n">K_transformed</span>

<span class="c1"># 이상치가 있을 때 작동하는 이유 - 예시
</span><span class="n">original_Q</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">tensor</span><span class="p">([[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">]])</span>  <span class="c1"># 하나의 거대한 이상치
# 변환 후: 각 요소가 원래 요소들의 합이 됨
# transformed_Q ≈ [[34, 34, 34]]  # 이상치가 모든 차원에 분산됨
# 양자화하기 훨씬 쉬워짐!
</span></code></pre></div></div> <p><strong>복권 비유</strong>:</p> <ul> <li> <strong>문제</strong>: 한 사람이 100만원, 999명이 0원 (공정하게 표현하기 어려움)</li> <li> <strong>해결책</strong>: 재분배하여 모든 사람이 1000원씩 (공정하게 표현하기 쉬움)</li> <li> <strong>핵심</strong>: 총 가치는 변하지 않지만 분포가 더 균일함</li> </ul> <p><strong>내 평가</strong>: 이는 수학적으로 우아합니다 - 직교 행렬의 성질을 사용하여 최종 결과를 보존하면서 중간 연산을 양자화에 더 친화적으로 만듭니다. 수학적 손재주와 같습니다.</p> <h2 id="3-중요한-성공-조건">3. 중요한 성공 조건</h2> <h3 id="하드웨어-요구사항">하드웨어 요구사항</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">HardwareChecklist</span><span class="p">:</span>
    <span class="n">required_features</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">hopper_gpu</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Hopper 아키텍처를 가진 H100 이상</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">async_tensor_cores</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">WGMMA 명령어 지원</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">tensor_memory_accelerator</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">비동기 메모리 연산을 위한 TMA</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">fp8_support</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">E4M3 형식 텐서 코어</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">shared_memory</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">SM당 최소 228KB</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">register_flexibility</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">동적 레지스터 할당</span><span class="sh">'</span>
    <span class="p">}</span>
    
    <span class="k">def</span> <span class="nf">check_compatibility</span><span class="p">(</span><span class="n">self</span><span class="p">):</span>
        <span class="c1"># 실제로는 실제 하드웨어를 쿼리할 것
</span>        <span class="k">for</span> <span class="n">feature</span><span class="p">,</span> <span class="n">description</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">required_features</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
            <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">✓ </span><span class="si">{</span><span class="n">feature</span><span class="si">}</span><span class="s">: </span><span class="si">{</span><span class="n">description</span><span class="si">}</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><strong>내 분석</strong>: 이는 강점이자 제한사항입니다. 최적화들이 특정 하드웨어에 깊이 연결되어 H100에서는 믿을 수 없이 효과적이지만 다른 가속기로의 이식성은 떨어질 수 있습니다.</p> <h3 id="알고리즘-가정">알고리즘 가정</h3> <ol> <li> <p><strong>메모리 대역폭 균형</strong>: 생산자 워프가 메모리 대역폭을 포화시킬 수 있어야 함</p> <ul> <li> <strong>함정</strong>: 데이터 로딩이 연산 대비 너무 빠르면 생산자가 유휴 상태가 됨</li> <li> <strong>해결책</strong>: 블록 크기와 파이프라인 단계 수의 신중한 조정</li> </ul> </li> <li> <p><strong>파이프라인 단계 타이밍</strong>: GEMM과 소프트맥스 연산이 유사한 타이밍을 가져야 함</p> <ul> <li> <strong>함정</strong>: 소프트맥스가 너무 빠르면 중첩 이득 없음; 너무 느리면 병목이 됨</li> <li> <strong>성공</strong>: H100의 GEMM과 지수함수 간 256배 속도 차이가 이를 가능하게 함</li> </ul> </li> <li> <p><strong>컴파일러 협력</strong>: 컴파일러가 의도된 명령어 순서를 생성해야 함</p> <ul> <li> <strong>함정</strong>: 컴파일러가 명령어를 재배열하여 파이프라인을 깨뜨릴 수 있음</li> <li> <strong>해결책</strong>: 메모리 장벽과 인라인 어셈블리 힌트의 신중한 사용</li> </ul> </li> </ol> <h2 id="4-실험-결과-분석">4. 실험 결과 분석</h2> <h3 id="주요-성능-결과">주요 성능 결과</h3> <table> <thead> <tr> <th>구성</th> <th>FlashAttention-2</th> <th>FlashAttention-3</th> <th>가속비</th> <th>내 해석</th> </tr> </thead> <tbody> <tr> <td><strong>헤드차원 64, 16k seq</strong></td> <td>324 TFLOPs/s</td> <td>497 TFLOPs/s</td> <td><strong>1.53×</strong></td> <td>메모리 바운드 영역이 가장 많은 이득</td> </tr> <tr> <td><strong>헤드차원 128, 16k seq</strong></td> <td>370 TFLOPs/s</td> <td>648 TFLOPs/s</td> <td><strong>1.75×</strong></td> <td>최적화의 스위트 스팟</td> </tr> <tr> <td><strong>헤드차원 256, 16k seq</strong></td> <td>581 TFLOPs/s</td> <td>756 TFLOPs/s</td> <td><strong>1.30×</strong></td> <td>연산 바운드, 상대적 이득 적음</td> </tr> <tr> <td><strong>최고 활용률</strong></td> <td>35%</td> <td><strong>75%</strong></td> <td><strong>2.1×</strong></td> <td>하드웨어가 마침내 효과적으로 사용됨</td> </tr> </tbody> </table> <p><strong>내 분석</strong>: 결과는 논리적 패턴을 따릅니다 - 더 작은 헤드 차원은 더 메모리 바운드이므로 메모리 최적화가 더 도움이 됩니다. 75% 활용률은 놀랍습니다; 비교하자면, 복잡한 시스템에서 75% 효율성을 얻는 것은 뛰어납니다.</p> <h3 id="fp8-성능-돌파">FP8 성능 돌파</h3> <table> <thead> <tr> <th>정밀도</th> <th>최고 성능</th> <th>정확도 (RMSE)</th> <th>내 평가</th> </tr> </thead> <tbody> <tr> <td><strong>FP16 표준</strong></td> <td>139 TFLOPs/s</td> <td>3.2e-4</td> <td>기준 정확도</td> </tr> <tr> <td><strong>FP16 FlashAttention-3</strong></td> <td><strong>648 TFLOPs/s</strong></td> <td><strong>1.9e-4</strong></td> <td>더 빠른 속도 AND 정확도</td> </tr> <tr> <td><strong>FP8 표준</strong></td> <td>~800 TFLOPs/s</td> <td>2.4e-2</td> <td>빠르지만 사용할 수 없을 정도로 부정확</td> </tr> <tr> <td><strong>FP8 FlashAttention-3</strong></td> <td><strong>1008 TFLOPs/s</strong></td> <td><strong>9.1e-3</strong></td> <td>빠르고 사용 가능한 정확도</td> </tr> </tbody> </table> <p><strong>핵심 통찰</strong>: 표준 FP8은 FP16보다 75배 더 나쁜 정확도 - 완전히 사용 불가능합니다. FlashAttention-3의 FP8은 48배만 더 나쁨 - 여전히 완벽하지 않지만 많은 애플리케이션에서 잠재적으로 사용 가능합니다.</p> <h3 id="제거-연구-결과">제거 연구 결과</h3> <table> <thead> <tr> <th>구성</th> <th>시간 (ms)</th> <th>TFLOPs/s</th> <th>구성요소</th> </tr> </thead> <tbody> <tr> <td><strong>기준선</strong></td> <td>4.105</td> <td>570</td> <td>최적화 없음</td> </tr> <tr> <td><strong>+ 워프 특화</strong></td> <td>4.021</td> <td>582</td> <td>+2% 개선</td> </tr> <tr> <td><strong>+ 파이프라이닝만</strong></td> <td>4.105</td> <td>570</td> <td>단독으로는 개선 없음</td> </tr> <tr> <td><strong>+ 둘 다</strong></td> <td>3.538</td> <td><strong>661</strong></td> <td><strong>+16% 총합</strong></td> </tr> </tbody> </table> <p><strong>중요한 통찰</strong>: 이는 가산적이 아닌 <strong>시너지 효과</strong>를 보여줍니다. 파이프라이닝 단독으로는 0% 이득을 주는데, 이는 워프 특화를 기반으로 필요하기 때문입니다. 이는 시스템 최적화에서 흔한 일입니다 - 개별 기술들은 효과적이지 않아 보일 수 있지만, 조합은 변혁적일 수 있습니다.</p> <p><strong>내 해석</strong>: 이는 “시스템 사고” 접근법을 검증합니다. 저자들은 개별 구성요소만 최적화한 것이 아니라 전체 연산 파이프라인을 재설계했습니다. 7.6배 시너지 인수(16% 조합 vs 2% 개별)는 아키텍처 변경이 가산적이 아닌 곱셈적 이득을 어떻게 풀어낼 수 있는지 보여줍니다.</p> <h2 id="5-더-넓은-의미와-미래-임팩트">5. 더 넓은 의미와 미래 임팩트</h2> <h3 id="즉각적인-애플리케이션">즉각적인 애플리케이션</h3> <ul> <li> <strong>장문맥 언어 모델</strong>: 전체 책, 코드베이스, 또는 대화 처리</li> <li> <strong>고해상도 비전</strong>: 의료 이미지나 위성 데이터 분석</li> <li> <strong>멀티모달 AI</strong>: 오디오와 텍스트가 있는 긴 비디오 처리</li> <li> <strong>과학 컴퓨팅</strong>: 어텐션 메커니즘을 가진 분자 역학, 기후 모델링</li> </ul> <h3 id="아키텍처-교훈">아키텍처 교훈</h3> <ol> <li> <strong>하드웨어-소프트웨어 공동 설계의 중요성</strong>: 범용 알고리즘은 성능을 탁자 위에 남겨둠</li> <li> <strong>비동기성이 핵심</strong>: 현대 하드웨어는 병렬 실행을 위해 설계됨</li> <li> <strong>정밀도는 협상 가능</strong>: 모든 연산이 완전한 정밀도를 필요로 하지 않음</li> <li> <strong>시스템 최적화</strong>: 전체가 부분의 합보다 클 수 있음</li> </ol> <p><strong>내 최종 생각</strong>: FlashAttention-3는 분야의 성숙을 나타냅니다 - “작동하게 만들기”에서 “최적으로 작동하게 만들기”로의 이동입니다. 여기의 기술들은 미래 가속기를 위한 알고리즘 설계 방식에 영향을 줄 것 같습니다. 가장 중요한 것은, 어텐션 같은 잘 연구된 문제에서도 영리한 알고리즘 설계를 통해 상당한 성능 향상이 여전히 가능하다는 것을 보여준다는 점입니다.</p> <p>75% GPU 활용률 달성은 특히 주목할 만합니다 - 이는 우리가 마침내 비효율적인 알고리즘에 더 많은 하드웨어를 투입하는 대신, 우리가 구축한 강력한 하드웨어를 효과적으로 사용하는 방법을 배우고 있음을 시사합니다.</p> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa2/">Flash Attention 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa/">Flash Attention</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/usp/">Unified Sequence Parallelism</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/deepspeed_ulysses/">DeepSpeed Ulysses</a> </li> <div id="disqus_thread" style="max-width: 930px; margin: 0 auto;"> <script type="text/javascript">
    var disqus_shortname  = 'sungyubkim';
    var disqus_identifier = '/blog/fa3';
    var disqus_title      = "Flash Attention 3";
    (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Sung-Yub Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 29, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>