<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Mixture of Experts | Sung-Yub Kim </title> <meta name="author" content="Sung-Yub Kim"> <meta name="description" content="Sparsely-Gated Mixture-of-Experts (MoE) enables 1000x parameter scaling with minimal compute overhead."> <meta name="keywords" content="machine-learning, deep-learning, llm, foundation-models, research"> <meta property="og:site_name" content="Sung-Yub Kim"> <meta property="og:type" content="article"> <meta property="og:title" content="Sung-Yub Kim | Mixture of Experts"> <meta property="og:url" content="https://sungyubkim.github.io/blog/moe/"> <meta property="og:description" content="Sparsely-Gated Mixture-of-Experts (MoE) enables 1000x parameter scaling with minimal compute overhead."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Mixture of Experts"> <meta name="twitter:description" content="Sparsely-Gated Mixture-of-Experts (MoE) enables 1000x parameter scaling with minimal compute overhead."> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Sung-Yub Kim"
        },
        "url": "https://sungyubkim.github.io/blog/moe/",
        "@type": "BlogPosting",
        "description": "Sparsely-Gated Mixture-of-Experts (MoE) enables 1000x parameter scaling with minimal compute overhead.",
        "headline": "Mixture of Experts",
        
        "sameAs": ["https://github.com/sungyubkim","https://scholar.google.com/citations?user=m2rhgrkAAAAJ","https://www.linkedin.com/in/sung-yub-kim-0a82a1264","https://twitter.com/SungyubK"],
        
        "name": "Sung-Yub Kim",
        "@context": "https://schema.org"
    }
  </script> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sungyubkim.github.io/blog/moe/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Sung-Yub</span> Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Mixture of Experts</h1> <p class="post-meta"> Created on May 31, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/mixture-of-experts"> <i class="fa-solid fa-hashtag fa-sm"></i> mixture-of-experts</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="tldr">TL;DR</h1> <blockquote> <p>이 2017년 기념비적 논문은 <strong>희소 게이팅 전문가 혼합(Sparsely-Gated Mixture-of-Experts, MoE)</strong>을 도입하여 신경망 확장의 근본적 과제를 해결했습니다. 훈련 가능한 게이팅 네트워크가 전문가 서브네트워크를 희소하게 활성화하는 방법으로 1000배 이상의 매개변수 증가를 최소한의 계산 오버헤드로 달성합니다. 정교한 부하 분산과 분산 훈련 솔루션이 결합되었습니다. 언어 모델링(24-39% 향상된 perplexity)과 기계 번역(+1-1.3 BLEU)에서 상당한 개선을 보여주지만, 효율성 주장은 과장되었고 상당한 인프라 복잡성을 수반합니다. 이 연구의 지속적인 영향은 조건부 계산이 대규모로 작동할 수 있음을 증명하여 오늘날의 최대 언어 모델을 구동하는 희소 신경 아키텍처의 현대적 시대를 열었다는 것입니다.</p> </blockquote> <ul> <li>Paper Link: <a href="https://arxiv.org/pdf/1701.06538" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/1701.06538</a> </li> </ul> <hr> <h1 id="related-papers">Related Papers</h1> <p><strong>전문가 혼합 아키텍처:</strong></p> <ul> <li> <a href="https://arxiv.org/pdf/2101.03961" rel="external nofollow noopener" target="_blank">Switch Transformers</a> - 단순화된 MoE 라우팅으로 조 단위 매개변수 달성</li> <li> <a href="/blog/pp/">GPipe</a> - MoE와 결합 가능한 파이프라인 병렬화</li> </ul> <p><strong>분산 훈련:</strong></p> <ul> <li> <a href="/blog/tp/">Tensor Parallelism</a> - MoE와 호환되는 텐서 병렬화 기법</li> <li> <a href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> - 대규모 MoE 훈련을 위한 메모리 최적화</li> </ul> <p><strong>확장성 및 효율성:</strong></p> <ul> <li> <a href="https://arxiv.org/pdf/2211.05102" rel="external nofollow noopener" target="_blank">Efficiently Scaling Transformer Inference</a> - MoE 모델의 효율적 추론</li> <li> <a href="/blog/usp/">USP</a> - MoE와 결합 가능한 시퀀스 병렬화</li> </ul> <hr> <h1 id="takeaways">Takeaways</h1> <h2 id="문제-신경망-확장이-벽에-부딪히다">문제: 신경망 확장이 벽에 부딪히다</h2> <p>신경망은 간단한 원리를 따릅니다: 더 많은 매개변수 = 더 나은 성능. 하지만 전통적인 확장은 이차적 비용 증가로 이어집니다. 모델 크기를 두 배로 늘리고 훈련 데이터를 두 배로 늘리면 계산 비용이 4배 증가합니다. 이는 빠르게 지속 불가능해집니다.</p> <p><strong>조건부 계산</strong>은 해결책을 약속했습니다 - 서로 다른 입력에 대해 네트워크의 다른 부분을 활성화하여 계산량은 일정하게 유지하면서 용량을 대폭 증가시키는 것입니다. 하지만 수십 년의 이론적 연구에도 불구하고, 근본적인 과제들로 인해 아무도 대규모로 작동시키지 못했습니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 핵심 확장 문제
</span><span class="n">전통적_비용</span> <span class="o">=</span> <span class="n">모델_크기</span> <span class="o">*</span> <span class="n">훈련_데이터</span> <span class="o">*</span> <span class="n">매개변수당_계산량</span>
<span class="c1"># 모델_크기와 훈련_데이터가 모두 증가하면 비용이 이차적으로 폭증
</span></code></pre></div></div> <p><strong>주요 장벽들:</strong></p> <ul> <li> <strong>GPU 비효율성</strong>: GPU는 분기가 아닌 밀집 계산에 최적화됨</li> <li> <strong>배치 크기 감소</strong>: 조건부 활성화가 각 구성 요소의 효과적 배치 크기를 줄임</li> <li> <strong>네트워크 대역폭</strong>: 매개변수가 분산될 때 통신 비용이 지배적</li> <li> <strong>부하 분산</strong>: 모델이 몇 개의 “선호하는” 구성 요소만 사용하여 용량을 낭비하는 경향</li> </ul> <h2 id="해결책-희소-게이팅-전문가-혼합">해결책: 희소 게이팅 전문가 혼합</h2> <p>저자들의 돌파구는 이 모든 과제를 동시에 해결하는 완전한 시스템을 만든 것이었습니다. 핵심 혁신은 신경망 어디에나 임베드될 수 있는 <strong>MoE 층</strong>입니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SparseMoELayer</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">num_experts</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">4</span><span class="p">):</span>
        <span class="c1"># 게이팅 네트워크가 어떤 전문가를 사용할지 결정
</span>        <span class="n">self</span><span class="p">.</span><span class="n">gating_network</span> <span class="o">=</span> <span class="nc">GatingNetwork</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">num_experts</span><span class="p">)</span>
        
        <span class="c1"># 수천 개의 전문가 서브네트워크
</span>        <span class="n">self</span><span class="p">.</span><span class="n">experts</span> <span class="o">=</span> <span class="p">[</span><span class="nc">FeedForward</span><span class="p">(</span><span class="n">d_model</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_experts</span><span class="p">)]</span>
        
        <span class="c1"># 입력당 상위 k개 전문가만 활성화
</span>        <span class="n">self</span><span class="p">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">k</span>
        
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1"># 1. 게이팅 네트워크가 입력당 k개 전문가 선택
</span>        <span class="n">gate_probs</span><span class="p">,</span> <span class="n">selected_experts</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">gating_network</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="n">self</span><span class="p">.</span><span class="n">k</span><span class="p">)</span>
        
        <span class="c1"># 2. 선택된 전문가만 계산 (핵심 효율성 이득)
</span>        <span class="n">expert_outputs</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">compute_selected_experts</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">selected_experts</span><span class="p">)</span>
        
        <span class="c1"># 3. 전문가 출력의 가중 조합
</span>        <span class="n">output</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">combine_outputs</span><span class="p">(</span><span class="n">expert_outputs</span><span class="p">,</span> <span class="n">gate_probs</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">output</span>

<span class="c1"># 예시: 1000개 전문가와 k=4로 1000배 용량을 얻지만
# 입력당 계산량은 4배만 증가
</span></code></pre></div></div> <h2 id="성공을-가능하게-한-기술적-혁신들">성공을 가능하게 한 기술적 혁신들</h2> <h3 id="1-노이즈-top-k-게이팅-희소하지만-미분-가능">1. 노이즈 Top-K 게이팅: 희소하지만 미분 가능</h3> <p>게이팅 네트워크는 시스템의 핵심입니다. 훈련 가능성을 유지하면서 어떤 전문가를 사용할지 선택해야 합니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">noisy_top_k_gating</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">w_gate</span><span class="p">,</span> <span class="n">w_noise</span><span class="p">,</span> <span class="n">k</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="bp">True</span><span class="p">):</span>
    <span class="c1"># 기본 게이팅 점수
</span>    <span class="n">gate_logits</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">w_gate</span>  <span class="c1"># [batch, num_experts]
</span>    
    <span class="k">if</span> <span class="n">training</span><span class="p">:</span>
        <span class="c1"># 부하 분산을 위한 노이즈 추가 (중요한 혁신!)
</span>        <span class="n">noise</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">randn_like</span><span class="p">(</span><span class="n">gate_logits</span><span class="p">)</span> <span class="o">*</span> <span class="nf">softplus</span><span class="p">(</span><span class="n">x</span> <span class="o">@</span> <span class="n">w_noise</span><span class="p">)</span>
        <span class="n">gate_logits</span> <span class="o">=</span> <span class="n">gate_logits</span> <span class="o">+</span> <span class="n">noise</span>
    
    <span class="c1"># 상위 k개 전문가 선택
</span>    <span class="n">top_k_logits</span><span class="p">,</span> <span class="n">top_k_indices</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">topk</span><span class="p">(</span><span class="n">gate_logits</span><span class="p">,</span> <span class="n">k</span><span class="p">)</span>
    
    <span class="c1"># 희소 마스크 생성: 나머지를 음의 무한대로 설정
</span>    <span class="n">sparse_logits</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">full_like</span><span class="p">(</span><span class="n">gate_logits</span><span class="p">,</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>
    <span class="n">sparse_logits</span><span class="p">.</span><span class="nf">scatter_</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">top_k_indices</span><span class="p">,</span> <span class="n">top_k_logits</span><span class="p">)</span>
    
    <span class="c1"># 소프트맥스가 최종 전문가 가중치 제공
</span>    <span class="n">gate_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">softmax</span><span class="p">(</span><span class="n">sparse_logits</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">gate_probs</span><span class="p">,</span> <span class="n">top_k_indices</span>

<span class="c1"># 핵심 통찰: 노이즈는 단순한 탐색이 아니라 
# 부하 분산을 위한 이산적 전문가 선택을 미분 가능하게 만듦
</span></code></pre></div></div> <h3 id="2-하이브리드-분산-훈련-배치-크기-문제-해결">2. 하이브리드 분산 훈련: 배치 크기 문제 해결</h3> <p>전통적인 데이터 병렬처리는 각 전문가에게 작은 배치를 줍니다. 해결책은 데이터와 모델 병렬처리를 결합하는 것입니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 대신에: 각 디바이스가 batch_size/num_devices 예제를 받음
# 이렇게 하기: 전문가 계산을 위해 디바이스 간 배치 결합
</span>
<span class="k">def</span> <span class="nf">distributed_moe_forward</span><span class="p">(</span><span class="n">inputs_per_device</span><span class="p">,</span> <span class="n">expert_assignments</span><span class="p">):</span>
    <span class="c1"># 1. 각 디바이스가 독립적으로 게이팅 계산 (데이터 병렬)
</span>    <span class="n">local_gates</span> <span class="o">=</span> <span class="p">[</span><span class="nf">compute_gates</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span> <span class="k">for</span> <span class="n">inp</span> <span class="ow">in</span> <span class="n">inputs_per_device</span><span class="p">]</span>
    
    <span class="c1"># 2. 각 전문가가 필요한 모든 입력 수집 (모델 병렬)
</span>    <span class="n">expert_batches</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">device_id</span><span class="p">,</span> <span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">gates</span><span class="p">)</span> <span class="ow">in</span> <span class="nf">enumerate</span><span class="p">(</span><span class="nf">zip</span><span class="p">(</span><span class="n">inputs_per_device</span><span class="p">,</span> <span class="n">local_gates</span><span class="p">)):</span>
        <span class="k">for</span> <span class="n">expert_id</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_experts</span><span class="p">):</span>
            <span class="n">selected_inputs</span> <span class="o">=</span> <span class="n">inputs</span><span class="p">[</span><span class="n">gates</span><span class="p">.</span><span class="n">selected_experts</span> <span class="o">==</span> <span class="n">expert_id</span><span class="p">]</span>
            <span class="k">if</span> <span class="n">expert_id</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">expert_batches</span><span class="p">:</span>
                <span class="n">expert_batches</span><span class="p">[</span><span class="n">expert_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">[]</span>
            <span class="n">expert_batches</span><span class="p">[</span><span class="n">expert_id</span><span class="p">].</span><span class="nf">append</span><span class="p">(</span><span class="n">selected_inputs</span><span class="p">)</span>
    
    <span class="c1"># 3. 각 전문가가 모든 디바이스의 결합된 배치 처리
</span>    <span class="n">expert_outputs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">expert_id</span><span class="p">,</span> <span class="n">batched_inputs</span> <span class="ow">in</span> <span class="n">expert_batches</span><span class="p">.</span><span class="nf">items</span><span class="p">():</span>
        <span class="n">combined_batch</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">cat</span><span class="p">(</span><span class="n">batched_inputs</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
        <span class="n">expert_outputs</span><span class="p">[</span><span class="n">expert_id</span><span class="p">]</span> <span class="o">=</span> <span class="n">experts</span><span class="p">[</span><span class="n">expert_id</span><span class="p">](</span><span class="n">combined_batch</span><span class="p">)</span>
    
    <span class="c1"># 4. 결과를 디바이스로 다시 분산
</span>    <span class="k">return</span> <span class="nf">scatter_outputs_to_devices</span><span class="p">(</span><span class="n">expert_outputs</span><span class="p">)</span>

<span class="c1"># 결과: 전문가 배치 크기 = k * 총_배치_크기 / 전문가_수
# d개 디바이스로: k * d * 로컬_배치_크기 / 전문가_수
</span></code></pre></div></div> <h3 id="3-부하-분산-전문가-붕괴-방지">3. 부하 분산: 전문가 붕괴 방지</h3> <p>개입 없이는 모델이 몇 개의 전문가만 사용하도록 학습하여 용량을 낭비합니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">compute_load_balancing_losses</span><span class="p">(</span><span class="n">gate_probs</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># 손실 1: 동등한 중요도 장려 (게이트 가중치 합계)
</span>    <span class="n">importance</span> <span class="o">=</span> <span class="n">gate_probs</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 배치에 대한 합계
</span>    <span class="n">importance_loss</span> <span class="o">=</span> <span class="nf">coefficient_of_variation</span><span class="p">(</span><span class="n">importance</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    
    <span class="c1"># 손실 2: 동등한 부하 장려 (전문가당 예제 수)
</span>    <span class="c1"># 전문가 선택이 이산적이므로 더 복잡함
</span>    <span class="n">load_estimator</span> <span class="o">=</span> <span class="nf">estimate_load_differentiably</span><span class="p">(</span><span class="n">gate_probs</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
    <span class="n">load_loss</span> <span class="o">=</span> <span class="nf">coefficient_of_variation</span><span class="p">(</span><span class="n">load_estimator</span><span class="p">)</span> <span class="o">**</span> <span class="mi">2</span>
    
    <span class="k">return</span> <span class="n">importance_loss</span><span class="p">,</span> <span class="n">load_loss</span>

<span class="k">def</span> <span class="nf">estimate_load_differentiably</span><span class="p">(</span><span class="n">gate_probs</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># 이산적 전문가 할당의 부드러운 추정기
</span>    <span class="c1"># 게이팅의 노이즈를 사용하여 이를 미분 가능하게 만듦
</span>    <span class="n">load_probs</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">gate_probs</span><span class="p">)</span>
    
    <span class="k">for</span> <span class="n">expert_idx</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_experts</span><span class="p">):</span>
        <span class="c1"># 각 예제에 대해 이 전문가가 선택될 확률
</span>        <span class="c1"># 현재 게이트 값과 노이즈 분포에 기반
</span>        <span class="n">other_experts</span> <span class="o">=</span> <span class="n">gate_probs</span><span class="p">[:,</span> <span class="n">others</span><span class="p">]</span>
        <span class="n">kth_largest_other</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">kthvalue</span><span class="p">(</span><span class="n">other_experts</span><span class="p">,</span> <span class="n">k</span><span class="o">-</span><span class="mi">1</span><span class="p">)[</span><span class="mi">0</span><span class="p">]</span>
        
        <span class="c1"># k번째로 큰 경쟁자를 이길 확률
</span>        <span class="n">prob_selected</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">sigmoid</span><span class="p">(</span>
            <span class="p">(</span><span class="n">gate_probs</span><span class="p">[:,</span> <span class="n">expert_idx</span><span class="p">]</span> <span class="o">-</span> <span class="n">kth_largest_other</span><span class="p">)</span> <span class="o">/</span> <span class="n">noise_std</span>
        <span class="p">)</span>
        <span class="n">load_probs</span><span class="p">[:,</span> <span class="n">expert_idx</span><span class="p">]</span> <span class="o">=</span> <span class="n">prob_selected</span>
    
    <span class="k">return</span> <span class="n">load_probs</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>  <span class="c1"># 전문가당 예상 부하
</span></code></pre></div></div> <h2 id="실험-결과-인상적이지만-복잡함">실험 결과: 인상적이지만 복잡함</h2> <h3 id="주요-성능-결과">주요 성능 결과</h3> <table> <thead> <tr> <th>작업</th> <th>기준 모델</th> <th>MoE 모델</th> <th>개선</th> <th>숨겨진 비용</th> </tr> </thead> <tbody> <tr> <td><strong>1B 단어 LM</strong></td> <td>45.0 perplexity (9.4M 매개변수)</td> <td> <strong>34.1 perplexity</strong> (4.3B 매개변수)</td> <td><strong>24% 향상</strong></td> <td>430배 더 많은 매개변수</td> </tr> <tr> <td><strong>100B 단어 LM</strong></td> <td>47.0 perplexity (8.4M 매개변수)</td> <td> <strong>28.9 perplexity</strong> (68.8B 매개변수)</td> <td><strong>39% 향상</strong></td> <td>8200배 더 많은 매개변수</td> </tr> <tr> <td><strong>WMT’14 En→Fr</strong></td> <td>39.22 BLEU (278M 매개변수)</td> <td> <strong>40.56 BLEU</strong> (8.7B 매개변수)</td> <td><strong>+1.34 BLEU</strong></td> <td>31배 더 많은 매개변수</td> </tr> <tr> <td><strong>WMT’14 En→De</strong></td> <td>24.91 BLEU (278M 매개변수)</td> <td> <strong>26.03 BLEU</strong> (8.7B 매개변수)</td> <td><strong>+1.12 BLEU</strong></td> <td>31배 더 많은 매개변수</td> </tr> </tbody> </table> <p><strong>핵심 통찰</strong>: 개선은 실제적이고 의미 있지만, 대규모 매개변수 증가와 함께 옵니다. “동일한 계산, 더 나은 결과”가 아니라 “대폭 늘어난 매개변수, 더 나은 결과”입니다.</p> <h3 id="부하-분산-절제-연구">부하 분산 절제 연구</h3> <table> <thead> <tr> <th>구성</th> <th>테스트 Perplexity</th> <th>전문가 균형</th> <th>부하 균형</th> <th>해석</th> </tr> </thead> <tbody> <tr> <td><strong>부하 손실 없음</strong></td> <td>39.8</td> <td>매우 나쁨 (CV=3.04)</td> <td>매우 나쁨 (17.8배 불균형)</td> <td> <strong>완전한 실패</strong> - 모델이 ~3개 전문가 사용</td> </tr> <tr> <td><strong>중요도 손실만</strong></td> <td>35.6</td> <td>좋음 (CV=0.06)</td> <td>나쁨 (1.47배 불균형)</td> <td>게이트 가중치는 균형 맞지만 실제 부하는 안 맞음</td> </tr> <tr> <td><strong>부하 손실만</strong></td> <td>35.7</td> <td>보통 (CV=0.22)</td> <td>좋음 (1.15배 불균형)</td> <td>분산 효율성에 더 좋음</td> </tr> <tr> <td><strong>두 손실 모두</strong></td> <td>35.6</td> <td>좋음 (CV=0.06)</td> <td>좋음 (1.14배 불균형)</td> <td> <strong>전체적으로 최고</strong> - 두 측면 모두 균형</td> </tr> </tbody> </table> <p><strong>중요한 발견</strong>: 두 손실 함수 모두 필수적입니다. 이것들 없이는 모델이 완전히 실패합니다 (39.8 vs 35.6 perplexity).</p> <h3 id="계산-효율성-현실-점검">계산 효율성 현실 점검</h3> <table> <thead> <tr> <th>모델 유형</th> <th>주장된 이득</th> <th>측정된 TFLOPS/GPU</th> <th>현실</th> </tr> </thead> <tbody> <tr> <td><strong>밀집 기준선</strong></td> <td>-</td> <td>1.07-1.29</td> <td>깔끔한 기준선</td> </tr> <tr> <td><strong>희소 MoE (낮은 계산)</strong></td> <td>“동일한 계산”</td> <td><strong>0.74-0.90</strong></td> <td><strong>실제로는 덜 효율적!</strong></td> </tr> <tr> <td><strong>희소 MoE (높은 계산)</strong></td> <td>“더 나은 효율성”</td> <td>1.56</td> <td>더 나음, 하지만 더 큰 행렬이 도움</td> </tr> </tbody> </table> <p><strong>현실 점검</strong>: 효율성 주장은 의문스럽습니다. 낮은 계산 MoE 모델들은 실제로 밀집 기준선보다 덜 효율적이며, 아마도 분기 오버헤드와 통신 비용 때문일 것입니다.</p> <h2 id="비판적-평가-주의사항이-있는-돌파구">비판적 평가: 주의사항이 있는 돌파구</h2> <h3 id="진정한-돌파구들">진정한 돌파구들</h3> <ol> <li> <strong>조건부 계산이 대규모로 작동함을 증명</strong> - 대규모 희소 네트워크의 첫 번째 성공적 실증</li> <li> <strong>실용적 과제를 위한 엔지니어링 솔루션</strong> - 부하 분산, 분산 훈련, GPU 효율성</li> <li> <strong>여러 작업에서 일관된 개선</strong> - 언어 모델링, 기계 번역, 다국어 설정</li> <li> <strong>전문가 특수화가 자연스럽게 나타남</strong> - 모델이 구문적, 의미적 특수화를 학습</li> </ol> <h3 id="상당한-한계들">상당한 한계들</h3> <ol> <li> <strong>불공정한 매개변수 비교</strong> - 430배-8200배 매개변수 증가가 “효율성” 주장을 오해의 소지가 있게 만듦</li> <li> <strong>인프라 복잡성이 숨겨짐</strong> - 메모리 요구사항, 분산 설정 비용이 충분히 고려되지 않음</li> <li> <strong>하드웨어 비교 문제</strong> - 다른 GPU 세대들이 훈련 시간 비교를 무효화함</li> <li> <strong>통계적 엄밀성 부족</strong> - 오차 막대, 유의성 검정, 또는 여러 무작위 시드 없음</li> </ol> <h3 id="결과가-실제로-보여주는-것">결과가 실제로 보여주는 것</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 그들이 주장하는 것:
</span><span class="sh">"</span><span class="s">최소한의 계산 오버헤드로 1000배 모델 용량</span><span class="sh">"</span>

<span class="c1"># 그들이 실제로 실증하는 것:
</span><span class="sh">"</span><span class="s">1000배 매개변수가 의미 있는 개선을 줄 수 있지만, 다음과 함께:
 - 상당한 메모리/저장 오버헤드
 - 복잡한 분산 훈련 요구사항  
 - 의문스러운 계산 효율성 이득
 - 신중한 부하 분산의 필요성</span><span class="sh">"</span>
</code></pre></div></div> <h2 id="실용적-시사점과-교훈">실용적 시사점과 교훈</h2> <h3 id="연구자들을-위해">연구자들을 위해</h3> <ol> <li> <strong>희소 모델이 작동할 수 있음</strong> - 하지만 정교한 엔지니어링 솔루션이 필요</li> <li> <strong>부하 분산이 중요함</strong> - 개입 없이는 모델이 몇 개 전문가 사용으로 붕괴</li> <li> <strong>평가에 주의가 필요함</strong> - 매개변수 증가가 공정한 비교를 어렵게 만듦</li> <li> <strong>인프라가 중요함</strong> - 분산 훈련 복잡성이 상당함</li> </ol> <h3 id="실무자들을-위해">실무자들을 위해</h3> <ol> <li> <strong>제약 조건 고려</strong>: <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="k">if</span> <span class="n">메모리_예산_크고</span> <span class="ow">and</span> <span class="n">분산_훈련_가능</span><span class="p">:</span>
    <span class="n">sparse_moe_고려</span><span class="p">()</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">밀집_모델이_더_나을_수도</span><span class="p">()</span>
</code></pre></div> </div> </li> <li> <strong>배치 크기 요구사항</strong>: <div class="language-python highlighter-rouge"> <div class="highlight"><pre class="highlight"><code><span class="n">최소_배치_크기</span> <span class="o">=</span> <span class="n">전문가_수</span> <span class="o">*</span> <span class="n">k</span> <span class="o">/</span> <span class="n">원하는_전문가당_예제수</span>
<span class="c1"># 효율성을 위해 큰 배치 필요 - 일반적으로 512+ 예제
</span></code></pre></div> </div> </li> <li> <strong>고려해야 할 숨겨진 비용들</strong>: <ul> <li>메모리: 대형 희소 모델의 경우 10-100배 증가</li> <li>저장: 모델 파일이 거대해짐</li> <li>통신: 전문가 라우팅이 상당한 대역폭 필요</li> <li>복잡성: 부하 분산, 분산 훈련 설정</li> </ul> </li> </ol> <h2 id="장기적-영향과-미래-방향">장기적 영향과 미래 방향</h2> <p>이 논문은 희소 신경망의 현대적 시대를 열었습니다. 그 영향은 다음에서 볼 수 있습니다:</p> <ul> <li> <strong>Switch Transformer</strong> (Google, 2021) - 단순화된 MoE 설계</li> <li> <strong>GLaM</strong> (Google, 2021) - 64개 전문가 언어 모델</li> <li> <strong>PaLM</strong> (Google, 2022) - 밀집 모델에 적용된 확장 통찰</li> <li> <strong>GPT-4</strong> (OpenAI, 2023) - 아마도 MoE 기법 사용 (미확인)</li> </ul> <h3 id="분야를-위한-핵심-교훈">분야를 위한 핵심 교훈</h3> <ol> <li> <strong>조건부 계산이 실행 가능함</strong> - 하지만 신중한 엔지니어링 필요</li> <li> <strong>매개변수 효율성 vs 계산 효율성</strong> - 이들은 다른 최적화 목표</li> <li> <strong>확장 법칙이 중요함</strong> - 더 큰 데이터셋이 훨씬 더 큰 모델을 효과적으로 활용할 수 있음</li> <li> <strong>인프라 공동 설계</strong> - 알고리즘과 시스템 엔지니어링이 함께 발전해야 함</li> </ol> <h3 id="열린-질문들과-미래-연구">열린 질문들과 미래 연구</h3> <ul> <li> <strong>더 나은 부하 분산 알고리즘</strong> - 현재 방법들은 여전히 차선책</li> <li> <strong>희소 어텐션 메커니즘</strong> - 어텐션에 유사한 원리 적용</li> <li> <strong>자동화된 전문가 특수화</strong> - 언제 어떻게 특수화할지 학습</li> <li> <strong>희소 모델의 효율적 추론</strong> - 현재 방법들은 훈련에 최적화됨</li> </ul> <h2 id="결론">결론</h2> <p>이 논문은 조건부 계산이 대규모로 작동할 수 있음을 증명한 진정한 돌파구를 나타냅니다. 효율성 주장이 과장되고 실험적 비교에 상당한 한계가 있지만, 핵심 기여 - 희소 MoE가 의미 있는 개선을 달성할 수 있음을 실증 - 는 전체 연구 방향을 출범시켰습니다.</p> <p>이 연구의 지속적인 가치는 특정 효율성 수치(의문스러운)에 있지 않고 조건부 계산을 실용적으로 만드는 엔지니어링 청사진에 있습니다. 2017년 이후의 모든 주요 희소 신경망은 여기서 놓인 기초 위에 구축됩니다: 노이즈 게이팅, 부하 분산 손실, 하이브리드 분산 훈련.</p> <p>오늘날 대규모 신경망을 다루는 누구에게나 이러한 기법들을 이해하는 것은 필수적입니다 - 반드시 MoE를 사용해야 하기 때문이 아니라, 희소성, 부하 분산, 확장의 원리들이 현대 딥러닝의 기본이 되었기 때문입니다.</p> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa3/">Flash Attention 3</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa2/">Flash Attention 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa/">Flash Attention</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/usp/">Unified Sequence Parallelism</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> </li> <div id="disqus_thread" style="max-width: 930px; margin: 0 auto;"> <script type="text/javascript">
    var disqus_shortname  = 'sungyubkim';
    var disqus_identifier = '/blog/moe';
    var disqus_title      = "Mixture of Experts";
    (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Sung-Yub Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 29, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>