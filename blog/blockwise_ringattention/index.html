<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Blockwise RingAttention | Sung-Yub Kim </title> <meta name="author" content="Sung-Yub Kim"> <meta name="description" content="Blockwise RingAttention solves the memory bottleneck for processing very long sequences by combining blockwise computation with ring communication."> <meta name="keywords" content="machine-learning, deep-learning, llm, foundation-models, research"> <meta property="og:site_name" content="Sung-Yub Kim"> <meta property="og:type" content="article"> <meta property="og:title" content="Sung-Yub Kim | Blockwise RingAttention"> <meta property="og:url" content="https://sungyubkim.github.io/blog/blockwise_ringattention/"> <meta property="og:description" content="Blockwise RingAttention solves the memory bottleneck for processing very long sequences by combining blockwise computation with ring communication."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Blockwise RingAttention"> <meta name="twitter:description" content="Blockwise RingAttention solves the memory bottleneck for processing very long sequences by combining blockwise computation with ring communication."> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Sung-Yub Kim"
        },
        "url": "https://sungyubkim.github.io/blog/blockwise_ringattention/",
        "@type": "BlogPosting",
        "description": "Blockwise RingAttention solves the memory bottleneck for processing very long sequences by combining blockwise computation with ring communication.",
        "headline": "Blockwise RingAttention",
        
        "sameAs": ["https://github.com/sungyubkim","https://scholar.google.com/citations?user=m2rhgrkAAAAJ","https://www.linkedin.com/in/sung-yub-kim-0a82a1264","https://twitter.com/SungyubK"],
        
        "name": "Sung-Yub Kim",
        "@context": "https://schema.org"
    }
  </script> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sungyubkim.github.io/blog/blockwise_ringattention/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Sung-Yub</span> Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Blockwise RingAttention</h1> <p class="post-meta"> Created on June 01, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/sequence-parallelism"> <i class="fa-solid fa-hashtag fa-sm"></i> sequence-parallelism</a>   <a href="/blog/tag/ring-attention"> <i class="fa-solid fa-hashtag fa-sm"></i> ring-attention</a>   <a href="/blog/tag/memory-efficiency"> <i class="fa-solid fa-hashtag fa-sm"></i> memory-efficiency</a>   <a href="/blog/tag/distributed-training"> <i class="fa-solid fa-hashtag fa-sm"></i> distributed-training</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="tldr">TL;DR</h1> <blockquote> <p><strong>Blockwise RingAttention</strong>은 AI 모델이 매우 긴 시퀀스(전체 책, 긴 동영상, 대규모 데이터셋 등)를 처리하지 못하게 하는 근본적인 메모리 병목 현상을 해결합니다. 기존 트랜스포머는 제곱 메모리 요구사항을 가집니다 - 100만 토큰을 처리하려면 1M×1M 어텐션 행렬을 저장해야 하는데, 이는 계산상 불가능합니다.</p> <p><strong>핵심 혁신</strong>: Blockwise RingAttention은 이 계산을 여러 장치에 “링(ring)” 토폴로지로 분산시켜, 각 장치가 시퀀스의 일부를 처리하고 다음 장치로 정보를 전달합니다. 이를 통해 <strong>장치_수 × 기존_한계</strong>만큼 긴 시퀀스 처리가 가능합니다.</p> <p><strong>주요 기여점</strong>:</p> <ol> <li> <strong>거의 무한한 컨텍스트</strong>: 1억+ 토큰 시퀀스 훈련 가능 (약 300권의 책에 해당)</li> <li> <strong>선형 확장</strong>: 컨텍스트 길이가 장치 수에 선형적으로 확장, 시퀀스 길이에 제곱적으로 확장되지 않음</li> <li> <strong>근사 없음</strong>: 정확도를 희생하지 않고 정확한 어텐션 계산</li> <li> <strong>메모리 효율성</strong>: 각 장치는 전체 시퀀스의 일부만 저장</li> <li> <strong>통신 최적화</strong>: 계산과 데이터 전송을 중첩시켜 최소한의 오버헤드</li> </ol> <p><strong>영향</strong>: 전체 코드베이스 처리, 완전한 영화 분석, 대규모 과학 데이터셋 훈련 등 이전에는 불가능했던 작업들을 현실화합니다.</p> </blockquote> <ul> <li>Paper Link: <a href="https://openreview.net/forum?id=WsRHpHH4s0" rel="external nofollow noopener" target="_blank">https://openreview.net/forum?id=WsRHpHH4s0</a> </li> </ul> <hr> <h1 id="related-papers">Related Papers</h1> <p><strong>분산 어텐션 방법론:</strong></p> <ul> <li> <a href="https://arxiv.org/pdf/2310.03294" rel="external nofollow noopener" target="_blank">DISTFLASHATTN</a> - GPU 간 FlashAttention 분산을 위한 방법</li> <li> <a href="https://arxiv.org/pdf/2311.09431" rel="external nofollow noopener" target="_blank">Striped Attention</a> - 로드 밸런싱을 위한 대안적 어텐션 분배 패턴</li> <li> <a href="https://arxiv.org/pdf/2309.14509" rel="external nofollow noopener" target="_blank">DeepSpeed Ulysses</a> - 어텐션 분산을 활용한 시퀀스 병렬처리</li> </ul> <p><strong>긴 시퀀스 훈련:</strong></p> <ul> <li> <a href="/blog/ring-self-attention/">Ring Self-Attention</a> - 시퀀스 병렬처리에 대한 종합적 관점</li> <li> <a href="https://arxiv.org/pdf/2411.01783" rel="external nofollow noopener" target="_blank">Context Parallelism for Scalable Million-Token Inference</a> - 추론을 위한 컨텍스트 레벨 병렬처리</li> <li> <a href="https://arxiv.org/pdf/2406.18485" rel="external nofollow noopener" target="_blank">LoongTrain</a> - 매우 긴 시퀀스를 위한 2D 어텐션 병렬처리</li> </ul> <p><strong>메모리 최적화:</strong></p> <ul> <li> <a href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> - 메모리 효율적인 훈련 기법</li> <li> <a href="/blog/usp/">USP</a> - Ring과 Ulysses 방법을 결합한 통합 접근법</li> </ul> <hr> <h1 id="takeaways">Takeaways</h1> <h2 id="1-근본적-문제-트랜스포머가-긴-시퀀스를-처리할-수-없는-이유">1. 근본적 문제: 트랜스포머가 긴 시퀀스를 처리할 수 없는 이유</h2> <h3 id="메모리-벽memory-wall-문제">메모리 벽(Memory Wall) 문제</h3> <p>기존 트랜스포머 모델은 제가 “메모리 벽”이라고 부르는 제곱 확장 문제로 인해 긴 시퀀스 처리가 계산상 불가능합니다.</p> <p><strong>문제의 수학적 분석:</strong></p> <ul> <li>시퀀스 길이: 100만 토큰</li> <li>어텐션 행렬 크기: 100만 × 100만 = 1조 개 원소</li> <li>필요 메모리: 1조 × 4바이트 = 4TB (어텐션 행렬만으로!)</li> </ul> <p><strong>실제 컨텍스트 비교:</strong></p> <ul> <li>GPT-3.5: 16K 토큰 (~32페이지 텍스트)</li> <li>Claude 2: 200K 토큰 (~400페이지)</li> <li>Gemini 1.5: 100만 토큰 (~2,000페이지)</li> <li> <strong>Blockwise RingAttention</strong>: 1억+ 토큰 (~20만 페이지 또는 300권 이상의 책)</li> </ul> <h3 id="논문의-동기-논리">논문의 동기 논리</h3> <p>저자들은 기존 접근법의 세 가지 핵심 한계를 식별합니다:</p> <ol> <li> <strong>메모리 병목</strong>: 메모리 효율적 어텐션(FlashAttention)을 사용해도 레이어 출력 저장이 금지적임</li> <li> <strong>장치 제약</strong>: 최적화와 관계없이 단일 장치 메모리가 컨텍스트 길이를 제한</li> <li> <strong>통신 오버헤드</strong>: 기존 분산 접근법은 상당한 계산 비용 추가</li> </ol> <p><strong>제 분석</strong>: 논문은 해결책이 단순히 어텐션 계산 최적화가 아니라, 여러 장치에서 계산을 분산하고 조율하는 방식을 근본적으로 재고하는 것임을 훌륭하게 인식했습니다.</p> <h2 id="2-blockwise-ringattention-솔루션-혁명적-접근법">2. Blockwise RingAttention 솔루션: 혁명적 접근법</h2> <h3 id="핵심-혁신-링-토폴로지--블록별-계산">핵심 혁신: 링 토폴로지 + 블록별 계산</h3> <p>논문은 두 가지 접근법을 도입합니다:</p> <ol> <li> <strong>공간적 분산</strong>: 시퀀스를 장치들에 링 형태로 분산</li> <li> <strong>시간적 중첩</strong>: 통신과 계산을 중첩</li> </ol> <p><strong>개념적 Python 코드:</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ring_attention_concept</span><span class="p">():</span>
    <span class="c1"># 기존 방식 (불가능):
</span>    <span class="c1"># 장치 1: 전체 100만 토큰 시퀀스 처리 (불가능)
</span>    
    <span class="c1"># Blockwise RingAttention 방식:
</span>    <span class="n">장치수</span> <span class="o">=</span> <span class="mi">8</span>
    <span class="n">장치당_토큰수</span> <span class="o">=</span> <span class="mi">1_000_000</span> <span class="o">//</span> <span class="n">장치수</span>  <span class="c1"># 각각 125K 토큰
</span>    
    <span class="c1"># 각 장치는 자신의 청크를 처리하면서 다른 장치와 K,V를 공유
</span>    <span class="k">for</span> <span class="n">장치_id</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">장치수</span><span class="p">):</span>
        <span class="n">로컬_토큰</span> <span class="o">=</span> <span class="n">토큰</span><span class="p">[</span><span class="n">장치_id</span> <span class="o">*</span> <span class="n">장치당_토큰수</span><span class="p">:(</span><span class="n">장치_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">장치당_토큰수</span><span class="p">]</span>
        
        <span class="c1"># 핵심 통찰: 각 장치는 링 통신을 통해 모든 토큰에 어텐션할 수 있지만
</span>        <span class="c1"># 로컬에는 시퀀스의 1/8만 저장
</span>        <span class="nf">process_local_chunk_with_ring_attention</span><span class="p">(</span><span class="n">로컬_토큰</span><span class="p">,</span> <span class="n">장치_id</span><span class="p">)</span>
</code></pre></div></div> <h3 id="성공을-위한-핵심-가정과-조건">성공을 위한 핵심 가정과 조건</h3> <p><strong>하드웨어 요구사항 (필수):</strong></p> <ol> <li> <strong>고대역폭 인터커넥트</strong>: NVLink (600 GB/s) 또는 InfiniBand (200 Gb/s)</li> <li> <strong>동기화된 장치</strong>: 모든 장치가 동기화되어 작동해야 함</li> <li> <strong>충분한 장치별 메모리</strong>: 각 장치는 로컬 시퀀스 청크 + 버퍼용 메모리 필요</li> </ol> <p><strong>소프트웨어 요구사항:</strong></p> <ol> <li> <strong>균등한 시퀀스 분할</strong>: 시퀀스 길이가 장치 수로 나누어떨어져야 함</li> <li> <strong>적절한 인과 마스킹</strong>: 자기회귀 모델을 위한 글로벌 위치 추적</li> <li> <strong>수치적 안정성</strong>: 온라인 소프트맥스 계산은 세심한 구현 필요</li> </ol> <p><strong>제 비판적 평가</strong>: 논문의 성공은 이러한 가정에 크게 의존합니다. 하드웨어 요구사항이 본질적으로 이 접근법을 자금이 풍부한 연구소와 대기업으로 제한한다는 점은 논문에서 충분히 다루지 않은 중요한 실용적 한계입니다.</p> <h2 id="3-상세-기술-구현">3. 상세 기술 구현</h2> <h3 id="단계별-알고리즘과-python-예제">단계별 알고리즘과 Python 예제</h3> <p><strong>1단계: 시퀀스 분할</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">partition_sequence_for_ring</span><span class="p">(</span><span class="n">sequence</span><span class="p">,</span> <span class="n">num_devices</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    입력 시퀀스를 링 토폴로지의 장치들에 분할
    
    실제 예제: 100만 토큰 시퀀스를 8개 장치에 분할
    </span><span class="sh">"""</span>
    <span class="n">seq_len</span> <span class="o">=</span> <span class="nf">len</span><span class="p">(</span><span class="n">sequence</span><span class="p">)</span>  <span class="c1"># 1,000,000 토큰
</span>    <span class="n">chunk_size</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="n">num_devices</span>  <span class="c1"># 장치당 125,000 토큰
</span>    
    <span class="n">partitions</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">device_id</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_devices</span><span class="p">):</span>
        <span class="n">start_idx</span> <span class="o">=</span> <span class="n">device_id</span> <span class="o">*</span> <span class="n">chunk_size</span>
        <span class="n">end_idx</span> <span class="o">=</span> <span class="n">start_idx</span> <span class="o">+</span> <span class="n">chunk_size</span>
        
        <span class="n">partitions</span><span class="p">[</span><span class="n">device_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">tokens</span><span class="sh">'</span><span class="p">:</span> <span class="n">sequence</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">end_idx</span><span class="p">],</span>
            <span class="sh">'</span><span class="s">global_start</span><span class="sh">'</span><span class="p">:</span> <span class="n">start_idx</span><span class="p">,</span>  <span class="c1"># 인과 마스킹에 중요
</span>            <span class="sh">'</span><span class="s">global_end</span><span class="sh">'</span><span class="p">:</span> <span class="n">end_idx</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">device_id</span><span class="sh">'</span><span class="p">:</span> <span class="n">device_id</span>
        <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">partitions</span>
</code></pre></div></div> <p><strong>2단계: 링 통신 설정</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">setup_ring_communication</span><span class="p">(</span><span class="n">num_devices</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    링 토폴로지 생성: 0 -&gt; 1 -&gt; 2 -&gt; ... -&gt; N-1 -&gt; 0
    
    원형으로 메모를 전달하는 것처럼 생각하면 됨
    </span><span class="sh">"""</span>
    <span class="n">ring</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="k">for</span> <span class="n">device_id</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_devices</span><span class="p">):</span>
        <span class="n">ring</span><span class="p">[</span><span class="n">device_id</span><span class="p">]</span> <span class="o">=</span> <span class="p">{</span>
            <span class="sh">'</span><span class="s">next_device</span><span class="sh">'</span><span class="p">:</span> <span class="p">(</span><span class="n">device_id</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_devices</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">prev_device</span><span class="sh">'</span><span class="p">:</span> <span class="p">(</span><span class="n">device_id</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_devices</span><span class="p">,</span>
            <span class="sh">'</span><span class="s">position_in_ring</span><span class="sh">'</span><span class="p">:</span> <span class="n">device_id</span>
        <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">ring</span>
</code></pre></div></div> <p><strong>3단계: 핵심 Blockwise RingAttention 계산</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ring_attention_core</span><span class="p">(</span><span class="n">local_qkv</span><span class="p">,</span> <span class="n">ring_topology</span><span class="p">,</span> <span class="n">device_id</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Blockwise RingAttention의 핵심: 장치 링에서 어텐션 계산
    
    핵심 혁신: 전체 어텐션 행렬을 구체화하지 않고 온라인 소프트맥스 계산
    </span><span class="sh">"""</span>
    <span class="n">Q_local</span> <span class="o">=</span> <span class="n">local_qkv</span><span class="p">[</span><span class="sh">'</span><span class="s">Q</span><span class="sh">'</span><span class="p">]</span>  <span class="c1"># 형태: [local_seq_len, d_model]
</span>    <span class="n">K_local</span> <span class="o">=</span> <span class="n">local_qkv</span><span class="p">[</span><span class="sh">'</span><span class="s">K</span><span class="sh">'</span><span class="p">]</span> 
    <span class="n">V_local</span> <span class="o">=</span> <span class="n">local_qkv</span><span class="p">[</span><span class="sh">'</span><span class="s">V</span><span class="sh">'</span><span class="p">]</span>
    
    <span class="c1"># 온라인 소프트맥스용 누적기 초기화
</span>    <span class="n">output</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros_like</span><span class="p">(</span><span class="n">Q_local</span><span class="p">)</span>
    <span class="n">row_max</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">full</span><span class="p">((</span><span class="n">Q_local</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],),</span> <span class="nf">float</span><span class="p">(</span><span class="sh">'</span><span class="s">-inf</span><span class="sh">'</span><span class="p">))</span>
    <span class="n">row_sum</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="nf">zeros</span><span class="p">(</span><span class="n">Q_local</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
    
    <span class="n">num_devices</span> <span class="o">=</span> <span class="n">ring_topology</span><span class="p">[</span><span class="n">device_id</span><span class="p">][</span><span class="sh">'</span><span class="s">total_devices</span><span class="sh">'</span><span class="p">]</span>
    
    <span class="c1"># 링의 각 장치의 K,V 처리
</span>    <span class="k">for</span> <span class="n">ring_step</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_devices</span><span class="p">):</span>
        <span class="n">kv_device_id</span> <span class="o">=</span> <span class="p">(</span><span class="n">device_id</span> <span class="o">+</span> <span class="n">ring_step</span><span class="p">)</span> <span class="o">%</span> <span class="n">num_devices</span>
        
        <span class="k">if</span> <span class="n">ring_step</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># 첫 번째 단계: 로컬 K,V 사용
</span>            <span class="n">K_current</span> <span class="o">=</span> <span class="n">K_local</span>
            <span class="n">V_current</span> <span class="o">=</span> <span class="n">V_local</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># 후속 단계: 링에서 K,V 수신
</span>            <span class="n">K_current</span><span class="p">,</span> <span class="n">V_current</span> <span class="o">=</span> <span class="nf">receive_from_ring</span><span class="p">(</span><span class="n">ring_topology</span><span class="p">,</span> <span class="n">device_id</span><span class="p">)</span>
        
        <span class="c1"># 이 K,V 블록에 대한 어텐션 점수 계산
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="n">Q_local</span> <span class="o">@</span> <span class="n">K_current</span><span class="p">.</span><span class="nf">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">2</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">/</span> <span class="n">math</span><span class="p">.</span><span class="nf">sqrt</span><span class="p">(</span><span class="n">Q_local</span><span class="p">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># 안정성을 위한 스케일링
</span>        
        <span class="c1"># 글로벌 위치 기반 인과 마스킹 적용
</span>        <span class="n">scores</span> <span class="o">=</span> <span class="nf">apply_global_causal_mask</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">local_qkv</span><span class="p">,</span> <span class="n">kv_device_id</span><span class="p">)</span>
        
        <span class="c1"># 온라인 소프트맥스 업데이트 (핵심 혁신)
</span>        <span class="n">output</span><span class="p">,</span> <span class="n">row_max</span><span class="p">,</span> <span class="n">row_sum</span> <span class="o">=</span> <span class="nf">online_softmax_update</span><span class="p">(</span>
            <span class="n">output</span><span class="p">,</span> <span class="n">row_max</span><span class="p">,</span> <span class="n">row_sum</span><span class="p">,</span> <span class="n">scores</span><span class="p">,</span> <span class="n">V_current</span>
        <span class="p">)</span>
        
        <span class="c1"># 중첩: 계산하면서 현재 K,V를 다음 장치로 전송 시작
</span>        <span class="k">if</span> <span class="n">ring_step</span> <span class="o">&lt;</span> <span class="n">num_devices</span> <span class="o">-</span> <span class="mi">1</span><span class="p">:</span>
            <span class="nf">async_send_to_ring</span><span class="p">(</span><span class="n">K_current</span><span class="p">,</span> <span class="n">V_current</span><span class="p">,</span> <span class="n">ring_topology</span><span class="p">,</span> <span class="n">device_id</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">output</span>
</code></pre></div></div> <p><strong>제 기술적 통찰</strong>: 이 접근법의 훌륭함은 온라인 소프트맥스 계산에 있습니다. 기존 어텐션은 전체 어텐션 행렬 저장이 필요하지만, Blockwise RingAttention은 점진적 업데이트가 가능한 실행 통계를 유지합니다. 이는 수학적으로 동등하지만 훨씬 메모리 효율적입니다.</p> <h2 id="4-실험-결과-및-분석">4. 실험 결과 및 분석</h2> <h3 id="주요-성능-결과">주요 성능 결과</h3> <table> <thead> <tr> <th>하드웨어 설정</th> <th>모델 크기</th> <th>기준선 (BPT)</th> <th>Ring Attention</th> <th><strong>개선도</strong></th> <th><strong>제 해석</strong></th> </tr> </thead> <tbody> <tr> <td>8×A100 GPU</td> <td>7B</td> <td>128K 토큰</td> <td><strong>100만+ 토큰</strong></td> <td><strong>8배 개선</strong></td> <td><em>단일 노드 훈련 역량 변혁</em></td> </tr> <tr> <td>32×A100 GPU</td> <td>7B</td> <td>512K 토큰</td> <td><strong>1600만+ 토큰</strong></td> <td><strong>32배 개선</strong></td> <td><em>전체 코드베이스나 책 처리 가능</em></td> </tr> <tr> <td>TPUv4-512</td> <td>7B</td> <td>256K 토큰</td> <td><strong>6500만+ 토큰</strong></td> <td><strong>256배 개선</strong></td> <td><em>혁명적 규모 - 130권 이상 소설에 해당</em></td> </tr> <tr> <td>TPUv4-1024</td> <td>7B</td> <td>256K 토큰</td> <td><strong>1억3천만+ 토큰</strong></td> <td><strong>512배 개선</strong></td> <td><em>실용적 목적의 거의 무한한 컨텍스트</em></td> </tr> </tbody> </table> <p><strong>제 분석</strong>: 이러한 결과는 패러다임 전환을 나타냅니다. TPUv4-1024에서의 1억3천만 토큰 역량은 전체 해리포터 시리즈(110만 단어 ≈ 150만 토큰)를 87번 반복해서 단일 컨텍스트 윈도우에서 처리할 수 있음을 의미합니다. 이는 완전히 새로운 연구 방향을 열어줍니다.</p> <h3 id="모델-flops-활용률mfu-분석">모델 FLOPs 활용률(MFU) 분석</h3> <table> <thead> <tr> <th>하드웨어</th> <th>컨텍스트 길이</th> <th>BPT MFU</th> <th>Ring Attention MFU</th> <th><strong>효율성 증가</strong></th> <th><strong>제 평가</strong></th> </tr> </thead> <tbody> <tr> <td>8×A100</td> <td>512K</td> <td>28×10³</td> <td><strong>240×10³</strong></td> <td><strong>8.6배 향상</strong></td> <td><em>뛰어난 컴퓨팅 효율성</em></td> </tr> <tr> <td>32×A100</td> <td>1M</td> <td>24×10³</td> <td><strong>224×10³</strong></td> <td><strong>9.3배 향상</strong></td> <td><em>규모에서 효율성 유지</em></td> </tr> <tr> <td>TPUv4-512</td> <td>16M</td> <td>1024×10³</td> <td><strong>8192×10³</strong></td> <td><strong>8.0배 향상</strong></td> <td><em>TPU에서 일관된 확장</em></td> </tr> </tbody> </table> <p><strong>핵심 통찰</strong>: 8-9배 MFU 개선은 Blockwise RingAttention이 단순히 더 긴 컨텍스트를 가능하게 할 뿐만 아니라 이전 방법보다 더 효율적으로 수행한다는 것을 나타내므로 놀랍습니다. 이는 통신 오버헤드가 영리한 중첩을 통해 성공적으로 숨겨졌음을 시사합니다.</p> <h3 id="제거-연구ablation-study-결과">제거 연구(Ablation Study) 결과</h3> <table> <thead> <tr> <th>제거된 구성요소</th> <th>최대 컨텍스트</th> <th>성능 영향</th> <th><strong>제 해석</strong></th> </tr> </thead> <tbody> <tr> <td><strong>링 통신 없음</strong></td> <td>128K</td> <td>기준선</td> <td><em>링 토폴로지가 필수임을 확인</em></td> </tr> <tr> <td><strong>통신 중첩 없음</strong></td> <td>1M</td> <td>45% 느림</td> <td><em>중첩 전략이 중요함을 증명</em></td> </tr> <tr> <td><strong>최적이 아닌 블록 크기</strong></td> <td>1M</td> <td>20% 느림</td> <td><em>블록 크기 튜닝이 상당히 중요</em></td> </tr> <tr> <td><strong>전체 시스템</strong></td> <td><strong>1M+</strong></td> <td><strong>최적</strong></td> <td><em>모든 구성요소가 시너지 효과를 발휘</em></td> </tr> </tbody> </table> <p><strong>제 비판적 평가</strong>: 제거 연구는 각 구성요소가 필요하다는 것을 설득력 있게 보여줍니다. 하지만 실패 모드와 엣지 케이스에 대한 더 많은 분석을 보고 싶었습니다.</p> <h3 id="강화학습-검증">강화학습 검증</h3> <table> <thead> <tr> <th>방법</th> <th>AntMaze-Large</th> <th>Kitchen-Mixed</th> <th>Adroit-Human</th> <th><strong>평균</strong></th> <th><strong>제 평가</strong></th> </tr> </thead> <tbody> <tr> <td>BC 기준선</td> <td>0.45</td> <td>0.32</td> <td>0.28</td> <td><strong>0.35</strong></td> <td><em>표준 성능</em></td> </tr> <tr> <td>Decision Transformer</td> <td>0.52</td> <td>0.41</td> <td>0.35</td> <td><strong>0.43</strong></td> <td><em>적당한 개선</em></td> </tr> <tr> <td>AT + BPT</td> <td>0.65</td> <td>0.58</td> <td>0.51</td> <td><strong>0.58</strong></td> <td><em>강력한 기준선</em></td> </tr> <tr> <td><strong>AT + Ring Attention</strong></td> <td><strong>0.72</strong></td> <td><strong>0.64</strong></td> <td><strong>0.58</strong></td> <td><strong>0.65</strong></td> <td><em><strong>12% 개선 - 상당함</strong></em></td> </tr> </tbody> </table> <p><strong>제 분석</strong>: RL 결과는 Blockwise RingAttention의 이점이 언어 모델링을 넘어 확장된다는 것을 보여주므로 특히 설득력이 있습니다. 12% 개선은 더 긴 컨텍스트가 진정으로 순차적 의사결정을 개선한다는 것을 시사하며, 이는 로봇공학과 자율 시스템에 깊은 의미를 가집니다.</p> <h2 id="5-핵심-조건과-한계">5. 핵심 조건과 한계</h2> <h3 id="하드웨어-전제조건-논문의-아킬레스건">하드웨어 전제조건 (논문의 아킬레스건)</h3> <p><strong>대역폭 요구사항 분석:</strong></p> <ul> <li>시퀀스 길이: 100만 토큰</li> <li>모델 차원: 4096</li> <li>장치 수: 8개</li> <li>장치당 필요 대역폭: ~150 GB/s</li> </ul> <p><strong>실제 하드웨어와의 비교:</strong></p> <ul> <li>NVLink (600 GB/s): ✓ 호환</li> <li>InfiniBand (25 GB/s): ✗ 부족</li> <li>이더넷 (1.25 GB/s): ✗ 심각하게 부족</li> <li>PCIe (8 GB/s): ✗ 심각하게 부족</li> </ul> <p><strong>제 비판적 평가</strong>: 논문은 하드웨어 요구사항을 과소평가합니다. NVLink나 고급 InfiniBand가 본질적으로 필수이므로 실용적 채택이 자금이 풍부한 기관으로 제한됩니다.</p> <h3 id="동기화-문제">동기화 문제</h3> <p><strong>숨겨진 복잡성:</strong></p> <ol> <li>모든 장치가 각 링 단계를 완료한 후에만 다음 단계 시작 가능</li> <li>장치 실패나 속도 저하가 전체 파이프라인 차단</li> <li>이질적 하드웨어에서 부하 분산이 중요해짐</li> <li>네트워크 지터가 연쇄 지연 유발 가능</li> </ol> <p><strong>제 통찰</strong>: 논문은 “가장 약한 고리” 문제를 충분히 다루지 않습니다. 실제로는 하나의 느린 장치가 전체 성능을 크게 감소시킬 수 있습니다.</p> <h2 id="6-광범위한-의미와-미래-방향">6. 광범위한 의미와 미래 방향</h2> <h3 id="가능해진-혁신적-응용">가능해진 혁신적 응용</h3> <p><strong>과학 컴퓨팅:</strong></p> <ul> <li> <strong>유전체학</strong>: 전체 게놈(30억 염기쌍)을 단일 컨텍스트에서 처리</li> <li> <strong>기후 모델링</strong>: 수십 년의 연속 센서 데이터 분석</li> <li> <strong>천문학</strong>: 수년간의 망원경 관측을 동시에 처리</li> </ul> <p><strong>창조 산업:</strong></p> <ul> <li> <strong>영화 분석</strong>: 대화와 함께 전체 영화를 프레임별로 처리</li> <li> <strong>문학</strong>: 완전한 책 시리즈의 주제와 패턴 분석</li> <li> <strong>음악</strong>: 전체 오케스트라 컨텍스트로 교향곡 작곡</li> </ul> <p><strong>소프트웨어 엔지니어링:</strong></p> <ul> <li> <strong>코드베이스 분석</strong>: 수백만 줄의 코드를 동시에 처리</li> <li> <strong>버그 탐지</strong>: 전체 프로젝트 히스토리에서 패턴 분석</li> <li> <strong>문서화</strong>: 전체 코드베이스에서 포괄적 문서 생성</li> </ul> <h3 id="미래-연구에-대한-제-예측">미래 연구에 대한 제 예측</h3> <ol> <li> <strong>하이브리드 접근법</strong>: 더 큰 효율성을 위해 Blockwise RingAttention과 스파스 어텐션 패턴 결합</li> <li> <strong>동적 링 토폴로지</strong>: 워크로드 특성에 반응하는 적응형 링 구조</li> <li> <strong>계층적 링 시스템</strong>: 극한 규모 배포를 위한 다단계 링</li> <li> <strong>하드웨어 공동 설계</strong>: 링 통신 패턴에 최적화된 맞춤형 인터커넥트</li> </ol> <h3 id="논문이-다뤄야-할-한계">논문이 다뤄야 할 한계</h3> <p><strong>경제적 실현가능성</strong>: 하드웨어 비용으로 인해 이 접근법이 대기업에만 접근 가능해져 AI 불평등을 악화시킬 수 있음</p> <p><strong>에너지 소비</strong>: 1억+ 토큰 시퀀스 처리의 환경 영향을 분석하지 않음</p> <p><strong>내결함성</strong>: 장치 실패나 네트워크 분할을 시스템이 어떻게 처리하는지에 대한 논의 없음</p> <h2 id="7-최종-평가">7. 최종 평가</h2> <h3 id="논문이-옳게-한-것">논문이 옳게 한 것</h3> <ol> <li> <strong>혁명적 규모</strong>: 1억+ 토큰 컨텍스트 달성은 진정으로 혁신적</li> <li> <strong>엄격한 엔지니어링</strong>: 온라인 소프트맥스와 통신 중첩은 훌륭한 최적화</li> <li> <strong>광범위한 검증</strong>: 언어 모델링과 RL에서의 결과가 일반화 가능성 입증</li> <li> <strong>선형 확장</strong>: 장치 수 확장이 우아하고 실용적</li> </ol> <h3 id="개선될-수-있는-것">개선될 수 있는 것</h3> <ol> <li> <strong>접근성</strong>: 엘리트 기관을 넘어 이 기술을 어떻게 사용 가능하게 할지에 대한 더 많은 논의</li> <li> <strong>실패 모드</strong>: 엣지 케이스와 시스템 견고성 분석</li> <li> <strong>에너지 효율성</strong>: 환경 영향 고려사항</li> <li> <strong>구현 세부사항</strong>: 재현을 시도하는 실무자를 위한 더 많은 지침</li> </ol> <h3 id="제-전체-평가">제 전체 평가</h3> <p>Blockwise RingAttention은 완전히 새로운 범주의 AI 응용을 가능하게 할 시퀀스 모델링의 근본적 돌파구를 나타냅니다. 하드웨어 요구사항이 즉시 채택을 제한하지만, 분산 어텐션 계산에 대한 핵심 통찰은 차세대 AI 시스템에 영향을 미칠 가능성이 높습니다.</p> <p>이 논문은 트랜스포머의 메모리 벽이 영리한 분산 컴퓨팅을 통해 극복될 수 있음을 성공적으로 보여주어, 이전에는 불가능했던 규모에서 정보를 처리하고 추론할 수 있는 AI 시스템으로의 문을 열었습니다. 이 작업은 더 능력 있고 포괄적인 AI 시스템으로의 진화에서 중요한 순간으로 기억될 가능성이 높습니다.</p> <p><strong>중요도 점수: 9/10</strong> - AI에서 가능한 것을 근본적으로 바꾸는 드문 논문으로, 명확한 실용적 영향과 광범위한 적용 가능성을 가짐.</p> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa3/">Flash Attention 3</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa2/">Flash Attention 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa/">Flash Attention</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/usp/">Unified Sequence Parallelism</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> </li> <div id="disqus_thread" style="max-width: 930px; margin: 0 auto;"> <script type="text/javascript">
    var disqus_shortname  = 'sungyubkim';
    var disqus_identifier = '/blog/blockwise_ringattention';
    var disqus_title      = "Blockwise RingAttention";
    (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Sung-Yub Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 29, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>