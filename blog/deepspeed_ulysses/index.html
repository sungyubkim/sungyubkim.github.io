<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> DeepSpeed Ulysses | Sung-Yub Kim </title> <meta name="author" content="Sung-Yub Kim"> <meta name="description" content="DeepSpeed-Ulysses achieves efficient sequence parallelism via all-to-all communication for 1M+ token training."> <meta name="keywords" content="machine-learning, deep-learning, llm, foundation-models, research"> <meta property="og:site_name" content="Sung-Yub Kim"> <meta property="og:type" content="article"> <meta property="og:title" content="Sung-Yub Kim | DeepSpeed Ulysses"> <meta property="og:url" content="https://sungyubkim.github.io/blog/deepspeed_ulysses/"> <meta property="og:description" content="DeepSpeed-Ulysses achieves efficient sequence parallelism via all-to-all communication for 1M+ token training."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="DeepSpeed Ulysses"> <meta name="twitter:description" content="DeepSpeed-Ulysses achieves efficient sequence parallelism via all-to-all communication for 1M+ token training."> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Sung-Yub Kim"
        },
        "url": "https://sungyubkim.github.io/blog/deepspeed_ulysses/",
        "@type": "BlogPosting",
        "description": "DeepSpeed-Ulysses achieves efficient sequence parallelism via all-to-all communication for 1M+ token training.",
        "headline": "DeepSpeed Ulysses",
        
        "sameAs": ["https://github.com/sungyubkim","https://scholar.google.com/citations?user=m2rhgrkAAAAJ","https://www.linkedin.com/in/sung-yub-kim-0a82a1264","https://twitter.com/SungyubK"],
        
        "name": "Sung-Yub Kim",
        "@context": "https://schema.org"
    }
  </script> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sungyubkim.github.io/blog/deepspeed_ulysses/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Sung-Yub</span> Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">DeepSpeed Ulysses</h1> <p class="post-meta"> Created on June 01, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/sequence-parallelism"> <i class="fa-solid fa-hashtag fa-sm"></i> sequence-parallelism</a>   <a href="/blog/tag/distributed-training"> <i class="fa-solid fa-hashtag fa-sm"></i> distributed-training</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="tldr">TL;DR</h1> <blockquote> <p>DeepSpeed-Ulysses는 대형 Transformer 모델 훈련에서 “시퀀스 차원 스케일링 문제”를 해결하는 새로운 시퀀스 병렬처리 접근법을 제시합니다. <strong>주요 기여:</strong> (1) 영리한 데이터 재분배를 통해 4배 더 긴 시퀀스(1M+ 토큰) 훈련 가능, (2) 기존 O(N) 방법 대비 O(N/P) 통신 복잡도 달성, (3) 모델 품질 유지하면서 구성에 따라 1.5-3.5배 속도 향상 제공. <strong>핵심 혁신:</strong> all-to-all 통신을 사용하여 시퀀스 병렬 데이터를 헤드 병렬 계산으로 변환, 각 GPU가 어텐션 헤드의 부분집합에 대해 완전한 어텐션을 계산할 수 있게 함. <strong>중요한 제약:</strong> 구성에 따라 성능 향상이 크게 다름, sparse attention에서 성능 저하, 실험 검증의 통계적 엄밀성 부족.</p> </blockquote> <ul> <li>Paper Link: <a href="https://arxiv.org/pdf/2309.14509" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2309.14509</a> </li> </ul> <hr> <h1 id="related-papers">Related Papers</h1> <p><strong>시퀀스 병렬화 방법론:</strong></p> <ul> <li> <a href="/blog/blockwise_ringattention/">Blockwise RingAttention</a> - 링 토폴로지를 활용한 시퀀스 병렬화</li> <li> <a href="/blog/ring-self-attention/">Ring Self-Attention</a> - 시퀀스 병렬화 종합 분석</li> <li> <a href="/blog/usp/">USP</a> - Ulysses와 Ring을 통합한 시퀀스 병렬화</li> </ul> <p><strong>긴 시퀀스 훈련:</strong></p> <ul> <li> <a href="https://arxiv.org/pdf/2406.18485" rel="external nofollow noopener" target="_blank">LoongTrain</a> - 2D 어텐션을 활용한 긴 시퀀스 훈련</li> <li> <a href="https://arxiv.org/pdf/2411.01783" rel="external nofollow noopener" target="_blank">Context Parallelism for Scalable Million-Token Inference</a> - 컨텍스트 병렬화를 통한 추론</li> </ul> <p><strong>어텐션 최적화:</strong></p> <ul> <li> <a href="https://arxiv.org/pdf/2310.03294" rel="external nofollow noopener" target="_blank">DISTFLASHATTN</a> - 분산 FlashAttention 구현</li> <li> <a href="https://arxiv.org/pdf/2311.09431" rel="external nofollow noopener" target="_blank">Striped Attention</a> - 효율적인 어텐션 분배 패턴</li> </ul> <p><strong>시스템 통합:</strong></p> <ul> <li> <a href="/blog/tp/">Tensor Parallelism</a> - 텐서 병렬화와의 결합</li> <li> <a href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> - 메모리 효율적인 훈련</li> </ul> <hr> <h1 id="takeaways">Takeaways</h1> <p>문제 정의: 시퀀스 스케일링이 중요한 이유</p> <p>현재 AI 애플리케이션들은 점점 더 긴 컨텍스트 추론을 요구합니다:</p> <ul> <li> <strong>대화형 AI</strong>: 확장된 대화에서 컨텍스트 유지</li> <li> <strong>문서 분석</strong>: 전체 책 처리 (100K+ 단어)</li> <li> <strong>과학 컴퓨팅</strong>: 유전체 시퀀스 분석 (수십억 염기쌍)</li> <li> <strong>비디오 생성</strong>: 긴 시퀀스에서 시간적 관계 이해</li> </ul> <p>하지만 기존 병렬처리 전략들은 시퀀스 차원에서 실패합니다:</p> <ul> <li> <strong>데이터 병렬처리</strong>: 배치 크기 확장, 시퀀스 길이 아님</li> <li> <strong>텐서 병렬처리</strong>: 모델 너비 확장, 시퀀스 길이 아님</li> <li> <strong>파이프라인 병렬처리</strong>: 모델 깊이 확장, 시퀀스 길이 아님</li> </ul> <p><strong>근본적 도전</strong>: 어텐션 계산은 O(N²) 메모리 복잡도를 가져 긴 시퀀스를 처리하기에 비용이 너무 큽니다.</p> <h2 id="해결책-어텐션-중심-시퀀스-병렬처리">해결책: 어텐션 중심 시퀀스 병렬처리</h2> <h3 id="핵심-혁신-데이터-재분배-전략">핵심 혁신: 데이터 재분배 전략</h3> <p>DeepSpeed-Ulysses는 영리한 통찰을 통해 시퀀스 병렬처리 문제를 변환합니다: 어텐션 연산 내에서 병렬화를 시도하는 대신, 데이터를 재분배하여 헤드 간 병렬 어텐션 계산을 가능하게 합니다.</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">deepspeed_ulysses_pipeline</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    핵심 변환을 보여주는 완전한 파이프라인
    </span><span class="sh">"""</span>
    <span class="c1"># 단계 1: 디바이스 간 시퀀스 분할
</span>    <span class="c1"># 입력: [batch, full_seq_len, hidden_dim]
</span>    <span class="c1"># 결과: 각 디바이스가 [batch, seq_len/P, hidden_dim] 보유
</span>    <span class="n">local_input</span> <span class="o">=</span> <span class="nf">partition_sequence</span><span class="p">(</span><span class="n">input_tokens</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
    
    <span class="c1"># 단계 2: QKV 로컬 계산 (embarrassingly parallel)
</span>    <span class="n">local_q</span><span class="p">,</span> <span class="n">local_k</span><span class="p">,</span> <span class="n">local_v</span> <span class="o">=</span> <span class="nf">compute_qkv</span><span class="p">(</span><span class="n">local_input</span><span class="p">)</span>
    
    <span class="c1"># 단계 3: ALL-TO-ALL 변환 (핵심 혁신)
</span>    <span class="c1"># 변환: 시퀀스 병렬 → 헤드 병렬
</span>    <span class="c1"># 이전: 각 디바이스가 부분 시퀀스, 모든 헤드 보유
</span>    <span class="c1"># 이후: 각 디바이스가 전체 시퀀스, 헤드 부분집합 보유
</span>    <span class="n">global_q</span><span class="p">,</span> <span class="n">global_k</span><span class="p">,</span> <span class="n">global_v</span> <span class="o">=</span> <span class="nf">all_to_all_redistribute</span><span class="p">(</span>
        <span class="n">local_q</span><span class="p">,</span> <span class="n">local_k</span><span class="p">,</span> <span class="n">local_v</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">world_size</span>
    <span class="p">)</span>
    
    <span class="c1"># 단계 4: 할당된 헤드에서 어텐션 계산 (디바이스 간 병렬)
</span>    <span class="c1"># 각 디바이스가 num_heads/P 헤드에 대해 완전한 어텐션 계산
</span>    <span class="n">attention_output</span> <span class="o">=</span> <span class="nf">compute_attention_heads</span><span class="p">(</span><span class="n">global_q</span><span class="p">,</span> <span class="n">global_k</span><span class="p">,</span> <span class="n">global_v</span><span class="p">)</span>
    
    <span class="c1"># 단계 5: ALL-TO-ALL 복원 (시퀀스 병렬처리 복원)
</span>    <span class="c1"># 변환: 헤드 병렬 → 시퀀스 병렬
</span>    <span class="n">final_output</span> <span class="o">=</span> <span class="nf">all_to_all_restore</span><span class="p">(</span><span class="n">attention_output</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">final_output</span>
</code></pre></div></div> <h3 id="작동-원리-수학적-통찰">작동 원리: 수학적 통찰</h3> <p>핵심 통찰은 multi-head attention이 자연스럽게 분해 가능하다는 것입니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 표준 어텐션: 모든 헤드를 함께 계산
</span><span class="k">def</span> <span class="nf">standard_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">):</span>
    <span class="c1"># Q, K, V: [batch, seq_len, hidden_dim]
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_heads</span><span class="p">):</span>
        <span class="n">q_h</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">head</span><span class="o">*</span><span class="n">head_dim</span><span class="p">:(</span><span class="n">head</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">head_dim</span><span class="p">]</span>
        <span class="n">k_h</span> <span class="o">=</span> <span class="n">K</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">head</span><span class="o">*</span><span class="n">head_dim</span><span class="p">:(</span><span class="n">head</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">head_dim</span><span class="p">]</span>  
        <span class="n">v_h</span> <span class="o">=</span> <span class="n">V</span><span class="p">[:,</span> <span class="p">:,</span> <span class="n">head</span><span class="o">*</span><span class="n">head_dim</span><span class="p">:(</span><span class="n">head</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">head_dim</span><span class="p">]</span>
        
        <span class="c1"># 이 계산은 헤드 간에 독립적입니다!
</span>        <span class="n">attn_h</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">q_h</span> <span class="o">@</span> <span class="n">k_h</span><span class="p">.</span><span class="n">T</span> <span class="o">/</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">head_dim</span><span class="p">))</span> <span class="o">@</span> <span class="n">v_h</span>
        <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">attn_h</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="nf">concat</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>

<span class="c1"># DeepSpeed-Ulysses: 디바이스 간 헤드 분산
</span><span class="k">def</span> <span class="nf">distributed_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">device_heads</span><span class="p">):</span>
    <span class="c1"># 각 디바이스는 할당된 헤드만 계산
</span>    <span class="c1"># 하지만 해당 헤드들에 대해 전체 시퀀스를 봄
</span>    <span class="n">outputs</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">head</span> <span class="ow">in</span> <span class="n">device_heads</span><span class="p">:</span>  <span class="c1"># 전체 헤드의 부분집합
</span>        <span class="n">attn_h</span> <span class="o">=</span> <span class="nf">compute_single_head_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">head</span><span class="p">)</span>
        <span class="n">outputs</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">attn_h</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="nf">concat</span><span class="p">(</span><span class="n">outputs</span><span class="p">)</span>
</code></pre></div></div> <h2 id="중요한-가정과-조건">중요한 가정과 조건</h2> <h3 id="수학적-요구사항">수학적 요구사항</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 방법이 작동하기 위한 하드 제약:
</span><span class="k">assert</span> <span class="n">sequence_length</span> <span class="o">%</span> <span class="n">world_size</span> <span class="o">==</span> <span class="mi">0</span>  <span class="c1"># 균등 분할 필수
</span><span class="k">assert</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">world_size</span> <span class="o">==</span> <span class="mi">0</span>        <span class="c1"># 헤드 균등 분산 필수
</span><span class="k">assert</span> <span class="n">hidden_dim</span> <span class="o">%</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">0</span>        <span class="c1"># 표준 어텐션 요구사항
</span>
<span class="c1"># 실패 사례 예시:
</span><span class="n">sequence_length</span> <span class="o">=</span> <span class="mi">1000</span>  <span class="c1"># world_size=8로 나누어떨어지지 않음
# 패딩이나 불균등 분산이 필요하여 구현 복잡화
</span></code></pre></div></div> <h3 id="인프라-요구사항">인프라 요구사항</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">validate_infrastructure</span><span class="p">(</span><span class="n">bandwidth_gbps</span><span class="p">,</span> <span class="n">latency_ms</span><span class="p">,</span> <span class="n">world_size</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    All-to-all 통신 효율성은 네트워크 토폴로지에 크게 의존
    </span><span class="sh">"""</span>
    <span class="c1"># 경험법칙: 높은 bisection bandwidth 필요
</span>    <span class="n">required_bandwidth</span> <span class="o">=</span> <span class="n">world_size</span> <span class="o">*</span> <span class="mi">10</span>  <span class="c1"># GB/s per device
</span>    <span class="k">if</span> <span class="n">bandwidth_gbps</span> <span class="o">&lt;</span> <span class="n">required_bandwidth</span><span class="p">:</span>
        <span class="k">return</span> <span class="bp">False</span><span class="p">,</span> <span class="sh">"</span><span class="s">네트워크 대역폭 부족</span><span class="sh">"</span>
    
    <span class="k">if</span> <span class="n">latency_ms</span> <span class="o">&gt;</span> <span class="mf">1.0</span><span class="p">:</span>  <span class="c1"># 높은 지연시간은 작은 메시지 성능 저하
</span>        <span class="k">return</span> <span class="bp">False</span><span class="p">,</span> <span class="sh">"</span><span class="s">네트워크 지연시간 과다</span><span class="sh">"</span>
    
    <span class="k">return</span> <span class="bp">True</span><span class="p">,</span> <span class="sh">"</span><span class="s">인프라 적합</span><span class="sh">"</span>
</code></pre></div></div> <h3 id="부하-균형-가정">부하 균형 가정</h3> <p>이 방법은 다음에 대한 균등한 계산 부하를 가정합니다:</p> <ul> <li>시퀀스 청크 (구조화된 데이터에서는 성립하지 않을 수 있음)</li> <li>어텐션 헤드 (일반적으로 참이지만 보장되지 않음)</li> <li>디바이스 (동질적 하드웨어 필요)</li> </ul> <h2 id="실험-결과-비판적-분석">실험 결과: 비판적 분석</h2> <h3 id="주요-성능-결과">주요 성능 결과</h3> <table> <thead> <tr> <th>구성</th> <th>DeepSpeed-Ulysses</th> <th>Megatron-LM</th> <th>속도향상</th> <th>현실 검증</th> </tr> </thead> <tbody> <tr> <td><strong>7B 모델, 8K seq</strong></td> <td>175 TFLOPs</td> <td>105 TFLOPs</td> <td>1.67x</td> <td> <strong>좋음</strong>: 견고한 개선</td> </tr> <tr> <td><strong>7B 모델, 32K seq</strong></td> <td>175 TFLOPs</td> <td>85 TFLOPs</td> <td>2.06x</td> <td><strong>주장에 근접</strong></td> </tr> <tr> <td><strong>7B 모델, 128K seq</strong></td> <td>165 TFLOPs</td> <td>OOM</td> <td>∞</td> <td><strong>능력 해제</strong></td> </tr> <tr> <td><strong>30B 모델, 8K seq</strong></td> <td>165 TFLOPs</td> <td>45 TFLOPs</td> <td>3.67x</td> <td><strong>주장 초과</strong></td> </tr> <tr> <td><strong>7B Sparse, 256K seq</strong></td> <td>65 TFLOPs</td> <td>OOM</td> <td>∞</td> <td><strong>성능 붕괴 우려</strong></td> </tr> </tbody> </table> <p><strong>핵심 통찰:</strong></p> <ul> <li> <strong>가변 성능</strong>: 속도향상이 1.35x에서 3.67x까지 다양, 균일한 “2.5x” 주장과 모순</li> <li> <strong>능력 vs 성능</strong>: 명확한 능력 해제 (더 긴 시퀀스) vs 혼재된 성능 향상</li> <li> <strong>Sparse Attention 문제</strong>: 상당한 성능 저하로 “attention-agnostic” 주장이 과장됨</li> </ul> <h3 id="스케일링-분석">스케일링 분석</h3> <table> <thead> <tr> <th>연구 유형</th> <th>구성</th> <th>효율성</th> <th>해석</th> </tr> </thead> <tbody> <tr> <td><strong>Strong Scaling</strong></td> <td>131K seq, 64→256 GPUs</td> <td>165→136 TFLOPs (18% 손실)</td> <td> <strong>통신 오버헤드</strong>가 O(N/P) 이론과 모순</td> </tr> <tr> <td><strong>Proportional Scaling</strong></td> <td>Seq∝GPUs, 65K→262K</td> <td>161→147 TFLOPs (9% 손실)</td> <td> <strong>더 나은 그러나 완벽하지 않은</strong> 스케일링</td> </tr> </tbody> </table> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 스케일링 결과는 숨겨진 비용을 드러냄:
</span><span class="k">def</span> <span class="nf">real_communication_complexity</span><span class="p">(</span><span class="n">N</span><span class="p">,</span> <span class="n">P</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    실제 통신은 오버헤드 요소들을 포함
    </span><span class="sh">"""</span>
    <span class="n">theoretical</span> <span class="o">=</span> <span class="n">N</span> <span class="o">/</span> <span class="n">P</span>
    <span class="n">network_contention</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">P</span>  <span class="c1"># 디바이스 수에 따라 증가
</span>    <span class="k">return</span> <span class="n">theoretical</span> <span class="o">+</span> <span class="n">network_contention</span>

<span class="c1"># 이것이 strong scaling 실험에서 효율성 손실을 설명함
</span></code></pre></div></div> <h3 id="통계적-엄밀성-부족">통계적 엄밀성 부족</h3> <p><strong>중요한 결함</strong>: 오차막대, 신뢰구간, 또는 다중 실행 보고 없음. 성능 주장을 하는 시스템 논문에서 이는 신뢰성을 훼손합니다.</p> <h2 id="강점과-제약">강점과 제약</h2> <h3 id="주요-강점">주요 강점</h3> <ol> <li> <strong>근본적 능력 해제</strong>: 이전보다 4배 긴 시퀀스 훈련 가능</li> <li> <strong>품질 보존</strong>: 수렴 연구에서 모델 품질에 영향 없음을 확인</li> <li> <strong>광범위한 적용성</strong>: 다양한 모델 크기에서 작동, 기존 최적화와 통합</li> <li> <strong>이론적 기반</strong>: 시퀀스 병렬처리를 위한 통신 복잡도 분석 제공</li> </ol> <h3 id="중요한-제약">중요한 제약</h3> <ol> <li> <strong>성능 불일치</strong>: 구성에 따라 개선이 극적으로 다름</li> <li> <strong>인프라 의존성</strong>: 고대역폭, 저지연 네트워크 필요</li> <li> <strong>Sparse Attention 문제</strong>: 일반성 주장과 모순되는 상당한 성능 저하</li> <li> <strong>실험 엄밀성</strong>: 성능 주장의 통계적 검증 부족</li> </ol> <h3 id="숨겨진-복잡성">숨겨진 복잡성</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">performance_prediction</span><span class="p">(</span><span class="n">model_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">,</span> <span class="n">world_size</span><span class="p">,</span> <span class="n">network_quality</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    성능 향상은 단순한 메트릭으로 예측 불가능
    </span><span class="sh">"""</span>
    <span class="c1"># 실제 성능에 영향을 주는 요소들:
</span>    <span class="n">communication_ratio</span> <span class="o">=</span> <span class="nf">estimate_comm_vs_compute</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
    <span class="n">memory_pressure</span> <span class="o">=</span> <span class="nf">check_memory_bottlenecks</span><span class="p">(</span><span class="n">model_size</span><span class="p">,</span> <span class="n">seq_len</span><span class="p">)</span>
    <span class="n">network_efficiency</span> <span class="o">=</span> <span class="nf">evaluate_all_to_all_performance</span><span class="p">(</span><span class="n">network_quality</span><span class="p">)</span>
    <span class="n">load_balance</span> <span class="o">=</span> <span class="nf">assess_workload_distribution</span><span class="p">(</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">world_size</span><span class="p">)</span>
    
    <span class="c1"># 성능은 모든 요소들의 복잡한 상호작용에 의존
</span>    <span class="k">return</span> <span class="nf">complex_interaction</span><span class="p">(</span><span class="n">communication_ratio</span><span class="p">,</span> <span class="n">memory_pressure</span><span class="p">,</span> 
                             <span class="n">network_efficiency</span><span class="p">,</span> <span class="n">load_balance</span><span class="p">)</span>
</code></pre></div></div> <h2 id="실용적-함의">실용적 함의</h2> <h3 id="deepspeed-ulysses-사용-시점">DeepSpeed-Ulysses 사용 시점</h3> <p><strong>강력한 후보:</strong></p> <ul> <li>32K 토큰 이상의 시퀀스 훈련</li> <li>Dense attention 패턴</li> <li>고대역폭 클러스터 인프라</li> <li>원시 성능보다 능력 해제(더 긴 시퀀스)가 중요한 애플리케이션</li> </ul> <p><strong>부적합한 후보:</strong></p> <ul> <li>짧은 시퀀스 (&lt; 8K 토큰) - 오버헤드가 지배적</li> <li>Sparse attention 패턴 - 성능 저하</li> <li>제한된 네트워크 대역폭 - 통신이 병목</li> <li>지연시간에 민감한 추론 - 이 용도로 설계되지 않음</li> </ul> <h3 id="구현-체크리스트">구현 체크리스트</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">deployment_readiness_check</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    성공적인 배포를 위한 전제조건
    </span><span class="sh">"""</span>
    <span class="n">checks</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">"</span><span class="s">sequence_divisibility</span><span class="sh">"</span><span class="p">:</span> <span class="n">sequence_length</span> <span class="o">%</span> <span class="n">world_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">head_divisibility</span><span class="sh">"</span><span class="p">:</span> <span class="n">num_heads</span> <span class="o">%</span> <span class="n">world_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">network_bandwidth</span><span class="sh">"</span><span class="p">:</span> <span class="n">bandwidth</span> <span class="o">&gt;</span> <span class="n">world_size</span> <span class="o">*</span> <span class="mi">10</span><span class="p">,</span>  <span class="c1"># GB/s
</span>        <span class="sh">"</span><span class="s">memory_capacity</span><span class="sh">"</span><span class="p">:</span> <span class="nf">can_fit_attention_matrix</span><span class="p">(),</span>
        <span class="sh">"</span><span class="s">load_balance</span><span class="sh">"</span><span class="p">:</span> <span class="nf">validate_uniform_hardware</span><span class="p">(),</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="nf">all</span><span class="p">(</span><span class="n">checks</span><span class="p">.</span><span class="nf">values</span><span class="p">()),</span> <span class="n">checks</span>
</code></pre></div></div> <h2 id="향후-방향과-연구-기회">향후 방향과 연구 기회</h2> <h3 id="기술적-개선">기술적 개선</h3> <ol> <li> <strong>적응적 통신</strong>: 네트워크 조건에 기반한 동적 조정</li> <li> <strong>이기종 최적화</strong>: 혼합 하드웨어 환경 지원</li> <li> <strong>Sparse Attention 통합</strong>: 구조화된 sparsity 패턴에 대한 더 나은 지원</li> <li> <strong>메모리 최적화</strong>: all-to-all 연산 중 최대 메모리 감소</li> </ol> <h3 id="이론적-확장">이론적 확장</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 잠재적 연구 방향:
</span><span class="k">def</span> <span class="nf">future_work_opportunities</span><span class="p">():</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">최적 헤드 분산 전략</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">통신-계산 중첩 기법</span><span class="sh">"</span><span class="p">,</span> 
        <span class="sh">"</span><span class="s">다른 병렬처리 차원과의 통합</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">실제 네트워크 효과의 이론적 분석</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">다른 어텐션 메커니즘으로의 확장 (예: cross-attention)</span><span class="sh">"</span>
    <span class="p">]</span>
</code></pre></div></div> <h3 id="더-넓은-영향">더 넓은 영향</h3> <p>이 연구는 Transformer 훈련 스케일링에 대한 사고방식의 <strong>패러다임 전환</strong>을 나타냅니다:</p> <ul> <li> <strong>이전</strong>: 배치 크기, 모델 크기, 또는 깊이 스케일링</li> <li> <strong>이후</strong>: 시퀀스 길이를 일급 시민으로 스케일링</li> </ul> <p>이는 완전히 새로운 애플리케이션 클래스와 긴 컨텍스트 AI 시스템의 연구 방향을 가능하게 합니다.</p> <h2 id="최종-평가">최종 평가</h2> <p>DeepSpeed-Ulysses는 AI 시스템 환경에서 실제적이고 중요한 문제를 해결하는 <strong>중요한 기여</strong>입니다. 실험 검증에 공백이 있고 성능 주장이 다소 과장되었지만, 핵심 혁신은 건전하고 능력 해제는 진정한 것입니다.</p> <p><strong>실무자들을 위해</strong>: 특정 사용 사례(긴 시퀀스, dense attention, 좋은 인프라)에는 가치 있는 도구이지만 범용 솔루션은 아닙니다.</p> <p><strong>연구자들을 위해</strong>: 어텐션 중심 병렬처리 접근법은 시스템 최적화에서 새로운 길을 열고 시퀀스 스케일링의 향후 연구를 위한 기반을 제공합니다.</p> <p><strong>결론</strong>: 새로운 능력을 가능하게 하는 AI 시스템의 의미 있는 발전이며, 구현 견고성과 실험 엄밀성 모두에서 개선의 여지가 있습니다.</p> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa3/">Flash Attention 3</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa2/">Flash Attention 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa/">Flash Attention</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/usp/">Unified Sequence Parallelism</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> </li> <div id="disqus_thread" style="max-width: 930px; margin: 0 auto;"> <script type="text/javascript">
    var disqus_shortname  = 'sungyubkim';
    var disqus_identifier = '/blog/deepspeed_ulysses';
    var disqus_title      = "DeepSpeed Ulysses";
    (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Sung-Yub Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 29, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>