<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Ring Self-Attention | Sung-Yub Kim </title> <meta name="author" content="Sung-Yub Kim"> <meta name="description" content="Ring Self-Attention enables sequence parallelism across GPUs using ring communication patterns for distributed attention."> <meta name="keywords" content="machine-learning, deep-learning, llm, foundation-models, research"> <meta property="og:site_name" content="Sung-Yub Kim"> <meta property="og:type" content="article"> <meta property="og:title" content="Sung-Yub Kim | Ring Self-Attention"> <meta property="og:url" content="https://sungyubkim.github.io/blog/ring-self-attention/"> <meta property="og:description" content="Ring Self-Attention enables sequence parallelism across GPUs using ring communication patterns for distributed attention."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Ring Self-Attention"> <meta name="twitter:description" content="Ring Self-Attention enables sequence parallelism across GPUs using ring communication patterns for distributed attention."> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Sung-Yub Kim"
        },
        "url": "https://sungyubkim.github.io/blog/ring-self-attention/",
        "@type": "BlogPosting",
        "description": "Ring Self-Attention enables sequence parallelism across GPUs using ring communication patterns for distributed attention.",
        "headline": "Ring Self-Attention",
        
        "sameAs": ["https://github.com/sungyubkim","https://scholar.google.com/citations?user=m2rhgrkAAAAJ","https://www.linkedin.com/in/sung-yub-kim-0a82a1264","https://twitter.com/SungyubK"],
        
        "name": "Sung-Yub Kim",
        "@context": "https://schema.org"
    }
  </script> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sungyubkim.github.io/blog/ring-self-attention/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Sung-Yub</span> Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Ring Self-Attention</h1> <p class="post-meta"> Created on May 30, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/sequence-parallelism"> <i class="fa-solid fa-hashtag fa-sm"></i> sequence-parallelism</a>   <a href="/blog/tag/ring-attention"> <i class="fa-solid fa-hashtag fa-sm"></i> ring-attention</a>   <a href="/blog/tag/distributed-training"> <i class="fa-solid fa-hashtag fa-sm"></i> distributed-training</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="tldr">TL;DR</h1> <blockquote> <p>이 논문은 <strong>시퀀스 병렬화(sequence parallelism)</strong>를 소개합니다 - 전체 시퀀스를 하나의 GPU에 저장해야 하는 기존 방식 대신, 시퀀스를 여러 GPU에 분할하여 더 긴 시퀀스로 Transformer 모델을 훈련하는 새로운 방법입니다. 핵심 혁신은 영리한 링 통신 패턴을 통해 분산 어텐션 계산을 가능하게 하는 <strong>링 셀프 어텐션(Ring Self-Attention, RSA)</strong>입니다.</p> <p><strong>주요 기여:</strong></p> <ul> <li>새로운 병렬화 차원: 시퀀스 길이 (기존 데이터/모델 병렬화와 대비)</li> <li>분산 어텐션 계산을 위한 링 셀프 어텐션 알고리즘</li> <li>텐서 병렬화 대비 3배 긴 시퀀스와 13.7배 큰 배치 크기</li> </ul> </blockquote> <ul> <li>Paper Link: <a href="https://arxiv.org/pdf/2105.13120" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/2105.13120</a> </li> </ul> <hr> <h1 id="related-papers">Related Papers</h1> <p><strong>시퀀스 병렬화 발전:</strong></p> <ul> <li> <a href="/blog/blockwise_ringattention/">Blockwise RingAttention</a> - 링 기반 시퀀스 병렬화의 현대적 발전</li> <li> <a href="https://arxiv.org/pdf/2309.14509" rel="external nofollow noopener" target="_blank">DeepSpeed Ulysses</a> - 시퀀스 병렬화의 실용적 구현</li> <li> <a href="/blog/usp/">USP</a> - 통합 시퀀스 병렬화 프레임워크</li> </ul> <p><strong>분산 어텐션:</strong></p> <ul> <li> <a href="https://arxiv.org/pdf/2310.03294" rel="external nofollow noopener" target="_blank">DISTFLASHATTN</a> - 분산 FlashAttention 구현</li> <li> <a href="https://arxiv.org/pdf/2311.09431" rel="external nofollow noopener" target="_blank">Striped Attention</a> - 효율적인 시퀀스 분배 패턴</li> <li> <a href="https://arxiv.org/pdf/2406.18485" rel="external nofollow noopener" target="_blank">LoongTrain</a> - 2D 어텐션 병렬화</li> </ul> <p><strong>병렬화 통합:</strong></p> <ul> <li> <a href="/blog/tp/">Tensor Parallelism</a> - 텐서 병렬화와의 결합</li> <li> <a href="/blog/pp/">GPipe</a> - 파이프라인 병렬화와의 통합</li> <li> <a href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> - 메모리 효율적인 병렬 훈련</li> </ul> <p><strong>긴 컨텍스트 처리:</strong></p> <ul> <li> <a href="https://arxiv.org/pdf/2411.01783" rel="external nofollow noopener" target="_blank">Context Parallelism for Scalable Million-Token Inference</a> - 추론 시 시퀀스 병렬화</li> </ul> <hr> <ul> <li>기존 병렬화 전략과 호환 가능하여 “4차원 병렬화” 구현</li> </ul> <p><strong>비판적 현실 점검:</strong> 실험 검증에 상당한 약점이 있습니다 - 불공정한 기준선, 제한된 범위, 과장된 개선 효과. 이 방법은 일반적인 돌파구라기보다는 특정 영역(많은 GPU에서 중간-긴 시퀀스)을 위한 것입니다.</p> <h1 id="takeaways">Takeaways</h1> <h2 id="문제와-동기">문제와 동기</h2> <h3 id="왜-중요한가">왜 중요한가</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 기존 제약: 전체 시퀀스가 하나의 GPU에 맞아야 함
</span><span class="n">traditional_limit</span> <span class="o">=</span> <span class="n">gpu_memory</span> <span class="o">/</span> <span class="p">(</span><span class="n">sequence_length</span><span class="o">^</span><span class="mi">2</span> <span class="o">*</span> <span class="n">batch_size</span> <span class="o">*</span> <span class="n">hidden_size</span><span class="p">)</span>

<span class="c1"># 의료 영상 예시
</span><span class="n">medical_image</span> <span class="o">=</span> <span class="mi">512</span> <span class="o">*</span> <span class="mi">512</span> <span class="o">*</span> <span class="mi">512</span>  <span class="c1"># 3D 의료 스캔
</span><span class="n">tokens_per_image</span> <span class="o">=</span> <span class="mi">134</span><span class="n">_million</span>    <span class="c1"># 각 복셀이 토큰이 됨
</span><span class="n">memory_required</span> <span class="o">=</span> <span class="sh">"</span><span class="s">단일 GPU로는 불가능</span><span class="sh">"</span>

<span class="c1"># 이것이 시퀀스 병렬화가 해결하는 근본적인 문제
</span></code></pre></div></div> <p><strong>핵심 통찰</strong>: 기존 연구가 알고리즘적 개선(스파스 어텐션, 선형 어텐션)에 집중하는 반면, 이 논문은 <strong>시스템 접근법</strong>을 취합니다 - 어텐션을 더 효율적으로 만드는 대신 더 많은 GPU를 사용하여 긴 시퀀스를 처리합니다.</p> <h3 id="현재-병렬화-환경">현재 병렬화 환경</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">parallelism_strategies</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">data_parallelism</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">배치를 GPU에 분할</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">pipeline_parallelism</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">모델 레이어를 GPU에 분할</span><span class="sh">'</span><span class="p">,</span> 
    <span class="sh">'</span><span class="s">tensor_parallelism</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">모델 매개변수를 GPU에 분할</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">sequence_parallelism</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">시퀀스 길이를 GPU에 분할 (새로운 방식)</span><span class="sh">'</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="기술적-혁신-링-셀프-어텐션">기술적 혁신: 링 셀프 어텐션</h2> <h3 id="핵심-알고리즘-설명">핵심 알고리즘 설명</h3> <p><strong>1단계: 어텐션 스코어 QK^T 계산</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ring_attention_scores_example</span><span class="p">():</span>
    <span class="c1"># 설정: 4개 GPU, 시퀀스 길이 8, 각 GPU가 2개 토큰 처리
</span>    <span class="n">devices</span> <span class="o">=</span> <span class="p">{</span>
        <span class="mi">0</span><span class="p">:</span> <span class="sh">"</span><span class="s">토큰 [0,1]</span><span class="sh">"</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">:</span> <span class="sh">"</span><span class="s">토큰 [2,3]</span><span class="sh">"</span><span class="p">,</span> 
        <span class="mi">2</span><span class="p">:</span> <span class="sh">"</span><span class="s">토큰 [4,5]</span><span class="sh">"</span><span class="p">,</span>
        <span class="mi">3</span><span class="p">:</span> <span class="sh">"</span><span class="s">토큰 [6,7]</span><span class="sh">"</span>
    <span class="p">}</span>
    
    <span class="c1"># Device 1의 관점에서 살펴보기
</span>    <span class="n">device_1_queries</span> <span class="o">=</span> <span class="sh">"</span><span class="s">토큰 [2,3]에 대한 Q</span><span class="sh">"</span>
    
    <span class="c1"># Device 1은 모든 키와 어텐션을 계산해야 함
</span>    <span class="n">ring_steps</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">단계 0: 로컬 K[2,3] 사용 -&gt; Q[2,3] @ K[2,3]^T 계산</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">단계 1: Device 0에서 K[0,1] 받음 -&gt; Q[2,3] @ K[0,1]^T</span><span class="sh">"</span><span class="p">,</span> 
        <span class="sh">"</span><span class="s">단계 2: Device 3에서 K[6,7] 받음 -&gt; Q[2,3] @ K[6,7]^T</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">단계 3: Device 2에서 K[4,5] 받음 -&gt; Q[2,3] @ K[4,5]^T</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="c1"># 결과: Device 1은 토큰 [2,3]이 모든 토큰 [0-7]에 어텐션하는 스코어를 가짐
</span>    <span class="n">attention_matrix_shape</span> <span class="o">=</span> <span class="sh">"</span><span class="s">[배치, 2_로컬_토큰, 8_전체_토큰]</span><span class="sh">"</span>
    
    <span class="k">return</span> <span class="sa">f</span><span class="sh">"</span><span class="s">Device 1이 완전한 어텐션 스코어 계산: </span><span class="si">{</span><span class="n">attention_matrix_shape</span><span class="si">}</span><span class="sh">"</span>
</code></pre></div></div> <p><strong>2단계: 값을 사용한 최종 출력 계산</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">ring_attention_output_example</span><span class="p">():</span>
    <span class="c1"># Device 1은 어텐션 가중치를 가짐: [배치, 2, 8] 
</span>    <span class="c1"># 이제 모든 값 임베딩과 곱해야 함
</span>    
    <span class="n">ring_steps</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">단계 0: 로컬 V[2,3] 사용 -&gt; weights[:,:,2:4] @ V[2,3]</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">단계 1: V[0,1] 받음 -&gt; weights[:,:,0:2] @ V[0,1]</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">단계 2: V[6,7] 받음 -&gt; weights[:,:,6:8] @ V[6,7]</span><span class="sh">"</span><span class="p">,</span> 
        <span class="sh">"</span><span class="s">단계 3: V[4,5] 받음 -&gt; weights[:,:,4:6] @ V[4,5]</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="n">final_output</span> <span class="o">=</span> <span class="sh">"</span><span class="s">모든 부분 출력의 합 = 토큰 [2,3]에 대한 완전한 어텐션</span><span class="sh">"</span>
    <span class="k">return</span> <span class="n">final_output</span>
</code></pre></div></div> <h3 id="중요한-구현-요구사항">중요한 구현 요구사항</h3> <p><strong>메모리 관리</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">memory_requirements</span><span class="p">():</span>
    <span class="c1"># 중요: 어텐션 행렬은 여전히 전체 시퀀스 차원이 필요
</span>    <span class="n">peak_memory_components</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">attention_scores</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">batch_size × local_seq_len × 전체_seq_len</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">attention_weights</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">batch_size × local_seq_len × 전체_seq_len</span><span class="sh">'</span><span class="p">,</span> 
        <span class="sh">'</span><span class="s">temp_embeddings</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">2 × batch_size × local_seq_len × hidden_size</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">gradients</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">역전파 중 추가 2배</span><span class="sh">'</span>
    <span class="p">}</span>
    
    <span class="c1"># 경고: 매우 긴 시퀀스의 경우 어텐션 행렬이 여전히 OOM 유발 가능
</span>    <span class="n">memory_scaling</span> <span class="o">=</span> <span class="sh">"</span><span class="s">O(sequence_length^2) 대신 O(sequence_length), 하지만 주의사항 있음</span><span class="sh">"</span>
    <span class="k">return</span> <span class="n">memory_scaling</span>
</code></pre></div></div> <p><strong>동기화 요구사항</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">synchronization_constraints</span><span class="p">():</span>
    <span class="n">assumptions</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">모든 장치가 링 통신에 참여해야 함</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">링 통신은 동기식이어야 함</span><span class="sh">"</span><span class="p">,</span> 
        <span class="sh">"</span><span class="s">어떤 장치 실패든 전체 링을 중단시킴</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">시퀀스 길이가 장치 수로 나누어떨어져야 함</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">모든 장치가 동일한 로컬 계산 시간을 가져야 함</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="n">failure_modes</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">장치 실패 -&gt; 전체 훈련 중단</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">네트워크 분할 -&gt; 링 통신 실패</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">로드 불균형 -&gt; 가장 느린 장치가 속도 결정</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">메모리 파편화 -&gt; 일관성 없는 성능</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="k">return</span> <span class="n">assumptions</span><span class="p">,</span> <span class="n">failure_modes</span>
</code></pre></div></div> <h2 id="실험-결과-분석">실험 결과 분석</h2> <h3 id="주요-결과와-비판적-해석">주요 결과와 비판적 해석</h3> <table> <thead> <tr> <th>실험</th> <th>기준선</th> <th>시퀀스 병렬화</th> <th>주장된 개선</th> <th><strong>현실 점검</strong></th> </tr> </thead> <tbody> <tr> <td><strong>최대 배치 크기</strong></td> <td>116 (12 GPUs)</td> <td>1,590 (64 GPUs)</td> <td>13.7×</td> <td> <strong>불공정</strong>: 5배 많은 GPU 사용. 실제 알고리즘 개선은 ~2-3×</td> </tr> <tr> <td><strong>최대 시퀀스 길이</strong></td> <td>750</td> <td>2,250</td> <td>3.0×</td> <td> <strong>보통</strong>: 합리적 개선이지만 혁명적이지 않음</td> </tr> <tr> <td><strong>처리량</strong></td> <td>18K tokens/sec</td> <td>24K tokens/sec</td> <td>1.3×</td> <td> <strong>미미함</strong>: 통신 오버헤드를 겨우 보상</td> </tr> <tr> <td><strong>스파스 어텐션</strong></td> <td>4.2K tokens</td> <td>114K tokens</td> <td>27×</td> <td> <strong>오해의 소지</strong>: 분산 vs 단일장치 비교, 알고리즘 비교 아님</td> </tr> </tbody> </table> <h3 id="부족한-절제-연구">부족한 절제 연구</h3> <table> <thead> <tr> <th>구성요소</th> <th>테스트된 것</th> <th><strong>누락된 것</strong></th> </tr> </thead> <tbody> <tr> <td>링 통신</td> <td>기본 기능</td> <td>all-gather, all-reduce 대안과 <strong>비교 없음</strong> </td> </tr> <tr> <td>2단계 설계</td> <td>정확성 검증</td> <td>단일 단계 또는 다른 분해의 <strong>분석 없음</strong> </td> </tr> <tr> <td>통신 패턴</td> <td>링 토폴로지</td> <td>트리, 버터플라이, 메시 패턴 <strong>평가 없음</strong> </td> </tr> <tr> <td>메모리 vs 통신</td> <td>정성적 논의</td> <td> <strong>체계적</strong> 트레이드오프 분석 없음</td> </tr> </tbody> </table> <h2 id="비판적-평가">비판적 평가</h2> <h3 id="강점">강점</h3> <ol> <li> <strong>새로운 문제 프레이밍</strong>: 시퀀스 길이를 병렬화 차원으로 체계적으로 다룬 첫 연구</li> <li> <strong>견고한 기술적 혁신</strong>: 링 셀프 어텐션이 수학적으로 정확하고 우아함</li> <li> <strong>구현 가능성</strong>: 순수 PyTorch, 특별한 컴파일러 불필요</li> <li> <strong>호환성</strong>: 기존 병렬화 전략과 함께 작동</li> </ol> <h3 id="주요-한계점">주요 한계점</h3> <p><strong>실험 검증 문제</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">experimental_problems</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">baseline_bias</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">시퀀스 스케일링에 텐서 병렬화와 비교</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">missing_baselines</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">그래디언트 체크포인팅, 혼합 정밀도와 비교 없음</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">hardware_dated</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">2016년 P100 GPU에서만 테스트</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">limited_scope</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">Wikipedia의 BERT만, 다양한 워크로드 없음</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">statistical_rigor</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">오차 막대, 분산 분석, 유의성 테스트 없음</span><span class="sh">'</span>
<span class="p">}</span>
</code></pre></div></div> <p><strong>실제 배포 과제</strong></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">deployment_reality</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">hardware_requirements</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">고대역폭 인터커넥트(InfiniBand) 필수</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">optimal_scale</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">16-32 GPU (이후 통신 오버헤드)</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">sweet_spot</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">중간 시퀀스 (1K-4K 토큰), 짧거나 매우 긴 것 아님</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">fault_tolerance</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">단일 장치 실패가 전체 훈련 중단</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">complexity</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">대안 대비 상당한 구현 복잡성</span><span class="sh">'</span>
<span class="p">}</span>
</code></pre></div></div> <h2 id="실용적-함의">실용적 함의</h2> <h3 id="시퀀스-병렬화를-언제-사용할지">시퀀스 병렬화를 언제 사용할지</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">should_use_sequence_parallelism</span><span class="p">(</span><span class="n">use_case</span><span class="p">):</span>
    <span class="n">good_fit</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">4K+ 토큰 시퀀스를 가진 의료 영상</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">긴 문서 처리 (법률, 과학 논문)</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">유전체 시퀀스 분석</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">비전 트랜스포머를 사용한 고해상도 이미지 분석</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="n">poor_fit</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">표준 NLP 작업 (≤512 토큰)</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">일반적인 이미지 크기의 컴퓨터 비전</span><span class="sh">"</span><span class="p">,</span> 
        <span class="sh">"</span><span class="s">자원 제약 환경</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">높은 신뢰성이 필요한 프로덕션 시스템</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="n">considerations</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">minimum_gpus</span><span class="sh">'</span><span class="p">:</span> <span class="mi">4</span><span class="p">,</span>  <span class="c1"># 이하에서는 오버헤드가 지배적
</span>        <span class="sh">'</span><span class="s">network_requirements</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">고대역폭, 저지연 인터커넥트</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">sequence_length</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">최적 지점: 1K-8K 토큰</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">batch_size</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">큰 배치가 이익 증폭</span><span class="sh">'</span>
    <span class="p">}</span>
    
    <span class="k">return</span> <span class="n">good_fit</span><span class="p">,</span> <span class="n">poor_fit</span><span class="p">,</span> <span class="n">considerations</span>
</code></pre></div></div> <h3 id="구현-가이드">구현 가이드</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">SequenceParallelismChecklist</span><span class="p">:</span>
    <span class="n">prerequisites</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">고대역폭 네트워크 (InfiniBand 권장)</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">동일한 하드웨어 (같은 GPU)</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">GPU 수로 나누어떨어지는 시퀀스 길이</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">견고한 분산 훈련 인프라</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="n">optimization_tips</span> <span class="o">=</span> <span class="p">[</span>
        <span class="sh">"</span><span class="s">링 통신 버퍼 미리 할당</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">가능한 곳에 비동기 통신 사용</span><span class="sh">"</span><span class="p">,</span> 
        <span class="sh">"</span><span class="s">메모리용 그래디언트 체크포인팅 구현</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">장치 간 로드 불균형 모니터링</span><span class="sh">"</span><span class="p">,</span>
        <span class="sh">"</span><span class="s">장치 실패에 대한 대안 전략 보유</span><span class="sh">"</span>
    <span class="p">]</span>
    
    <span class="n">performance_tuning</span> <span class="o">=</span> <span class="p">{</span>
        <span class="sh">'</span><span class="s">batch_size</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">메모리 한계까지 증가</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">sequence_chunks</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">계산 vs 통신 균형</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">ring_ordering</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">네트워크 토폴로지에 최적화</span><span class="sh">'</span><span class="p">,</span>
        <span class="sh">'</span><span class="s">memory_management</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">버퍼 풀로 파편화 방지</span><span class="sh">'</span>
    <span class="p">}</span>
</code></pre></div></div> <h2 id="향후-연구-방향">향후 연구 방향</h2> <h3 id="즉각적인-개선-필요사항">즉각적인 개선 필요사항</h3> <ol> <li> <strong>엄격한 실험 검증</strong>: 공정한 기준선, 최신 하드웨어, 다양한 워크로드</li> <li> <strong>장애 내성</strong>: 장치 실패를 우아하게 처리</li> <li> <strong>동적 로드 밸런싱</strong>: 이질적 하드웨어에 적응</li> <li> <strong>통신 최적화</strong>: 단순한 링 패턴을 넘어서</li> </ol> <h3 id="장기-연구-질문">장기 연구 질문</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">future_directions</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">algorithmic_fusion</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">스파스 어텐션, 선형 어텐션과 결합</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">adaptive_partitioning</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">어텐션 패턴 기반 동적 시퀀스 분할</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">cross_modal_scaling</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">멀티모달 트랜스포머로 확장</span><span class="sh">'</span><span class="p">,</span> 
    <span class="sh">'</span><span class="s">efficiency_optimization</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">통신 오버헤드 감소</span><span class="sh">'</span><span class="p">,</span>
    <span class="sh">'</span><span class="s">theoretical_analysis</span><span class="sh">'</span><span class="p">:</span> <span class="sh">'</span><span class="s">형식적 복잡도 분석과 최적성 경계</span><span class="sh">'</span>
<span class="p">}</span>
</code></pre></div></div> <h3 id="최신-기법과의-통합">최신 기법과의 통합</h3> <p>이 논문은 많은 중요한 발전 이전에 나왔습니다:</p> <ul> <li> <strong>Zero Redundancy Optimizer (ZeRO)</strong>: 메모리 최적화를 위해 결합 가능</li> <li> <strong>그래디언트 체크포인팅</strong>: 실용적 배포에 필수</li> <li> <strong>혼합 정밀도 훈련</strong>: 현재 표준 관행</li> <li> <strong>효율적 어텐션 변형</strong>: FlashAttention, LinearAttention 등</li> </ul> <h2 id="결론">결론</h2> <p><strong>시퀀스 병렬화는 병렬화 환경에서 특정 틈새를 채우는 견고한 기술적 기여를 나타냅니다.</strong> 링 셀프 어텐션 알고리즘은 우아하고 수학적으로 건전합니다. 그러나 실용적 영향은 논문이 주장하는 것보다 제한적입니다.</p> <p><strong>실무자들을 위해</strong>: 중간에서 긴 시퀀스(1K-8K 토큰), 충분한 하드웨어(고대역폭 네트워킹을 가진 16+ GPU), 그리고 시퀀스 길이가 주요 병목인 워크로드가 있을 때 시퀀스 병렬화를 고려하세요.</p> <p><strong>연구자들을 위해</strong>: 이 연구는 시퀀스 길이를 스케일링의 새로운 차원으로 열어주지만, 그래디언트 체크포인팅이나 더 효율적인 어텐션 메커니즘 같은 간단한 대안과 실질적으로 경쟁하려면 상당한 작업이 남아있습니다.</p> <p>근본적인 통찰 - 시스템 수준 솔루션이 알고리즘적 개선을 보완할 수 있다는 것 - 은 가치 있으며 분산 딥러닝의 향후 연구에 영감을 줄 가능성이 높습니다.</p> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa3/">Flash Attention 3</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa2/">Flash Attention 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa/">Flash Attention</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/usp/">Unified Sequence Parallelism</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> </li> <div id="disqus_thread" style="max-width: 930px; margin: 0 auto;"> <script type="text/javascript">
    var disqus_shortname  = 'sungyubkim';
    var disqus_identifier = '/blog/ring-self-attention';
    var disqus_title      = "Ring Self-Attention";
    (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Sung-Yub Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 29, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>