<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Flash Attention | Sung-Yub Kim </title> <meta name="author" content="Sung-Yub Kim"> <meta name="description" content="FlashAttention: IO-aware attention algorithm achieving 2-3x speedup with 10-20x memory reduction via tiling and online softmax."> <meta name="keywords" content="machine-learning, deep-learning, llm, foundation-models, research"> <meta property="og:site_name" content="Sung-Yub Kim"> <meta property="og:type" content="article"> <meta property="og:title" content="Sung-Yub Kim | Flash Attention"> <meta property="og:url" content="https://sungyubkim.github.io/blog/fa/"> <meta property="og:description" content="FlashAttention: IO-aware attention algorithm achieving 2-3x speedup with 10-20x memory reduction via tiling and online softmax."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Flash Attention"> <meta name="twitter:description" content="FlashAttention: IO-aware attention algorithm achieving 2-3x speedup with 10-20x memory reduction via tiling and online softmax."> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Sung-Yub Kim"
        },
        "url": "https://sungyubkim.github.io/blog/fa/",
        "@type": "BlogPosting",
        "description": "FlashAttention: IO-aware attention algorithm achieving 2-3x speedup with 10-20x memory reduction via tiling and online softmax.",
        "headline": "Flash Attention",
        
        "sameAs": ["https://github.com/sungyubkim","https://scholar.google.com/citations?user=m2rhgrkAAAAJ","https://www.linkedin.com/in/sung-yub-kim-0a82a1264","https://twitter.com/SungyubK"],
        
        "name": "Sung-Yub Kim",
        "@context": "https://schema.org"
    }
  </script> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sungyubkim.github.io/blog/fa/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Sung-Yub</span> Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Flash Attention</h1> <p class="post-meta"> Created on June 03, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/attention"> <i class="fa-solid fa-hashtag fa-sm"></i> attention</a>   <a href="/blog/tag/flash-attention"> <i class="fa-solid fa-hashtag fa-sm"></i> flash-attention</a>   <a href="/blog/tag/memory-efficiency"> <i class="fa-solid fa-hashtag fa-sm"></i> memory-efficiency</a>   <a href="/blog/tag/inference"> <i class="fa-solid fa-hashtag fa-sm"></i> inference</a>   <a href="/blog/tag/hardware-optimization"> <i class="fa-solid fa-hashtag fa-sm"></i> hardware-optimization</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="tldr">TL;DR</h1> <blockquote> <p><strong>FlashAttention</strong>은 표준 구현보다 2-3배 빠르면서 10-20배 적은 메모리를 사용하는 IO-aware 어텐션 알고리즘입니다. GPU 메모리에 거대한 N×N 어텐션 행렬을 만드는 대신, <strong>타일링(tiling)</strong>과 <strong>온라인 소프트맥스(online softmax)</strong>를 사용하여 빠른 온칩 SRAM에 맞는 작은 블록 단위로 어텐션을 처리합니다. 주요 기여사항:</p> <ol> <li> <strong>IO-Awareness</strong>: 어텐션의 실제 병목이 연산이 아닌 메모리 대역폭임을 파악</li> <li> <strong>타일링 알고리즘</strong>: 느린 메모리 접근을 최소화하기 위해 블록 단위로 처리</li> <li> <strong>정확한 계산</strong>: 근사 방법과 달리 정확한 어텐션을 계산하면서도 더 빠른 속도 달성</li> <li> <strong>선형 메모리</strong>: 메모리 복잡도를 O(N²)에서 O(N)으로 줄여 16K+ 시퀀스 길이 가능</li> <li> <strong>실용적 성과</strong>: MLPerf BERT 기록보다 15% 빠름, GPT-2 학습 3배 가속, Path-X/Path-256 최초 해결</li> </ol> </blockquote> <ul> <li><a href="https://arxiv.org/pdf/2205.14135" rel="external nofollow noopener" target="_blank">Paper Link</a></li> </ul> <hr> <p><strong>FlashAttention 시리즈 발전:</strong></p> <ul> <li> <a href="/blog/fa2/">FlashAttention-2</a> - 2배 빠른 속도와 향상된 병렬화</li> <li> <a href="/blog/fa3/">FlashAttention-3</a> - 비대칭 어텐션과 FP8 지원</li> <li> <a href="https://courses.cs.washington.edu/courses/cse599m/23sp/notes/flashattn.pdf" rel="external nofollow noopener" target="_blank">From Online Softmax to FlashAttention</a> - 이론적 기초와 온라인 소프트맥스</li> </ul> <p><strong>긴 시퀀스 처리:</strong></p> <ul> <li> <a href="https://arxiv.org/pdf/2411.01783" rel="external nofollow noopener" target="_blank">Context Parallelism for Scalable Million-Token Inference</a> - 추론 시 컨텍스트 분산</li> <li> <a href="https://arxiv.org/pdf/2310.05209" rel="external nofollow noopener" target="_blank">Scaling Laws of RoPE-based Extrapolation</a> - 위치 인코딩 확장</li> <li> <a href="https://arxiv.org/pdf/2309.00071" rel="external nofollow noopener" target="_blank">YaRN</a> - RoPE 기반 컨텍스트 길이 확장</li> <li> <a href="https://arxiv.org/pdf/2104.09864" rel="external nofollow noopener" target="_blank">RoFormer</a> - 회전 위치 임베딩</li> </ul> <p><strong>시스템 최적화:</strong></p> <ul> <li> <a href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> - 메모리 효율적인 병렬 훈련</li> <li> <a href="/blog/usp/">USP</a> - 통합 시퀀스 병렬화 프레임워크</li> <li> <a href="/blog/tp/">Tensor Parallelism</a> - 텐서 병렬화와의 결합</li> <li> <a href="/blog/pp/">GPipe</a> - 파이프라인 병렬화와의 통합</li> </ul> <hr> <h1 id="takeaways">Takeaways</h1> <h2 id="1-문제-표준-어텐션이-느린-이유">1. 문제: 표준 어텐션이 느린 이유</h2> <h3 id="메모리-계층-구조의-불일치">메모리 계층 구조의 불일치</h3> <p>이 논문은 <strong>대부분의 ML 연구자들이 놓치는</strong> 중요한 관찰로 시작합니다: 현대 GPU는 어텐션에서 연산 제한(compute-bound)이 아닌 메모리 제한(memory-bound) 상태입니다. 그 이유는:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 표준 어텐션 - 실제로 일어나는 일
</span><span class="k">def</span> <span class="nf">standard_attention_reality</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    Q, K, V: [batch, seq_len, dim]
    실제로 시간이 소요되는 부분을 보여줍니다
    </span><span class="sh">"""</span>
    <span class="c1"># 단계 1: 점수 계산 - 빠름 (텐서 코어 사용)
</span>    <span class="n">S</span> <span class="o">=</span> <span class="n">Q</span> <span class="o">@</span> <span class="n">K</span><span class="p">.</span><span class="n">T</span>  <span class="c1"># [seq_len, seq_len] - 하지만 느린 HBM에 써야 함!
</span>    
    <span class="c1"># 단계 2: 소프트맥스 - 느림 (메모리 제한)
</span>    <span class="c1"># 전체 S를 HBM에서 읽고, exp() 계산하고, 다시 쓰기
</span>    <span class="n">P</span> <span class="o">=</span> <span class="nf">softmax</span><span class="p">(</span><span class="n">S</span><span class="p">)</span>  <span class="c1"># seq_len² 개의 원소를 읽고/쓰기
</span>    
    <span class="c1"># 단계 3: 가중 합 - 연산은 빠르지만 메모리는 느림
</span>    <span class="n">O</span> <span class="o">=</span> <span class="n">P</span> <span class="o">@</span> <span class="n">V</span>  <span class="c1"># 거대한 P 행렬을 HBM에서 읽어야 함
</span>    
    <span class="c1"># 총 HBM 접근: O(seq_len²) - 이것이 병목!
</span></code></pre></div></div> <p><strong>제 통찰</strong>: 논문은 우리가 잘못된 것을 최적화하고 있다는 점을 훌륭하게 파악했습니다. 모두가 FLOPs(부동소수점 연산)를 줄이는 데 집중하는 동안, 실제 문제는 데이터 이동입니다. A100 GPU에서:</p> <ul> <li>행렬 곱셈: 312 TFLOPS</li> <li>메모리 대역폭: 1.5 TB/s</li> <li>어텐션의 경우, 312 TFLOPS가 아닌 1.5 TB/s에 제한됩니다!</li> </ul> <h3 id="동기-부여-실험">동기 부여 실험</h3> <p>저자들은 아마 다음과 같은 실험으로 연구 동기를 얻었을 것입니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 표준 어텐션 프로파일링으로 병목 지점 찾기
</span><span class="k">def</span> <span class="nf">profile_attention</span><span class="p">(</span><span class="n">seq_len</span><span class="p">):</span>
    <span class="c1"># 각 구성요소에서 소요되는 시간
</span>    <span class="n">compute_time</span> <span class="o">=</span> <span class="n">seq_len</span><span class="err">²</span> <span class="o">*</span> <span class="n">time_per_flop</span>
    <span class="n">memory_time</span> <span class="o">=</span> <span class="n">seq_len</span><span class="err">²</span> <span class="o">*</span> <span class="n">matrix_size</span> <span class="o">*</span> <span class="n">time_per_byte_transferred</span>
    
    <span class="c1"># A100에서 seq_len=2048인 경우:
</span>    <span class="c1"># 연산: ~0.5ms (활용도 낮음)
</span>    <span class="c1"># 메모리: ~10ms (병목!)
</span>    
    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">연산 활용도: </span><span class="si">{</span><span class="n">compute_time</span><span class="o">/</span><span class="n">total_time</span> <span class="o">*</span> <span class="mi">100</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>  <span class="c1"># ~5%
</span>    <span class="nf">print</span><span class="p">(</span><span class="sa">f</span><span class="sh">"</span><span class="s">메모리 제한 시간: </span><span class="si">{</span><span class="n">memory_time</span><span class="o">/</span><span class="n">total_time</span> <span class="o">*</span> <span class="mi">100</span><span class="si">}</span><span class="s">%</span><span class="sh">"</span><span class="p">)</span>     <span class="c1"># ~95%
</span></code></pre></div></div> <h2 id="2-해결책-flashattention-알고리즘">2. 해결책: FlashAttention 알고리즘</h2> <h3 id="핵심-아이디어-빠른-메모리에서-작업하기">핵심 아이디어: 빠른 메모리에서 작업하기</h3> <p>핵심 통찰은 데이터를 가능한 한 SRAM(빠른 온칩 메모리)에 유지하는 것입니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># GPU 메모리 계층 구조 (A100 예시)
</span><span class="n">SRAM_per_SM</span> <span class="o">=</span> <span class="mi">192</span> <span class="o">*</span> <span class="mi">1024</span>  <span class="c1"># 192 KB - 빠름 (19 TB/s)
</span><span class="n">HBM_TOTAL</span> <span class="o">=</span> <span class="mi">40</span> <span class="o">*</span> <span class="mi">1024</span><span class="o">**</span><span class="mi">3</span>   <span class="c1"># 40 GB - 느림 (1.5 TB/s)
</span>
<span class="c1"># 표준 어텐션은 N×N 행렬을 위해 HBM을 사용해야 함
# FlashAttention은 타일링을 사용해 모든 것을 SRAM에 유지!
</span></code></pre></div></div> <h3 id="flashattention-알고리즘">FlashAttention 알고리즘</h3> <p>구체적인 예시와 함께 알고리즘을 설명합니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">flash_attention</span><span class="p">(</span><span class="n">Q</span><span class="p">,</span> <span class="n">K</span><span class="p">,</span> <span class="n">V</span><span class="p">,</span> <span class="n">block_size</span><span class="o">=</span><span class="mi">128</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    상세한 예시와 함께하는 FlashAttention
    Q, K, V: [1024, 64] (seq_len=1024, dim=64)
    block_size: 128 (128×128 타일 처리)
    </span><span class="sh">"""</span>
    <span class="n">seq_len</span><span class="p">,</span> <span class="n">dim</span> <span class="o">=</span> <span class="n">Q</span><span class="p">.</span><span class="n">shape</span>
    <span class="n">num_blocks</span> <span class="o">=</span> <span class="n">seq_len</span> <span class="o">//</span> <span class="n">block_size</span>  <span class="c1"># 1024/128 = 8 블록
</span>    
    <span class="c1"># 출력과 통계 초기화
</span>    <span class="n">O</span> <span class="o">=</span> <span class="nf">zeros</span><span class="p">([</span><span class="n">seq_len</span><span class="p">,</span> <span class="n">dim</span><span class="p">])</span>
    <span class="n">m</span> <span class="o">=</span> <span class="nf">full</span><span class="p">([</span><span class="n">seq_len</span><span class="p">],</span> <span class="o">-</span><span class="n">inf</span><span class="p">)</span>  <span class="c1"># 수치 안정성을 위한 최댓값
</span>    <span class="n">l</span> <span class="o">=</span> <span class="nf">zeros</span><span class="p">([</span><span class="n">seq_len</span><span class="p">])</span>       <span class="c1"># 소프트맥스를 위한 지수 합
</span>    
    <span class="c1"># 블록 단위로 처리 - 이것이 핵심!
</span>    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>  <span class="c1"># i = 0,1,...,7
</span>        <span class="c1"># Q 블록 [128, 64]를 SRAM으로 로드
</span>        <span class="n">Q_block</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span>
        
        <span class="c1"># 블록 통계 초기화
</span>        <span class="n">m_block</span> <span class="o">=</span> <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span>
        <span class="n">l_block</span> <span class="o">=</span> <span class="n">l</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span>
        <span class="n">O_block</span> <span class="o">=</span> <span class="n">O</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span>
        
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nf">range</span><span class="p">(</span><span class="n">num_blocks</span><span class="p">):</span>  <span class="c1"># j = 0,1,...,7
</span>            <span class="c1"># K,V 블록 [128, 64]를 SRAM으로 로드
</span>            <span class="n">K_block</span> <span class="o">=</span> <span class="n">K</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span>
            <span class="n">V_block</span> <span class="o">=</span> <span class="n">V</span><span class="p">[</span><span class="n">j</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">j</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span>
            
            <span class="c1"># ---- 모든 계산이 SRAM에서 발생! ----
</span>            
            <span class="c1"># 1. 블록 어텐션 점수 계산 [128, 128]
</span>            <span class="n">S_block</span> <span class="o">=</span> <span class="p">(</span><span class="n">Q_block</span> <span class="o">@</span> <span class="n">K_block</span><span class="p">.</span><span class="n">T</span><span class="p">)</span> <span class="o">/</span> <span class="nf">sqrt</span><span class="p">(</span><span class="n">dim</span><span class="p">)</span>
            
            <span class="c1"># 2. 온라인 소프트맥스 - 실행 중 통계 업데이트
</span>            <span class="n">m_block_new</span> <span class="o">=</span> <span class="nf">maximum</span><span class="p">(</span><span class="n">m_block</span><span class="p">,</span> <span class="n">S_block</span><span class="p">.</span><span class="nf">max</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
            
            <span class="c1"># 3. 새 최댓값으로 지수 계산 (수치 안정성)
</span>            <span class="n">P_block</span> <span class="o">=</span> <span class="nf">exp</span><span class="p">(</span><span class="n">S_block</span> <span class="o">-</span> <span class="n">m_block_new</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
            
            <span class="c1"># 4. 지수 합 업데이트
</span>            <span class="n">l_block_new</span> <span class="o">=</span> <span class="nf">exp</span><span class="p">(</span><span class="n">m_block</span> <span class="o">-</span> <span class="n">m_block_new</span><span class="p">)</span> <span class="o">*</span> <span class="n">l_block</span> <span class="o">+</span> <span class="n">P_block</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># 5. 블록 출력 계산
</span>            <span class="n">O_block_new</span> <span class="o">=</span> <span class="n">P_block</span> <span class="o">@</span> <span class="n">V_block</span>
            
            <span class="c1"># 6. 이전 출력 재조정 후 새 출력 추가
</span>            <span class="n">O_block</span> <span class="o">=</span> <span class="p">(</span><span class="nf">exp</span><span class="p">(</span><span class="n">m_block</span> <span class="o">-</span> <span class="n">m_block_new</span><span class="p">).</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">l_block</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">*</span> <span class="n">O_block</span> <span class="o">+</span> 
                      <span class="n">O_block_new</span><span class="p">)</span> <span class="o">/</span> <span class="n">l_block_new</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
            
            <span class="c1"># 통계 업데이트
</span>            <span class="n">m_block</span> <span class="o">=</span> <span class="n">m_block_new</span>
            <span class="n">l_block</span> <span class="o">=</span> <span class="n">l_block_new</span>
        
        <span class="c1"># 최종 결과를 HBM에 쓰기 (블록당 한 번만!)
</span>        <span class="n">O</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">O_block</span>
        <span class="n">m</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">m_block</span>
        <span class="n">l</span><span class="p">[</span><span class="n">i</span><span class="o">*</span><span class="n">block_size</span><span class="p">:(</span><span class="n">i</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="n">block_size</span><span class="p">]</span> <span class="o">=</span> <span class="n">l_block</span>
    
    <span class="k">return</span> <span class="n">O</span>
</code></pre></div></div> <h3 id="구체적-예시-하나의-블록-처리">구체적 예시: 하나의 블록 처리</h3> <p>특정 블록에서 일어나는 일을 추적해보겠습니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 예시: Q[0:128]이 K[256:384]에 주목하는 처리
# 1024 길이 시퀀스에서 블록 (i=0, j=2)
</span>
<span class="c1"># 이 블록 이전 상태:
# - O[0:128]은 K[0:256]으로부터의 어텐션 결과 포함
# - m[0:128] = [0.82, 0.79, ...] (지금까지의 최대 점수)
# - l[0:128] = [45.2, 52.1, ...] (지금까지의 지수 합)
</span>
<span class="c1"># 단계 1: 새 점수 계산
</span><span class="n">S_block</span> <span class="o">=</span> <span class="n">Q</span><span class="p">[</span><span class="mi">0</span><span class="p">:</span><span class="mi">128</span><span class="p">]</span> <span class="o">@</span> <span class="n">K</span><span class="p">[</span><span class="mi">256</span><span class="p">:</span><span class="mi">384</span><span class="p">].</span><span class="n">T</span> <span class="o">/</span> <span class="mi">8</span>  <span class="c1"># [128, 128] 행렬
# 예시 값: [[0.71, 0.65, ...], [0.73, 0.68, ...], ...]
</span>
<span class="c1"># 단계 2: 최댓값 업데이트 (수치 안정성을 위해)
</span><span class="n">m_new</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.82</span><span class="p">,</span> <span class="mf">0.79</span><span class="p">,</span> <span class="p">...]</span>  <span class="c1"># 이전 최댓값이 여전히 더 큼
</span>
<span class="c1"># 단계 3: 이 블록의 어텐션 가중치 계산
</span><span class="n">P_block</span> <span class="o">=</span> <span class="nf">exp</span><span class="p">(</span><span class="n">S_block</span> <span class="o">-</span> <span class="n">m_new</span><span class="p">.</span><span class="nf">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="c1"># 값: [[0.89, 0.84, ...], [0.91, 0.87, ...], ...]
</span>
<span class="c1"># 단계 4: 지수 합 업데이트
</span><span class="n">l_new</span> <span class="o">=</span> <span class="n">l</span> <span class="o">*</span> <span class="mf">1.0</span> <span class="o">+</span> <span class="n">P_block</span><span class="p">.</span><span class="nf">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># 재조정 불필요
# 새 값: [67.3, 73.5, ...]
</span>
<span class="c1"># 단계 5: 가중 값 누적
</span><span class="n">O_new</span> <span class="o">=</span> <span class="p">(</span><span class="n">l</span> <span class="o">*</span> <span class="n">O</span> <span class="o">+</span> <span class="n">P_block</span> <span class="o">@</span> <span class="n">V</span><span class="p">[</span><span class="mi">256</span><span class="p">:</span><span class="mi">384</span><span class="p">])</span> <span class="o">/</span> <span class="n">l_new</span>
<span class="c1"># 이전과 새로운 기여를 올바르게 가중!
</span></code></pre></div></div> <p><strong>제 통찰</strong>: 아름다운 점은 시퀀스 길이와 관계없이 언제나 128×128 행렬만 SRAM에 유지한다는 것입니다!</p> <h2 id="3-중요한-가정과-조건">3. 중요한 가정과 조건</h2> <h3 id="하드웨어-요구사항">하드웨어 요구사항</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">check_flashattention_viability</span><span class="p">(</span><span class="n">gpu_specs</span><span class="p">,</span> <span class="n">model_config</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">
    FlashAttention은 특정 하드웨어 속성이 필요합니다
    </span><span class="sh">"""</span>
    <span class="c1"># 1. SM당 충분한 SRAM
</span>    <span class="n">sram_needed</span> <span class="o">=</span> <span class="p">(</span><span class="mi">3</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">*</span> <span class="n">dim</span> <span class="o">+</span>  <span class="c1"># Q, K, V 블록
</span>                   <span class="n">block_size</span> <span class="o">*</span> <span class="n">block_size</span> <span class="o">+</span>  <span class="c1"># S 블록  
</span>                   <span class="n">block_size</span> <span class="o">*</span> <span class="n">dim</span><span class="p">)</span>          <span class="c1"># O 블록
</span>    
    <span class="k">if</span> <span class="n">sram_needed</span> <span class="o">&gt;</span> <span class="n">gpu_specs</span><span class="p">.</span><span class="n">sram_per_sm</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">❌ 블록 크기가 SRAM에 비해 너무 큼</span><span class="sh">"</span><span class="p">)</span>
        <span class="k">return</span> <span class="bp">False</span>
    
    <span class="c1"># 2. 높은 메모리 대역폭 비율
</span>    <span class="n">compute_to_memory_ratio</span> <span class="o">=</span> <span class="n">gpu_specs</span><span class="p">.</span><span class="n">tflops</span> <span class="o">/</span> <span class="n">gpu_specs</span><span class="p">.</span><span class="n">memory_bandwidth</span>
    <span class="k">if</span> <span class="n">compute_to_memory_ratio</span> <span class="o">&lt;</span> <span class="mi">100</span><span class="p">:</span>  <span class="c1"># 대략적인 임계값
</span>        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">⚠️  GPU가 메모리 제한이 아닌 연산 제한일 수 있음</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="c1"># 3. 효율적인 행렬 곱셈 유닛
</span>    <span class="k">if</span> <span class="ow">not</span> <span class="n">gpu_specs</span><span class="p">.</span><span class="n">has_tensor_cores</span><span class="p">:</span>
        <span class="nf">print</span><span class="p">(</span><span class="sh">"</span><span class="s">⚠️  텐서 코어 없이는 성능 향상 감소</span><span class="sh">"</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="bp">True</span>

<span class="c1"># 예시 확인
</span><span class="n">a100_specs</span> <span class="o">=</span> <span class="p">{</span>
    <span class="sh">'</span><span class="s">sram_per_sm</span><span class="sh">'</span><span class="p">:</span> <span class="mi">192</span> <span class="o">*</span> <span class="mi">1024</span><span class="p">,</span>      <span class="c1"># 192 KB
</span>    <span class="sh">'</span><span class="s">tflops</span><span class="sh">'</span><span class="p">:</span> <span class="mi">312</span><span class="p">,</span>                   <span class="c1"># FP16
</span>    <span class="sh">'</span><span class="s">memory_bandwidth</span><span class="sh">'</span><span class="p">:</span> <span class="mf">1.5</span><span class="p">,</span>         <span class="c1"># TB/s
</span>    <span class="sh">'</span><span class="s">has_tensor_cores</span><span class="sh">'</span><span class="p">:</span> <span class="bp">True</span>
<span class="p">}</span>
<span class="nf">check_flashattention_viability</span><span class="p">(</span><span class="n">a100_specs</span><span class="p">,</span> <span class="p">{</span><span class="sh">'</span><span class="s">block_size</span><span class="sh">'</span><span class="p">:</span> <span class="mi">128</span><span class="p">,</span> <span class="sh">'</span><span class="s">dim</span><span class="sh">'</span><span class="p">:</span> <span class="mi">64</span><span class="p">})</span>
</code></pre></div></div> <h3 id="알고리즘적-가정">알고리즘적 가정</h3> <p>FlashAttention이 가장 잘 작동하는 경우에 대한 <strong>제 분석</strong>:</p> <ol> <li> <strong>메모리 제한 워크로드</strong>: 시퀀스 길이 &gt; 512</li> <li> <strong>표준 어텐션 패턴</strong>: 커스텀 어텐션 마스크와는 효율적으로 작동하지 않음</li> <li> <strong>수치 정밀도</strong>: FP16/BF16 권장 (FP32는 SRAM 효율성 감소)</li> <li> <strong>배치 크기 유연성</strong>: 시퀀스 길이 차원으로 병렬화 불가</li> </ol> <h2 id="4-실험-결과-분석">4. 실험 결과 분석</h2> <h3 id="주요-성능-결과">주요 성능 결과</h3> <table> <thead> <tr> <th><strong>모델</strong></th> <th><strong>설정</strong></th> <th><strong>기준선</strong></th> <th><strong>FlashAttention</strong></th> <th><strong>제 해석</strong></th> </tr> </thead> <tbody> <tr> <td>BERT-large</td> <td>MLPerf 1.1</td> <td>100% (기준)</td> <td><strong>115% (1.15배)</strong></td> <td>MLPerf가 이미 고도로 최적화된 점을 고려하면 인상적</td> </tr> <tr> <td>GPT-2</td> <td>HuggingFace</td> <td>100%</td> <td><strong>300% (3.0배)</strong></td> <td>덜 최적화된 구현에서의 가치를 보여줌</td> </tr> <tr> <td>GPT-2</td> <td>Megatron-LM</td> <td>100%</td> <td><strong>180% (1.8배)</strong></td> <td>최적화된 코드에서도 여전히 큰 향상</td> </tr> <tr> <td>Long Range Arena</td> <td>Seq 1K-4K</td> <td>100%</td> <td><strong>240% (2.4배)</strong></td> <td>예상대로 시퀀스 길이가 길수록 향상 증가</td> </tr> </tbody> </table> <p><strong>제 견해</strong>: 결과는 메모리 제한 가설을 검증합니다. 더 긴 시퀀스에서 더 큰 속도 향상은 O(N²) 메모리 접근이 병목임을 확인합니다.</p> <h3 id="메모리-효율성-결과">메모리 효율성 결과</h3> <table> <thead> <tr> <th><strong>시퀀스 길이</strong></th> <th><strong>표준 메모리</strong></th> <th><strong>FlashAttention 메모리</strong></th> <th><strong>절약</strong></th> </tr> </thead> <tbody> <tr> <td>512</td> <td>1 GB</td> <td>200 MB</td> <td>5배</td> </tr> <tr> <td>2,048</td> <td>16 GB</td> <td>1.6 GB</td> <td>10배</td> </tr> <tr> <td>8,192</td> <td>256 GB</td> <td>6.4 GB</td> <td>40배</td> </tr> <tr> <td>16,384</td> <td>1 TB (OOM)</td> <td>25.6 GB</td> <td>새로운 기능 가능</td> </tr> </tbody> </table> <p><strong>제 분석</strong>: 선형 대 이차 스케일링은 혁신적입니다. 이것은 단순한 최적화가 아니라 가능한 것을 근본적으로 바꿉니다.</p> <h3 id="품질-개선">품질 개선</h3> <table> <thead> <tr> <th><strong>작업</strong></th> <th><strong>지표</strong></th> <th><strong>결과</strong></th> <th><strong>제 해석</strong></th> </tr> </thead> <tbody> <tr> <td>GPT-2 (4K vs 1K 컨텍스트)</td> <td>Perplexity</td> <td>-0.7 개선</td> <td>더 긴 컨텍스트 = 더 나은 언어 이해</td> </tr> <tr> <td>문서 분류</td> <td>F1 점수</td> <td>+6.4 포인트</td> <td>전체 문서를 보는 것으로 큰 향상</td> </tr> <tr> <td>Path-X (16K)</td> <td>정확도</td> <td>61.4% (랜덤 50% 대비)</td> <td>이를 해결한 <strong>최초</strong> 사례!</td> </tr> <tr> <td>Path-256 (64K)</td> <td>정확도</td> <td>63.1% (랜덤 50% 대비)</td> <td>시퀀스 모델링의 한계를 넓힘</td> </tr> </tbody> </table> <p><strong>제 통찰</strong>: 이것들은 단순한 벤치마크 개선이 아니라 새로운 능력을 나타냅니다. Path-X는 트랜스포머에게 불가능하다고 여겨졌습니다!</p> <h2 id="5-절제-연구-무엇이-정말-중요한가">5. 절제 연구: 무엇이 정말 중요한가?</h2> <h3 id="구성요소-기여도">구성요소 기여도</h3> <table> <thead> <tr> <th><strong>구성요소</strong></th> <th><strong>없을 때</strong></th> <th><strong>있을 때</strong></th> <th><strong>영향</strong></th> <th><strong>제 분석</strong></th> </tr> </thead> <tbody> <tr> <td>타일링</td> <td>1.0배</td> <td>1.4배</td> <td>+40%</td> <td>핵심 혁신 - 데이터를 SRAM에 유지</td> </tr> <tr> <td>온라인 소프트맥스</td> <td>1.4배</td> <td>1.75배</td> <td>+25%</td> <td>데이터를 한 번만 통과 가능</td> </tr> <tr> <td>재계산</td> <td>1.75배</td> <td>2.1배</td> <td>+20%</td> <td>N×N 어텐션 행렬 저장 회피</td> </tr> <tr> <td>커널 융합</td> <td>2.1배</td> <td>2.4배</td> <td>+15%</td> <td>오버헤드 감소, SRAM 사용 최대화</td> </tr> </tbody> </table> <p><strong>제 견해</strong>: 각 구성요소는 필요하지만 충분하지 않습니다. 이들 간의 시너지가 FlashAttention을 특별하게 만듭니다.</p> <h3 id="블록-크기-민감도">블록 크기 민감도</h3> <table> <thead> <tr> <th><strong>블록 크기</strong></th> <th><strong>성능</strong></th> <th><strong>메모리 사용</strong></th> <th><strong>제 분석</strong></th> </tr> </thead> <tbody> <tr> <td>32</td> <td>85%</td> <td>최소</td> <td>너무 많은 HBM 접근이 목적을 무너뜨림</td> </tr> <tr> <td>64</td> <td>95%</td> <td>낮음</td> <td>작은 모델/차원에 좋음</td> </tr> <tr> <td>128</td> <td>100% (최고)</td> <td>최적</td> <td>대부분 GPU의 스위트 스팟</td> </tr> <tr> <td>256</td> <td>98%</td> <td>높음</td> <td>SRAM 제한이 나타남</td> </tr> </tbody> </table> <p><strong>제 통찰</strong>: 블록 크기는 중요하고 하드웨어 의존적입니다. 저자들은 128이 다양한 GPU에서 잘 작동한다는 것을 발견했지만, 조정이 필요합니다.</p> <h2 id="6-실용적-의미">6. 실용적 의미</h2> <h3 id="실무자들에게-의미하는-것">실무자들에게 의미하는 것</h3> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># FlashAttention 이전
</span><span class="k">def</span> <span class="nf">train_gpt_traditional</span><span class="p">(</span><span class="n">seq_len</span><span class="o">=</span><span class="mi">2048</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">seq_len</span> <span class="o">&gt;</span> <span class="mi">2048</span><span class="p">:</span>
        <span class="k">raise</span> <span class="nc">OOMError</span><span class="p">(</span><span class="sh">"</span><span class="s">어텐션 행렬을 메모리에 맞출 수 없음</span><span class="sh">"</span><span class="p">)</span>
    <span class="c1"># 느린 학습, 제한된 컨텍스트
</span>
<span class="c1"># FlashAttention 이후  
</span><span class="k">def</span> <span class="nf">train_gpt_flash</span><span class="p">(</span><span class="n">seq_len</span><span class="o">=</span><span class="mi">16384</span><span class="p">):</span>
    <span class="c1"># 그냥 작동합니다! 그리고 더 빠르게도
</span>    <span class="n">model</span> <span class="o">=</span> <span class="nc">GPT</span><span class="p">(</span><span class="n">seq_len</span><span class="o">=</span><span class="mi">16384</span><span class="p">,</span> <span class="n">use_flash_attention</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
    <span class="c1"># 8배 더 긴 컨텍스트, 3배 빠른 학습
</span></code></pre></div></div> <h3 id="미래-방향-제-생각">미래 방향 (제 생각)</h3> <ol> <li> <strong>하드웨어 공동 설계</strong>: 미래 GPU는 SRAM을 늘려 FlashAttention을 더 효과적으로 만들 수 있음</li> <li> <strong>희소 패턴</strong>: 블록 희소 FlashAttention은 100K+ 시퀀스에 대한 가능성을 보여줌</li> <li> <strong>다른 연산</strong>: IO-aware 원칙은 다른 메모리 제한 연산에도 적용 가능</li> <li> <strong>추론 최적화</strong>: 현재 작업은 학습에 초점을 맞추고 있지만, 추론은 다른 패턴을 가짐</li> </ol> <h2 id="결론">결론</h2> <p>FlashAttention은 시스템 사고의 걸작입니다. FLOPs 최적화에 몰려드는 대신, 저자들은 실제 병목(메모리 대역폭)을 파악하고 하드웨어 제약을 존중하는 알고리즘을 설계했습니다. 결과는 단순히 더 빠른 어텐션이 아니라 언어 모델링에서 완전히 새로운 능력을 가능하게 합니다.</p> <p><strong>핵심 교훈</strong>: 때로는 최고의 최적화는 더 적은 작업을 하는 것이 아니라, 기본 하드웨어에 대해 같은 작업을 더 지능적으로 수행하는 것에서 나옵니다.</p> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa3/">Flash Attention 3</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa2/">Flash Attention 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/usp/">Unified Sequence Parallelism</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/deepspeed_ulysses/">DeepSpeed Ulysses</a> </li> <div id="disqus_thread" style="max-width: 930px; margin: 0 auto;"> <script type="text/javascript">
    var disqus_shortname  = 'sungyubkim';
    var disqus_identifier = '/blog/fa';
    var disqus_title      = "Flash Attention";
    (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Sung-Yub Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 29, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>