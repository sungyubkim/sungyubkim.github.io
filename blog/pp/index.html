<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> Pipeline Parallel (GPipe) | Sung-Yub Kim </title> <meta name="author" content="Sung-Yub Kim"> <meta name="description" content="Deep dive into GPipe's micro-batch pipeline parallelism for training models beyond single-device memory limits."> <meta name="keywords" content="machine-learning, deep-learning, llm, foundation-models, research"> <meta property="og:site_name" content="Sung-Yub Kim"> <meta property="og:type" content="article"> <meta property="og:title" content="Sung-Yub Kim | Pipeline Parallel (GPipe)"> <meta property="og:url" content="https://sungyubkim.github.io/blog/pp/"> <meta property="og:description" content="Deep dive into GPipe's micro-batch pipeline parallelism for training models beyond single-device memory limits."> <meta property="og:locale" content="en"> <meta name="twitter:card" content="summary"> <meta name="twitter:title" content="Pipeline Parallel (GPipe)"> <meta name="twitter:description" content="Deep dive into GPipe's micro-batch pipeline parallelism for training models beyond single-device memory limits."> <script type="application/ld+json">
    {
        "author":
        {
            "@type": "Person",
            "name": "Sung-Yub Kim"
        },
        "url": "https://sungyubkim.github.io/blog/pp/",
        "@type": "BlogPosting",
        "description": "Deep dive into GPipe's micro-batch pipeline parallelism for training models beyond single-device memory limits.",
        "headline": "Pipeline Parallel (GPipe)",
        
        "sameAs": ["https://github.com/sungyubkim","https://scholar.google.com/citations?user=m2rhgrkAAAAJ","https://www.linkedin.com/in/sung-yub-kim-0a82a1264","https://twitter.com/SungyubK"],
        
        "name": "Sung-Yub Kim",
        "@context": "https://schema.org"
    }
  </script> <meta http-equiv="Content-Security-Policy" content="default-src 'self'; script-src 'self' 'unsafe-inline' https:; style-src 'self' 'unsafe-inline' https:; img-src 'self' data: https:; font-src 'self' data: https:; media-src 'self' https:; frame-src 'self' https:; connect-src 'self' https:;"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?v=a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?v=f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?v=62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?v=591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link defer href="/assets/css/bootstrap-toc.min.css?v=6f5af0bb9aab25d79b2448143cbeaa88" rel="stylesheet"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%F0%9F%A7%A0&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?v=d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://sungyubkim.github.io/blog/pp/"> <script src="/assets/js/theme.js?v=48c9b5bd7f2e0605e39e579400e22553"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?v=5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav sticky-bottom-footer"> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Sung-Yub</span> Kim </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">About </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">Blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">Publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">CV </a> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="fa-solid fa-magnifying-glass"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="fa-half-sun-moon" id="light-toggle-system"></i> <i class="fa-solid fa-moon" id="light-toggle-dark"></i> <i class="fa-solid fa-sun" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="row"> <div class="col-sm-3"> <nav id="toc-sidebar" class="sticky-top"></nav> </div> <div class="col-sm-9"> <div class="post"> <header class="post-header"> <h1 class="post-title">Pipeline Parallel (GPipe)</h1> <p class="post-meta"> Created on May 29, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/pipeline-parallelism"> <i class="fa-solid fa-hashtag fa-sm"></i> pipeline-parallelism</a>   <a href="/blog/tag/distributed-training"> <i class="fa-solid fa-hashtag fa-sm"></i> distributed-training</a>   <a href="/blog/tag/memory-efficiency"> <i class="fa-solid fa-hashtag fa-sm"></i> memory-efficiency</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <h1 id="tldr">TL;DR</h1> <blockquote> <p>GPipe는 <strong>마이크로 배치 파이프라인 병렬화</strong>를 도입하여 단일 기기 메모리 한계를 넘어서는 신경망 훈련을 가능하게 합니다. 주요 기여점:</p> <ul> <li> <strong>새로운 알고리즘</strong>: 미니배치를 마이크로배치로 분할하고 모델 파티션에 걸쳐 파이프라이닝하며 동기식 그라디언트 업데이트 수행</li> <li> <strong>메모리 효율성</strong>: 재계산을 통해 최대 메모리 사용량을 O(N×L)에서 O(N/M + L/K×N/M)로 감소</li> <li> <strong>아키텍처 독립성</strong>: 순차적 레이어 구조(CNN, Transformer 등)에서 범용적으로 작동</li> <li> <strong>실용적 확장성</strong>: 8배 파티셔닝에서 79% 효율성으로 25-298배 더 큰 모델 훈련 가능</li> </ul> <p><strong>핵심</strong>: 대규모 모델 훈련을 민주화한 강력한 시스템 기여이지만, 품질 향상에 대한 실험적 증거는 혼재된 비교로 인해 약함.</p> </blockquote> <ul> <li>Paper Link: <a href="https://arxiv.org/pdf/1811.06965" rel="external nofollow noopener" target="_blank">https://arxiv.org/pdf/1811.06965</a> </li> </ul> <hr> <h1 id="related-papers">Related Papers</h1> <p><strong>대규모 모델 훈련:</strong></p> <ul> <li> <a href="/blog/moe/">MoE</a> - 파이프라인과 결합 가능한 전문가 혼합 아키텍처</li> <li> <a href="https://arxiv.org/pdf/2101.03961" rel="external nofollow noopener" target="_blank">Switch Transformers</a> - 파이프라인 병렬화와 호환되는 MoE</li> <li> <a href="/blog/usp/">USP</a> - 파이프라인과 시퀀스 병렬화 통합</li> </ul> <p><strong>시스템 최적화:</strong></p> <ul> <li> <a href="https://arxiv.org/pdf/2211.05102" rel="external nofollow noopener" target="_blank">Efficiently Scaling Transformer Inference</a> - 파이프라인 기반 효율적 추론</li> <li> <a href="https://arxiv.org/pdf/2309.14509" rel="external nofollow noopener" target="_blank">DeepSpeed Ulysses</a> - 파이프라인과 결합 가능한 시퀀스 병렬화</li> </ul> <hr> <h1 id="takeaways">Takeaways</h1> <h3 id="핵심-문제와-동기">핵심 문제와 동기</h3> <p>딥러닝의 발전은 모델 확장과 밀접하게 연관되어 있습니다 - 더 큰 모델이 다양한 도메인에서 일관되게 더 나은 성능을 달성합니다. 하지만 하드웨어 메모리 제약이 근본적인 병목을 만듭니다: 가장 큰 가속기조차 특정 크기까지의 모델만 수용할 수 있습니다.</p> <p><strong>도전 과제</strong>: 기존 모델 병렬화 접근법들은 삼각 딜레마에 직면합니다:</p> <ul> <li> <strong>효율성</strong>: 대부분의 접근법이 하드웨어를 심각하게 활용하지 못함</li> <li> <strong>유연성</strong>: 해결책들이 종종 아키텍처 특화적임</li> <li> <strong>일관성</strong>: 비동기 방법들이 훈련 불안정성을 야기</li> </ul> <p>GPipe는 다양한 아키텍처에서 높은 하드웨어 활용률을 달성하면서 훈련 일관성을 유지하는 새로운 패러다임을 도입하여 이를 해결합니다.</p> <h3 id="기술적-혁신-마이크로배치-파이프라인-병렬화">기술적 혁신: 마이크로배치 파이프라인 병렬화</h3> <h4 id="핵심-개념">핵심 개념</h4> <p>기기들이 순차적으로 대기하는 전통적인 모델 병렬화 대신, GPipe는:</p> <ol> <li>모델을 K개 기기에 걸쳐 <strong>분할</strong> </li> <li>각 미니배치를 M개 마이크로배치로 <strong>분할</strong> </li> <li>마이크로배치를 파티션들을 통해 <strong>파이프라이닝</strong> </li> <li>그라디언트를 <strong>동기적으로 누적</strong> </li> </ol> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">class</span> <span class="nc">GPipe</span><span class="p">:</span>
    <span class="k">def</span> <span class="nf">__init__</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">layers</span><span class="p">,</span> <span class="n">num_partitions</span><span class="o">=</span><span class="mi">4</span><span class="p">,</span> <span class="n">num_microbatches</span><span class="o">=</span><span class="mi">8</span><span class="p">):</span>
        <span class="n">self</span><span class="p">.</span><span class="n">K</span> <span class="o">=</span> <span class="n">num_partitions</span>
        <span class="n">self</span><span class="p">.</span><span class="n">M</span> <span class="o">=</span> <span class="n">num_microbatches</span>
        <span class="c1"># 기기들 간 계산 균형을 맞추도록 모델 분할
</span>        <span class="n">self</span><span class="p">.</span><span class="n">cells</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_partition_model</span><span class="p">(</span><span class="n">layers</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">minibatch</span><span class="p">):</span>
        <span class="c1"># 마이크로배치로 분할
</span>        <span class="n">microbatches</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_split_batch</span><span class="p">(</span><span class="n">minibatch</span><span class="p">,</span> <span class="n">self</span><span class="p">.</span><span class="n">M</span><span class="p">)</span>
        
        <span class="c1"># 순방향 패스 파이프라이닝
</span>        <span class="n">activations</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_pipeline_forward</span><span class="p">(</span><span class="n">microbatches</span><span class="p">)</span>
        
        <span class="c1"># 역방향 패스 파이프라이닝
</span>        <span class="n">gradients</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="nf">_pipeline_backward</span><span class="p">(</span><span class="n">activations</span><span class="p">)</span>
        
        <span class="c1"># 동기식 그라디언트 업데이트 (일관성을 위한 핵심)
</span>        <span class="n">self</span><span class="p">.</span><span class="nf">_accumulate_and_update</span><span class="p">(</span><span class="n">gradients</span><span class="p">)</span>
</code></pre></div></div> <h4 id="파이프라인-스케줄링-예시">파이프라인 스케줄링 예시</h4> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>K=4개 기기, M=4개 마이크로배치에 대한 타임라인:

시간: 0  1  2  3  4  5  6
기기0: m0 m1 m2 m3 -- -- --
기기1: -- m0 m1 m2 m3 -- --  
기기2: -- -- m0 m1 m2 m3 --
기기3: -- -- -- m0 m1 m2 m3

버블 오버헤드 = (K-1)/(M+K-1) = 3/7 ≈ 43%
</code></pre></div></div> <p><strong>핵심 통찰</strong>: M ≥ 4×K일 때 버블 오버헤드가 무시할 수 있게 되어 거의 선형적 확장이 가능해집니다.</p> <h4 id="메모리-최적화-재계산">메모리 최적화: 재계산</h4> <p>전통적인 훈련은 역전파를 위해 모든 중간 활성화를 저장합니다. GPipe는 파티션 경계에서만 입력을 저장하고 역방향 패스 동안 순방향 패스를 재계산합니다:</p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">forward_with_checkpointing</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="c1"># 입력만 저장하고 중간값들은 버림
</span>    <span class="n">self</span><span class="p">.</span><span class="n">checkpoint</span> <span class="o">=</span> <span class="n">x</span><span class="p">.</span><span class="nf">detach</span><span class="p">().</span><span class="nf">requires_grad_</span><span class="p">(</span><span class="bp">True</span><span class="p">)</span>
    
    <span class="c1"># 순방향 패스
</span>    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">x</span>

<span class="k">def</span> <span class="nf">backward_with_rematerialization</span><span class="p">(</span><span class="n">self</span><span class="p">,</span> <span class="n">grad_output</span><span class="p">):</span>
    <span class="c1"># 중간값들을 얻기 위해 순방향 패스 재계산
</span>    <span class="n">x</span> <span class="o">=</span> <span class="n">self</span><span class="p">.</span><span class="n">checkpoint</span>
    <span class="n">intermediates</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="n">self</span><span class="p">.</span><span class="n">layers</span><span class="p">:</span>
        <span class="n">intermediates</span><span class="p">.</span><span class="nf">append</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="n">x</span> <span class="o">=</span> <span class="nf">layer</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    
    <span class="c1"># 재계산된 활성화를 사용한 표준 역방향 패스
</span>    <span class="k">return</span> <span class="n">self</span><span class="p">.</span><span class="nf">_backward_pass</span><span class="p">(</span><span class="n">grad_output</span><span class="p">,</span> <span class="n">intermediates</span><span class="p">)</span>
</code></pre></div></div> <p><strong>메모리 분석</strong>:</p> <ul> <li> <strong>GPipe 없이</strong>: 배치 크기 N, 레이어 수 L에 대해 O(N×L) 메모리</li> <li> <strong>GPipe 사용</strong>: O(N/M + L/K×N/M) 메모리</li> <li> <strong>예시</strong>: N=128, L=100, M=8, K=4 → 97% 메모리 감소</li> </ul> <h3 id="실험적-증거와-비판적-평가">실험적 증거와 비판적 평가</h3> <h4 id="주요-결과-분석">주요 결과 분석</h4> <table> <thead> <tr> <th><strong>능력</strong></th> <th><strong>달성 사항</strong></th> <th><strong>증거 품질</strong></th> <th><strong>실용적 임팩트</strong></th> </tr> </thead> <tbody> <tr> <td><strong>메모리 확장</strong></td> <td>AmoebaNet: 25배 증가<br>Transformer: 298배 증가</td> <td>✅ <strong>강함</strong>: 명확한 확장성 입증</td> <td>🔥 <strong>높음</strong>: 이전에 불가능했던 모델들 가능</td> </tr> <tr> <td><strong>처리량 효율성</strong></td> <td>8배 확장에서 79% 효율성</td> <td>✅ <strong>좋음</strong>: 버블 오버헤드 이론 검증</td> <td>🔥 <strong>높음</strong>: 거의 선형적 확장 달성</td> </tr> <tr> <td><strong>모델 품질</strong></td> <td>ImageNet 84.4% 정확도<br>이중언어 번역 모델 능가</td> <td>❌ <strong>약함</strong>: 불공정한 기준선, 혼재된 비교</td> <td>❓ <strong>불명확</strong>: 품질 향상이 분리되지 않음</td> </tr> <tr> <td><strong>아키텍처 유연성</strong></td> <td>CNN + Transformer에서 작동</td> <td>⚠️ <strong>제한적</strong>: 2개 아키텍처 계열만</td> <td>📈 <strong>보통</strong>: 유망하지만 더 넓은 검증 필요</td> </tr> </tbody> </table> <h4 id="절제-연구ablation-study-통찰">절제 연구(Ablation Study) 통찰</h4> <table> <thead> <tr> <th><strong>구성요소</strong></th> <th><strong>영향</strong></th> <th><strong>핵심 발견</strong></th> </tr> </thead> <tbody> <tr> <td><strong>재계산</strong></td> <td>3.9배 메모리 감소</td> <td>단일 기기에서 더 큰 모델 적재에 필수적</td> </tr> <tr> <td><strong>마이크로배치 수(M)</strong></td> <td>M=32: 6.3배 가속 vs M=1: 1배</td> <td>파이프라인 이론 검증 - 더 많은 마이크로배치가 버블 시간 감소</td> </tr> <tr> <td><strong>파티션 수(K)</strong></td> <td>Transformer: 선형 확장<br>AmoebaNet: 하위선형</td> <td> <strong>중요</strong>: 균일한 아키텍처가 이질적인 것보다 더 잘 확장됨</td> </tr> <tr> <td><strong>통신</strong></td> <td>고속 상호연결 없이도 유사한 확장</td> <td> <strong>놀라움</strong>: 예상과 달리 통신이 병목이 아님</td> </tr> </tbody> </table> <h4 id="가장-중요한-발견-아키텍처-의존성">가장 중요한 발견: 아키텍처 의존성</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># Transformer (균일한 레이어): 8개 파티션에서 6.3배 가속
# AmoebaNet (이질적): 8개 파티션에서 3.48배 가속
</span>
<span class="c1"># 왜? 부하 균형 문제:
</span><span class="n">transformer_layer_costs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">,</span> <span class="mi">100</span><span class="p">]</span>  <span class="c1"># 균형 맞추기 쉬움
</span><span class="n">amoebanet_layer_costs</span> <span class="o">=</span> <span class="p">[</span><span class="mi">50</span><span class="p">,</span> <span class="mi">200</span><span class="p">,</span> <span class="mi">80</span><span class="p">,</span> <span class="mi">300</span><span class="p">,</span> <span class="mi">60</span><span class="p">,</span> <span class="mi">150</span><span class="p">,</span> <span class="mi">90</span><span class="p">,</span> <span class="mi">250</span><span class="p">]</span>        <span class="c1"># 균형 맞추기 어려움
</span></code></pre></div></div> <p>이는 <strong>GPipe의 효과가 근본적으로 아키텍처 이질성에 의해 제한된다</strong>는 중요한 통찰을 보여줍니다 - 원본 논문에서 강조되지 않은 핵심적인 발견입니다.</p> <h3 id="중요한-가정과-성공-조건">중요한 가정과 성공 조건</h3> <h4 id="필수-조건들">필수 조건들</h4> <ol> <li> <strong>단일 레이어 메모리 제약</strong>: 각 레이어가 하나의 기기에 맞아야 함</li> <li> <strong>순차적 아키텍처</strong>: 모델이 레이어 시퀀스로 표현 가능해야 함</li> <li> <strong>충분한 마이크로배치</strong>: 효율성을 위해 M ≥ 4×K 필요</li> <li> <strong>균형 잡힌 파티셔닝</strong>: 레이어들이 대략 비슷한 계산 비용을 가져야 함</li> </ol> <h4 id="숨겨진-가정들">숨겨진 가정들</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># 배치 정규화 가정
</span><span class="k">def</span> <span class="nf">batch_norm_caveat</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    BatchNorm은 훈련 중 마이크로배치에 대해 통계를 계산하지만
    평가를 위해서는 전체 미니배치 통계가 필요함.
    작은 마이크로배치는 훈련을 불안정하게 만들 수 있음.
    </span><span class="sh">"""</span>
    <span class="k">if</span> <span class="n">micro_batch_size</span> <span class="o">&lt;</span> <span class="mi">8</span><span class="p">:</span>
        <span class="n">warnings</span><span class="p">.</span><span class="nf">warn</span><span class="p">(</span><span class="sh">"</span><span class="s">작은 마이크로배치가 BatchNorm을 불안정하게 만들 수 있음</span><span class="sh">"</span><span class="p">)</span>

<span class="c1"># 통신 가정
</span><span class="k">def</span> <span class="nf">communication_assumption</span><span class="p">():</span>
    <span class="sh">"""</span><span class="s">
    활성화 전송 시간 &lt;&lt; 계산 시간이라고 가정.
    매우 큰 활성화나 느린 상호연결에서는 성립하지 않음.
    </span><span class="sh">"""</span>
    <span class="n">transfer_time</span> <span class="o">=</span> <span class="n">activation_size</span> <span class="o">/</span> <span class="n">bandwidth</span>
    <span class="k">if</span> <span class="n">transfer_time</span> <span class="o">&gt;</span> <span class="mf">0.1</span> <span class="o">*</span> <span class="n">compute_time</span><span class="p">:</span>
        <span class="n">warnings</span><span class="p">.</span><span class="nf">warn</span><span class="p">(</span><span class="sh">"</span><span class="s">통신이 병목이 되고 있음</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <h3 id="강점과-한계">강점과 한계</h3> <h4 id="진정한-강점들">진정한 강점들</h4> <ul> <li> <strong>우아한 엔지니어링</strong>: 효율성-유연성-일관성 삼각 딜레마 해결</li> <li> <strong>광범위한 적용성</strong>: 다양한 아키텍처와 하드웨어에서 작동</li> <li> <strong>강력한 이론적 기반</strong>: 버블 오버헤드 분석이 명확한 지침 제공</li> <li> <strong>실용적 임팩트</strong>: 대규모 모델 혁명 가능하게 함</li> </ul> <h4 id="중요한-한계들">중요한 한계들</h4> <ul> <li> <strong>아키텍처 의존성</strong>: 이익이 레이어 균일성에 크게 의존</li> <li> <strong>메모리 제약</strong>: 단일 기기 메모리보다 큰 레이어 처리 불가</li> <li> <strong>배치 크기 제한</strong>: 효과적으로 분할할 수 있을 만큼 큰 배치 필요</li> <li> <strong>구현 복잡성</strong>: 데이터 병렬화보다 상당히 복잡</li> </ul> <h4 id="실험적-약점들">실험적 약점들</h4> <ul> <li> <strong>기준선 부재</strong>: 동일한 총 매개변수를 가진 데이터 병렬화와의 비교 없음</li> <li> <strong>혼재된 평가</strong>: 품질 향상이 규모 효과와 혼재됨</li> <li> <strong>제한된 통계적 엄밀성</strong>: 단일 실행, 신뢰구간 없음</li> <li> <strong>선별된 결과</strong>: 전이학습을 위한 선택적 데이터셋 선택</li> </ul> <h3 id="연구자와-실무자를-위한-실용적-함의">연구자와 실무자를 위한 실용적 함의</h3> <h4 id="gpipe를-언제-사용할-것인가">GPipe를 언제 사용할 것인가</h4> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="k">def</span> <span class="nf">should_use_gpipe</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">hardware</span><span class="p">,</span> <span class="n">batch_size</span><span class="p">):</span>
    <span class="sh">"""</span><span class="s">GPipe 채택을 위한 의사결정 프레임워크</span><span class="sh">"""</span>
    
    <span class="c1"># 메모리 확인
</span>    <span class="k">if</span> <span class="n">model</span><span class="p">.</span><span class="n">memory_requirement</span> <span class="o">&lt;=</span> <span class="n">single_device_memory</span><span class="p">:</span>
        <span class="k">return</span> <span class="sh">"</span><span class="s">대신 데이터 병렬화 사용</span><span class="sh">"</span>
    
    <span class="c1"># 아키텍처 확인
</span>    <span class="k">if</span> <span class="n">model</span><span class="p">.</span><span class="nf">has_uniform_layers</span><span class="p">():</span>
        <span class="k">return</span> <span class="sh">"</span><span class="s">GPipe가 잘 확장될 것</span><span class="sh">"</span>
    <span class="k">elif</span> <span class="n">model</span><span class="p">.</span><span class="nf">has_very_heterogeneous_layers</span><span class="p">():</span>
        <span class="k">return</span> <span class="sh">"</span><span class="s">다른 접근법 고려 (Mesh-TensorFlow)</span><span class="sh">"</span>
    
    <span class="c1"># 배치 크기 확인
</span>    <span class="n">min_micro_batch</span> <span class="o">=</span> <span class="n">batch_size</span> <span class="o">//</span> <span class="p">(</span><span class="mi">4</span> <span class="o">*</span> <span class="n">num_devices</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">min_micro_batch</span> <span class="o">&lt;</span> <span class="mi">8</span><span class="p">:</span>
        <span class="k">return</span> <span class="sh">"</span><span class="s">배치 크기 증가 또는 더 적은 기기 사용</span><span class="sh">"</span>
    
    <span class="k">return</span> <span class="sh">"</span><span class="s">GPipe가 적합함</span><span class="sh">"</span>
</code></pre></div></div> <h4 id="구현-고려사항들">구현 고려사항들</h4> <ol> <li> <strong>단순하게 시작</strong>: 균형 잡힌 파티셔닝으로 시작, 나중에 최적화</li> <li> <strong>효율성 모니터링</strong>: 버블 오버헤드와 통신 비용 추적</li> <li> <strong>배치 정규화 처리</strong>: 파티션 간 SyncBatchNorm 같은 기법 사용</li> <li> <strong>메모리 프로파일링</strong>: 재계산 메모리 급증 감시</li> </ol> <h4 id="대안적-접근법들">대안적 접근법들</h4> <ul> <li> <strong>데이터 병렬화</strong>: 모델이 단일 기기에 맞을 때</li> <li> <strong>텐서 병렬화</strong> (Mesh-TensorFlow): 매우 큰 개별 레이어용</li> <li> <strong>전문가 병렬화</strong>: 전문가 혼합 아키텍처용</li> <li> <strong>파이프라인 + 데이터 하이브리드</strong>: 최대 규모용</li> </ul> <h3 id="임팩트와-미래-방향">임팩트와 미래 방향</h3> <h4 id="역사적-중요성">역사적 중요성</h4> <p>GPipe는 다음을 통해 대규모 언어 모델 혁명을 가능하게 하는 데 중요한 역할을 했습니다:</p> <ul> <li>트랜스포머 확장을 실용적으로 가능하게 만듦</li> <li>실험을 위한 유연한 프레임워크 제공</li> <li>모델 병렬화가 효율적이면서 범용적일 수 있음을 입증</li> </ul> <h4 id="기술적-발전">기술적 발전</h4> <p>현대 접근법들이 GPipe의 통찰을 바탕으로 구축됩니다:</p> <ul> <li> <strong>PipeDream</strong>: 더 높은 처리량을 위한 비동기 파이프라인</li> <li> <strong>FairScale</strong>: 프로덕션 준비 구현</li> <li> <strong>DeepSpeed</strong>: 하이브리드 병렬화 전략</li> <li> <strong>Megatron</strong>: 트랜스포머 특화 최적화 접근법</li> </ul> <h4 id="열린-연구-질문들">열린 연구 질문들</h4> <ol> <li> <strong>최적 파티셔닝</strong>: 이질적 아키텍처를 위한 더 나은 알고리즘</li> <li> <strong>동적 스케줄링</strong>: 부하 기반 적응적 마이크로배치 크기 조정</li> <li> <strong>메모리 최적화</strong>: 더 정교한 체크포인팅 전략</li> <li> <strong>장애 허용성</strong>: 긴 훈련 실행에서 기기 장애 처리</li> </ol> <h3 id="큰-그림">큰 그림</h3> <p>GPipe는 직접적인 알고리즘적 돌파구를 제공하기보다는 새로운 과학적 가능성을 가능하게 한 고전적인 <strong>시스템 기여</strong>를 나타냅니다. 그 지속적인 가치는 다음에 있습니다:</p> <ol> <li> <strong>민주화</strong>: 기술 대기업을 넘어 대규모 모델 훈련을 접근 가능하게 만듦</li> <li> <strong>유연성</strong>: 아키텍처 특화 해결책이 아닌 일반적 프레임워크 제공</li> <li> <strong>기반</strong>: 후속 병렬화 연구를 위한 설계 원칙 확립</li> <li> <strong>실용적 임팩트</strong>: NLP와 컴퓨터 비전에서 많은 획기적 모델들을 직접 가능하게 함</li> </ol> <p><strong>핵심 통찰</strong>: 이 논문의 진정한 기여는 더 큰 모델이 더 낫다는 것을 증명하는 것이 아니라, 그러한 가설을 탐험할 수 있는 인프라를 제공하는 것입니다. 품질 향상에 대한 약한 실험적 증거가 그러한 탐험을 가능하게 만든 의미를 감소시키지는 않습니다.</p> <p>오늘날 실무자들에게 GPipe의 핵심 아이디어들은 여전히 관련이 있지만, 프로덕션 시스템들은 종종 더 정교한 하이브리드 접근법을 사용합니다. GPipe를 이해하는 것은 현대 분산 훈련 시스템과 모델 병렬화의 근본적인 트레이드오프에 대한 필수적인 직관을 제공합니다.</p> <hr> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa3/">Flash Attention 3</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa2/">Flash Attention 2</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/fa/">Flash Attention</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/usp/">Unified Sequence Parallelism</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/sp/">Reducing Activation Recomputation in Large Transformer Models</a> </li> <div id="disqus_thread" style="max-width: 930px; margin: 0 auto;"> <script type="text/javascript">
    var disqus_shortname  = 'sungyubkim';
    var disqus_identifier = '/blog/pp';
    var disqus_title      = "Pipeline Parallel (GPipe)";
    (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
    })();
  </script> <noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by Disqus.</a> </noscript> </div> </div> </div> </div> </div> <footer class="sticky-bottom mt-5" role="contentinfo"> <div class="container"> © Copyright 2026 Sung-Yub Kim. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Last updated: January 29, 2026. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?v=a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?v=85ddb88934d28b74e78031fd54cf8308"></script> <script defer src="/assets/js/bootstrap-toc.min.js?v=c82ff4de8b0955d6ff14f5b05eed7eb6"></script> <script src="/assets/js/no_defer.js?v=2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?v=c15de51d4bb57887caa2c21988d97279"></script> <script defer src="/assets/js/copy_code.js?v=c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?v=d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?v=a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?v=2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?v=f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?v=a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?v=6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?v=ccc841c459bfc0e64c1c2b5acd10df02"></script> </body> </html>